<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Defining Systems of PDEs · NeuralPDE.jl</title><meta name="title" content="Defining Systems of PDEs · NeuralPDE.jl"/><meta property="og:title" content="Defining Systems of PDEs · NeuralPDE.jl"/><meta property="twitter:title" content="Defining Systems of PDEs · NeuralPDE.jl"/><meta name="description" content="Documentation for NeuralPDE.jl."/><meta property="og:description" content="Documentation for NeuralPDE.jl."/><meta property="twitter:description" content="Documentation for NeuralPDE.jl."/><meta property="og:url" content="https://docs.sciml.ai/NeuralPDE/stable/tutorials/systems/"/><meta property="twitter:url" content="https://docs.sciml.ai/NeuralPDE/stable/tutorials/systems/"/><link rel="canonical" href="https://docs.sciml.ai/NeuralPDE/stable/tutorials/systems/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralPDE.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralPDE.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)</a></li><li><span class="tocitem">ODE PINN Tutorials</span><ul><li><a class="tocitem" href="../ode/">Introduction to NeuralPDE for ODEs</a></li><li><a class="tocitem" href="../Lotka_Volterra_BPINNs/">Bayesian PINNs for Coupled ODEs</a></li><li><a class="tocitem" href="../dae/">PINNs DAEs</a></li><li><a class="tocitem" href="../ode_parameter_estimation/">Parameter Estimation with PINNs for ODEs</a></li><li><a class="tocitem" href="../dgm/">Deep Galerkin Method</a></li></ul></li><li><span class="tocitem">PDE PINN Tutorials</span><ul><li><a class="tocitem" href="../pdesystem/">Introduction to NeuralPDE for PDEs</a></li><li><a class="tocitem" href="../low_level_2/">Bayesian PINNs for PDEs</a></li><li><a class="tocitem" href="../gpu/">Using GPUs</a></li><li class="is-active"><a class="tocitem" href>Defining Systems of PDEs</a><ul class="internal"><li><a class="tocitem" href="#Solution"><span>Solution</span></a></li><li><a class="tocitem" href="#Direct-Construction-via-symbolic_discretize"><span>Direct Construction via symbolic_discretize</span></a></li><li><a class="tocitem" href="#Solution-Representation"><span>Solution Representation</span></a></li></ul></li><li><a class="tocitem" href="../constraints/">Imposing Constraints</a></li><li><a class="tocitem" href="../low_level/">The symbolic_discretize Interface</a></li><li><a class="tocitem" href="../param_estim/">Optimising Parameters (Solving Inverse Problems)</a></li><li><a class="tocitem" href="../integro_diff/">Solving Integro Differential Equations</a></li><li><a class="tocitem" href="../neural_adapter/">Transfer Learning with Neural Adapter</a></li><li><a class="tocitem" href="../derivative_neural_network/">The Derivative Neural Network Approximation</a></li></ul></li><li><span class="tocitem">Extended Examples</span><ul><li><a class="tocitem" href="../../examples/wave/">1D Wave Equation with Dirichlet boundary conditions</a></li><li><a class="tocitem" href="../../examples/3rd/">ODE with a 3rd-Order Derivative</a></li><li><a class="tocitem" href="../../examples/ks/">Kuramoto–Sivashinsky equation</a></li><li><a class="tocitem" href="../../examples/heterogeneous/">PDEs with Dependent Variables on Heterogeneous Domains</a></li><li><a class="tocitem" href="../../examples/linear_parabolic/">Linear parabolic system of PDEs</a></li><li><a class="tocitem" href="../../examples/nonlinear_elliptic/">Nonlinear elliptic system of PDEs</a></li><li><a class="tocitem" href="../../examples/nonlinear_hyperbolic/">Nonlinear hyperbolic system of PDEs</a></li><li><a class="tocitem" href="../../examples/complex/">Complex Equations with PINNs</a></li></ul></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../../manual/ode/">ODE-Specialized Physics-Informed Neural Network (PINN) Solver</a></li><li><a class="tocitem" href="../../manual/dae/">Differential Algebraic Equation Specialized Physics-Informed Neural Solver</a></li><li><a class="tocitem" href="../../manual/pinns/"><code>PhysicsInformedNN</code> Discretizer for PDESystems</a></li><li><a class="tocitem" href="../../manual/bpinns/"><code>BayesianPINN</code> Discretizer for PDESystems</a></li><li><a class="tocitem" href="../../manual/training_strategies/">Training Strategies</a></li><li><a class="tocitem" href="../../manual/adaptive_losses/">Adaptive Loss Functions</a></li><li><a class="tocitem" href="../../manual/logging/">Logging Utilities</a></li><li><a class="tocitem" href="../../manual/neural_adapters/">Transfer Learning with neural_adapter</a></li></ul></li><li><span class="tocitem">Developer Documentation</span><ul><li><a class="tocitem" href="../../developer/debugging/">Debugging PINN Solutions</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">PDE PINN Tutorials</a></li><li class="is-active"><a href>Defining Systems of PDEs</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Defining Systems of PDEs</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/NeuralPDE.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/NeuralPDE.jl/blob/master/docs/src/tutorials/systems.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="systems"><a class="docs-heading-anchor" href="#systems">Defining Systems of PDEs for Physics-Informed Neural Networks (PINNs)</a><a id="systems-1"></a><a class="docs-heading-anchor-permalink" href="#systems" title="Permalink"></a></h1><p>In this example, we will solve the PDE system:</p><p class="math-container">\[\begin{align*}
∂_t^2 u_1(t, x) &amp; = ∂_x^2 u_1(t, x) + u_3(t, x) \, \sin(\pi x) \, ,\\
∂_t^2 u_2(t, x) &amp; = ∂_x^2 u_2(t, x) + u_3(t, x) \, \cos(\pi x) \, ,\\
0 &amp; = u_1(t, x) \sin(\pi x) + u_2(t, x) \cos(\pi x) - e^{-t} \, ,
\end{align*}\]</p><p>with the initial conditions:</p><p class="math-container">\[\begin{align*}
u_1(0, x) &amp; = \sin(\pi x) \, ,\\
∂_t u_1(0, x) &amp; = - \sin(\pi x) \, ,\\
u_2(0, x) &amp; = \cos(\pi x) \, ,\\
∂_t u_2(0, x) &amp; = - \cos(\pi x) \, ,
\end{align*}\]</p><p>and the boundary conditions:</p><p class="math-container">\[\begin{align*}
u_1(t, 0) &amp; = u_1(t, 1) = 0 \, ,\\
u_2(t, 0) &amp; = - u_2(t, 1) = e^{-t} \, ,
\end{align*}\]</p><p>with physics-informed neural networks.</p><h2 id="Solution"><a class="docs-heading-anchor" href="#Solution">Solution</a><a id="Solution-1"></a><a class="docs-heading-anchor-permalink" href="#Solution" title="Permalink"></a></h2><pre><code class="language-julia hljs">using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, LineSearches,
      OptimizationOptimisers
using ModelingToolkit: Interval, infimum, supremum

@parameters t, x
@variables u1(..), u2(..), u3(..)
Dt = Differential(t)
Dtt = Differential(t)^2
Dx = Differential(x)
Dxx = Differential(x)^2

eqs = [
    Dtt(u1(t, x)) ~ Dxx(u1(t, x)) + u3(t, x) * sinpi(x),
    Dtt(u2(t, x)) ~ Dxx(u2(t, x)) + u3(t, x) * cospi(x),
    0.0 ~ u1(t, x) * sinpi(x) + u2(t, x) * cospi(x) - exp(-t)
]

bcs = [
    u1(0, x) ~ sinpi(x),
    u2(0, x) ~ cospi(x),
    Dt(u1(0, x)) ~ -sinpi(x),
    Dt(u2(0, x)) ~ -cospi(x),
    u1(t, 0) ~ 0.0,
    u2(t, 0) ~ exp(-t),
    u1(t, 1) ~ 0.0,
    u2(t, 1) ~ -exp(-t)
]

# Space and time domains
domains = [t ∈ Interval(0.0, 1.0), x ∈ Interval(0.0, 1.0)]

# Neural network
input_ = length(domains)
n = 15
chain = [Chain(Dense(input_, n, σ), Dense(n, n, σ), Dense(n, 1)) for _ in 1:3]

strategy = StochasticTraining(128)
discretization = PhysicsInformedNN(chain, strategy)

@named pdesystem = PDESystem(eqs, bcs, domains, [t, x], [u1(t, x), u2(t, x), u3(t, x)])
prob = discretize(pdesystem, discretization)
sym_prob = symbolic_discretize(pdesystem, discretization)

pde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions
bcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions

callback = function (p, l)
    println(&quot;loss: &quot;, l)
    println(&quot;pde_losses: &quot;, map(l_ -&gt; l_(p.u), pde_inner_loss_functions))
    println(&quot;bcs_losses: &quot;, map(l_ -&gt; l_(p.u), bcs_inner_loss_functions))
    return false
end

res = solve(prob, OptimizationOptimisers.Adam(0.01); maxiters = 1000, callback)
prob = remake(prob, u0 = res.u)
res = solve(prob, LBFGS(linesearch = BackTracking()); maxiters = 200, callback)
phi = discretization.phi</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{NeuralPDE.Phi{Lux.StatefulLuxLayer{Static.True, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}}}:
 NeuralPDE.Phi{Lux.StatefulLuxLayer{Static.True, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}}(Lux.StatefulLuxLayer{Static.True, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}(Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(2 =&gt; 15, σ), layer_2 = Dense(15 =&gt; 15, σ), layer_3 = Dense(15 =&gt; 1)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple()), nothing, static(true)))
 NeuralPDE.Phi{Lux.StatefulLuxLayer{Static.True, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}}(Lux.StatefulLuxLayer{Static.True, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}(Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(2 =&gt; 15, σ), layer_2 = Dense(15 =&gt; 15, σ), layer_3 = Dense(15 =&gt; 1)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple()), nothing, static(true)))
 NeuralPDE.Phi{Lux.StatefulLuxLayer{Static.True, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}}(Lux.StatefulLuxLayer{Static.True, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}(Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(2 =&gt; 15, σ), layer_2 = Dense(15 =&gt; 15, σ), layer_3 = Dense(15 =&gt; 1)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple()), nothing, static(true)))</code></pre><h2 id="Direct-Construction-via-symbolic_discretize"><a class="docs-heading-anchor" href="#Direct-Construction-via-symbolic_discretize">Direct Construction via symbolic_discretize</a><a id="Direct-Construction-via-symbolic_discretize-1"></a><a class="docs-heading-anchor-permalink" href="#Direct-Construction-via-symbolic_discretize" title="Permalink"></a></h2><p>One can take apart the pieces and reassemble the loss functions using the <code>symbolic_discretize</code> interface. Here is an example using the components from <code>symbolic_discretize</code> to fully reproduce the <code>discretize</code> optimization:</p><pre><code class="language-julia hljs">pde_loss_functions = sym_prob.loss_functions.pde_loss_functions
bc_loss_functions = sym_prob.loss_functions.bc_loss_functions

loss_functions = [pde_loss_functions; bc_loss_functions]

loss_function(θ, _) = sum(l -&gt; l(θ), loss_functions)

f_ = OptimizationFunction(loss_function, AutoZygote())
prob = OptimizationProblem(f_, sym_prob.flat_init_params)

res = solve(prob, OptimizationOptimisers.Adam(0.01); maxiters = 1000, callback)
prob = remake(prob, u0 = res.u)
res = solve(prob, LBFGS(linesearch = BackTracking()); maxiters = 200, callback)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Failure
u: ComponentVector{Float64}(depvar = (u1 = (layer_1 = (weight = [-0.5576878919021959 1.4407975565858657; 1.3164502150190838 -0.7099642305801231; … ; -1.1439949678376087 0.5714805058565923; -0.12975653610061885 2.596525666214641], bias = [-1.017451236325104, 0.4747234672679025, 0.29696921319599207, 0.5997220236187438, -1.0363426919218182, 0.3484239402908909, 0.20559391937991126, 0.10754986399126201, -0.8036021724613254, -2.019673670165243, 1.0710438926310777, 1.379379360845833, 0.3005548618694266, -0.30277267030719973, -2.018314231574293]), layer_2 = (weight = [-0.07890979933209866 0.27424327459546644 … -0.038763129727218584 -0.25232456502608513; 0.7243223806508969 -1.1217855410079351 … -0.019021047629746452 0.3774229069145645; … ; -0.4196193964967584 0.09467891597839861 … -0.34767153519908817 -0.5810826429231664; -0.6959658512290368 0.5998632098219424 … -0.4863035476169601 -0.17091531470820134], bias = [0.07223283518131637, -0.0083795118391743, -0.21504289368080304, 0.23057805628074304, -0.00713435423045231, -0.20118136094752986, 0.0772453760047201, 0.27672607424475554, -0.14249217741923056, 0.4611255031340121, -0.011944110458446977, 0.033230743289982184, 0.2406852160702317, -0.014777325203380627, -0.06172030292491185]), layer_3 = (weight = [-0.5801610291753542 -0.7598946501477503 … 0.6595167432397635 0.9287730152835405], bias = [0.2490134792606935])), u2 = (layer_1 = (weight = [-2.633128656058501 0.0540417003936067; 2.208346227194978 0.09919991593117042; … ; 0.20746100857778418 -3.507236065748556; 1.0812604363099414 -2.0872754578569306], bias = [-1.422209637825651, 0.15997187535037397, -0.6065977354328379, 0.47779904210534235, 0.3662671272667102, 0.007268311381223781, 0.263105607259393, -0.22286033167871988, -0.750727455677491, -0.46245184513088805, -0.3601084898677611, 0.9848251690841575, 1.222432754039122, -1.0323214891968469, -0.5847807898343946]), layer_2 = (weight = [0.349611686054417 -0.4814725876285577 … -0.4191056483657446 -0.2572504068153205; -1.7664365333837522 1.1553893440823322 … 0.3214980752076947 0.8025864920027432; … ; -1.1775834609284117 -0.21396409201157632 … 1.1137564333718895 0.5649160662943016; 0.29472240183014425 0.18777632755481546 … 0.5828154845718883 0.9723474720611308], bias = [0.01683244744875473, 0.36149025209060753, 0.14857691410884263, -0.4760429567854276, -0.21371647418540704, 0.3460232495486037, -0.022924411897686302, 0.2689961465124483, -0.051633021717189094, 0.20147509362938934, -0.026883959230079246, 0.29905435304538774, -0.2642993986917438, -0.22991562957612904, 0.09058455615150608]), layer_3 = (weight = [-0.7565086635949251 0.7722141231294765 … 0.6515535085135458 -0.9282658384897967], bias = [-0.540307638551009])), u3 = (layer_1 = (weight = [1.5457294434829394 1.0575623423279268; -2.4535199450609557 -1.0647836826149226; … ; -1.9099238259012299 -0.9583113484687759; 2.702476774390216 -0.3863792780261057], bias = [-0.08073876677504647, 0.501419841827919, 0.3811486583616248, 0.623073318326144, 0.15770357412808364, 0.7115456971141673, 0.5239870097943358, -0.0958615752038661, -0.17891032500865656, 1.0022431083360384, 0.4809577999766399, -0.3447744201628915, 0.21473726781097713, 0.059252636461642146, -0.37736460701322344]), layer_2 = (weight = [-0.5113351315456461 1.3304413241877096 … 0.9089037950027808 -0.0849799018753247; -0.7301688734732102 1.0032267714633334 … 0.9770965588771715 -0.7077230837352347; … ; -0.04252493312991516 -0.19372913925177002 … -0.5740991891971788 -0.04433986460755901; 0.08440795654537099 0.34283632941348374 … 0.7081159041504222 -0.15341449746187505], bias = [-0.2881205967257246, 0.01357615805851734, 0.13245439105289888, 0.1039665347126024, -0.12744621195052233, 0.04649944514337063, 0.1019004663264618, 0.13644841506729022, -0.09116569166150011, -0.10598070625520495, -0.3974833162005099, -0.06271295020199173, 0.13291123000277805, 0.06550575027321343, 0.23520332527929774]), layer_3 = (weight = [1.0927245605301532 1.1195951424405457 … -0.20001657727918473 0.8651338788682327], bias = [0.5924540525935941]))))</code></pre><h2 id="Solution-Representation"><a class="docs-heading-anchor" href="#Solution-Representation">Solution Representation</a><a id="Solution-Representation-1"></a><a class="docs-heading-anchor-permalink" href="#Solution-Representation" title="Permalink"></a></h2><p>Now let&#39;s perform some analysis for both the <code>symbolic_discretize</code> and <code>discretize</code> APIs:</p><pre><code class="language-julia hljs">using Plots

phi = discretization.phi
ts, xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]

minimizers_ = [res.u.depvar[sym_prob.depvars[i]] for i in 1:3]

function analytic_sol_func(t, x)
    [exp(-t) * sinpi(x), exp(-t) * cospi(x), (1 + pi^2) * exp(-t)]
end

u_real = [[analytic_sol_func(t, x)[i] for t in ts for x in xs] for i in 1:3]
u_predict = [[phi[i]([t, x], minimizers_[i])[1] for t in ts for x in xs] for i in 1:3]

diff_u = [abs.(u_real[i] .- u_predict[i]) for i in 1:3]
ps = []
for i in 1:3
    p1 = plot(ts, xs, u_real[i], linetype = :contourf, title = &quot;u$i, analytic&quot;)
    p2 = plot(ts, xs, u_predict[i], linetype = :contourf, title = &quot;predict&quot;)
    p3 = plot(ts, xs, diff_u[i], linetype = :contourf, title = &quot;error&quot;)
    push!(ps, plot(p1, p2, p3))
end</code></pre><pre><code class="language-julia hljs">ps[1]</code></pre><img src="e0866506.svg" alt="Example block output"/><pre><code class="language-julia hljs">ps[2]</code></pre><img src="09cf6881.svg" alt="Example block output"/><pre><code class="language-julia hljs">ps[3]</code></pre><img src="24095948.svg" alt="Example block output"/><p>Notice here that the solution is represented in the <code>OptimizationSolution</code> with <code>u</code> as the parameters for the trained neural network. But, for the case where the neural network is from jl, it&#39;s given as a <code>ComponentArray</code> where <code>res.u.depvar.x</code> corresponds to the result for the neural network corresponding to the dependent variable <code>x</code>, i.e. <code>res.u.depvar.u1</code> are the trained parameters for <code>phi[1]</code> in our example. For simpler indexing, you can use <code>res.u.depvar[:u1]</code> or <code>res.u.depvar[Symbol(:u,1)]</code> as shown here.</p><p>Subsetting the array also works, but is inelegant.</p><p>(If <code>param_estim == true</code>, then <code>res.u.p</code> are the fit parameters)</p><h4 id="Note:-Solving-Matrices-of-PDEs"><a class="docs-heading-anchor" href="#Note:-Solving-Matrices-of-PDEs">Note: Solving Matrices of PDEs</a><a id="Note:-Solving-Matrices-of-PDEs-1"></a><a class="docs-heading-anchor-permalink" href="#Note:-Solving-Matrices-of-PDEs" title="Permalink"></a></h4><p>Also, in addition to vector systems, we can use the matrix form of PDEs:</p><pre><code class="language-julia hljs">using ModelingToolkit, NeuralPDE
@parameters x y
@variables (u(..))[1:2, 1:2]
Dxx = Differential(x)^2
Dyy = Differential(y)^2

# Initial and boundary conditions
bcs = [u[1](x, 0) ~ x, u[2](x, 0) ~ 2, u[3](x, 0) ~ 3, u[4](x, 0) ~ 4]

# matrix PDE
eqs = @. [(Dxx(u_(x, y)) + Dyy(u_(x, y))) for u_ in u] ~ -sinpi(x) * sinpi(y) * [0 1; 0 1]

size(eqs)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../gpu/">« Using GPUs</a><a class="docs-footer-nextpage" href="../constraints/">Imposing Constraints »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Friday 18 October 2024 05:21">Friday 18 October 2024</span>. Using Julia version 1.11.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
