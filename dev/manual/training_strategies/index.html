<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Training Strategies · NeuralPDE.jl</title><meta name="title" content="Training Strategies · NeuralPDE.jl"/><meta property="og:title" content="Training Strategies · NeuralPDE.jl"/><meta property="twitter:title" content="Training Strategies · NeuralPDE.jl"/><meta name="description" content="Documentation for NeuralPDE.jl."/><meta property="og:description" content="Documentation for NeuralPDE.jl."/><meta property="twitter:description" content="Documentation for NeuralPDE.jl."/><meta property="og:url" content="https://docs.sciml.ai/NeuralPDE/stable/manual/training_strategies/"/><meta property="twitter:url" content="https://docs.sciml.ai/NeuralPDE/stable/manual/training_strategies/"/><link rel="canonical" href="https://docs.sciml.ai/NeuralPDE/stable/manual/training_strategies/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralPDE.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralPDE.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)</a></li><li><span class="tocitem">ODE PINN Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/ode/">Introduction to NeuralPDE for ODEs</a></li><li><a class="tocitem" href="../../tutorials/Lotka_Volterra_BPINNs/">Bayesian PINNs for Coupled ODEs</a></li><li><a class="tocitem" href="../../tutorials/dae/">PINNs DAEs</a></li><li><a class="tocitem" href="../../tutorials/ode_parameter_estimation/">Parameter Estimation with PINNs for ODEs</a></li><li><a class="tocitem" href="../../tutorials/data_collocation_Inverse/">Improved PINNs for Inverse problems in ODEs</a></li><li><a class="tocitem" href="../../tutorials/pino_ode/">Physics informed Neural Operator ODEs</a></li><li><a class="tocitem" href="../../tutorials/dgm/">Deep Galerkin Method</a></li></ul></li><li><span class="tocitem">PDE PINN Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/pdesystem/">Introduction to NeuralPDE for PDEs</a></li><li><a class="tocitem" href="../../tutorials/low_level_2/">Bayesian PINNs for PDEs</a></li><li><a class="tocitem" href="../../tutorials/gpu/">Using GPUs</a></li><li><a class="tocitem" href="../../tutorials/systems/">Defining Systems of PDEs</a></li><li><a class="tocitem" href="../../tutorials/constraints/">Imposing Constraints</a></li><li><a class="tocitem" href="../../tutorials/low_level/">The symbolic_discretize Interface</a></li><li><a class="tocitem" href="../../tutorials/param_estim/">Optimising Parameters (Solving Inverse Problems)</a></li><li><a class="tocitem" href="../../tutorials/integro_diff/">Solving Integro Differential Equations</a></li><li><a class="tocitem" href="../../tutorials/neural_adapter/">Transfer Learning with Neural Adapter</a></li><li><a class="tocitem" href="../../tutorials/derivative_neural_network/">The Derivative Neural Network Approximation</a></li></ul></li><li><span class="tocitem">Extended Examples</span><ul><li><a class="tocitem" href="../../examples/wave/">1D Wave Equation with Dirichlet boundary conditions</a></li><li><a class="tocitem" href="../../examples/3rd/">ODE with a 3rd-Order Derivative</a></li><li><a class="tocitem" href="../../examples/ks/">Kuramoto–Sivashinsky equation</a></li><li><a class="tocitem" href="../../examples/heterogeneous/">PDEs with Dependent Variables on Heterogeneous Domains</a></li><li><a class="tocitem" href="../../examples/linear_parabolic/">Linear parabolic system of PDEs</a></li><li><a class="tocitem" href="../../examples/nonlinear_elliptic/">Nonlinear elliptic system of PDEs</a></li><li><a class="tocitem" href="../../examples/nonlinear_hyperbolic/">Nonlinear hyperbolic system of PDEs</a></li><li><a class="tocitem" href="../../examples/complex/">Complex Equations with PINNs</a></li></ul></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../ode/">ODE-Specialized Physics-Informed Neural Network (PINN) Solver</a></li><li><a class="tocitem" href="../dae/">Differential Algebraic Equation Specialized Physics-Informed Neural Solver</a></li><li><a class="tocitem" href="../pinns/"><code>PhysicsInformedNN</code> Discretizer for PDESystems</a></li><li><a class="tocitem" href="../bpinns/"><code>BayesianPINN</code> Discretizer for PDESystems</a></li><li class="is-active"><a class="tocitem" href>Training Strategies</a><ul class="internal"><li><a class="tocitem" href="#Recommendations"><span>Recommendations</span></a></li><li><a class="tocitem" href="#API"><span>API</span></a></li></ul></li><li><a class="tocitem" href="../adaptive_losses/">Adaptive Loss Functions</a></li><li><a class="tocitem" href="../logging/">Logging Utilities</a></li><li><a class="tocitem" href="../neural_adapters/">Transfer Learning with neural_adapter</a></li><li><a class="tocitem" href="../pino_ode/">Physics-Informed Neural Operator (PINO) for ODEs</a></li></ul></li><li><span class="tocitem">Developer Documentation</span><ul><li><a class="tocitem" href="../../developer/debugging/">Debugging PINN Solutions</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>Training Strategies</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Training Strategies</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/NeuralPDE.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/NeuralPDE.jl/blob/master/docs/src/manual/training_strategies.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Training-Strategies"><a class="docs-heading-anchor" href="#Training-Strategies">Training Strategies</a><a id="Training-Strategies-1"></a><a class="docs-heading-anchor-permalink" href="#Training-Strategies" title="Permalink"></a></h1><p>Training strategies are the choices for how the points are sampled for the definition of the physics-informed loss.</p><h2 id="Recommendations"><a class="docs-heading-anchor" href="#Recommendations">Recommendations</a><a id="Recommendations-1"></a><a class="docs-heading-anchor-permalink" href="#Recommendations" title="Permalink"></a></h2><p><code>QuasiRandomTraining</code> with its default <code>LatinHyperCubeSample()</code> is a well-rounded training strategy which can be used for most situations. It scales well for high dimensional spaces and is GPU-compatible. <code>QuadratureTraining</code> can lead to faster or more robust convergence with one of the H-Cubature or P-Cubature methods, but are not currently GPU compatible. For very high dimensional cases, <code>QuadratureTraining</code> with an adaptive Monte Carlo quadrature method, such as <code>CubaVegas</code>, can be beneficial for difficult or stiff problems.</p><p><code>GridTraining</code> should only be used for testing purposes and should not be relied upon for real training cases. <code>StochasticTraining</code> achieves a lower convergence rate in the quasi-Monte Carlo methods and thus <code>QuasiRandomTraining</code> should be preferred in most cases. <code>WeightedIntervalTraining</code> can only be used with ODEs (<code>NNODE</code>).</p><h2 id="API"><a class="docs-heading-anchor" href="#API">API</a><a id="API-1"></a><a class="docs-heading-anchor-permalink" href="#API" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralPDE.GridTraining" href="#NeuralPDE.GridTraining"><code>NeuralPDE.GridTraining</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">GridTraining(dx)</code></pre><p>A training strategy that uses the grid points in a multidimensional grid with spacings <code>dx</code>. If the grid is multidimensional, then <code>dx</code> is expected to be an array of <code>dx</code> values matching the dimension of the domain, corresponding to the grid spacing in each dimension.</p><p><strong>Positional Arguments</strong></p><ul><li><code>dx</code>: the discretization of the grid.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralPDE.jl/blob/6792c03fb3b7a57db05cd2f167b8f9579ba7ce7d/src/training_strategies.jl#L1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralPDE.StochasticTraining" href="#NeuralPDE.StochasticTraining"><code>NeuralPDE.StochasticTraining</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">StochasticTraining(points; bcs_points = points)</code></pre><p><strong>Positional Arguments</strong></p><ul><li><code>points</code>: number of points in random select training set</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>bcs_points</code>: number of points in random select training set for boundary conditions (by default, it equals <code>points</code>).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralPDE.jl/blob/6792c03fb3b7a57db05cd2f167b8f9579ba7ce7d/src/training_strategies.jl#L142-L153">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralPDE.QuasiRandomTraining" href="#NeuralPDE.QuasiRandomTraining"><code>NeuralPDE.QuasiRandomTraining</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">QuasiRandomTraining(points; bcs_points = points,
                            sampling_alg = LatinHypercubeSample(), resampling = true,
                            minibatch = 0)</code></pre><p>A training strategy which uses quasi-Monte Carlo sampling for low discrepancy sequences that accelerate the convergence in high dimensional spaces over pure random sequences.</p><p><strong>Positional Arguments</strong></p><ul><li><code>points</code>:  the number of quasi-random points in a sample</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>bcs_points</code>: the number of quasi-random points in a sample for boundary conditions (by default, it equals <code>points</code>),</li><li><code>sampling_alg</code>: the quasi-Monte Carlo sampling algorithm,</li><li><code>resampling</code>: if it&#39;s false - the full training set is generated in advance before training, and at each iteration, one subset is randomly selected out of the batch. If it&#39;s true - the training set isn&#39;t generated beforehand, and one set of quasi-random points is generated directly at each iteration in runtime. In this case, <code>minibatch</code> has no effect.</li><li><code>minibatch</code>: the number of subsets, if <code>!resampling</code>.</li></ul><p>For more information, see <a href="https://docs.sciml.ai/QuasiMonteCarlo/stable/">QuasiMonteCarlo.jl</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralPDE.jl/blob/6792c03fb3b7a57db05cd2f167b8f9579ba7ce7d/src/training_strategies.jl#L195-L221">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralPDE.QuadratureTraining" href="#NeuralPDE.QuadratureTraining"><code>NeuralPDE.QuadratureTraining</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">QuadratureTraining(; quadrature_alg = CubatureJLh(), reltol = 1e-6, abstol = 1e-3,
                    maxiters = 1_000, batch = 100)</code></pre><p>A training strategy which treats the loss function as the integral of ||condition|| over the domain. Uses an Integrals.jl algorithm for computing the (adaptive) quadrature of this loss with respect to the chosen tolerances, with a batching <code>batch</code> corresponding to the maximum number of points to evaluate in a given integrand call.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>quadrature_alg</code>: quadrature algorithm,</li><li><code>reltol</code>: relative tolerance,</li><li><code>abstol</code>: absolute tolerance,</li><li><code>maxiters</code>: the maximum number of iterations in quadrature algorithm,</li><li><code>batch</code>: the preferred number of points to batch.</li></ul><p>For more information on the argument values and algorithm choices, see <a href="https://docs.sciml.ai/Integrals/stable/">Integrals.jl</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralPDE.jl/blob/6792c03fb3b7a57db05cd2f167b8f9579ba7ce7d/src/training_strategies.jl#L285-L305">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralPDE.WeightedIntervalTraining" href="#NeuralPDE.WeightedIntervalTraining"><code>NeuralPDE.WeightedIntervalTraining</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WeightedIntervalTraining(weights, samples)</code></pre><p>A training strategy that generates points for training based on the given inputs. We split the timespan into equal segments based on the number of weights, then sample points in each segment based on that segments corresponding weight, such that the total number of sampled points is equivalent to the given samples</p><p><strong>Positional Arguments</strong></p><ul><li><code>weights</code>: A vector of weights that should sum to 1, representing the proportion of samples at each interval.</li><li><code>points</code>: the total number of samples that we want, across the entire time span</li></ul><p><strong>Limitations</strong></p><p>This training strategy can only be used with ODEs (<code>NNODE</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/NeuralPDE.jl/blob/6792c03fb3b7a57db05cd2f167b8f9579ba7ce7d/src/training_strategies.jl#L360-L377">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../bpinns/">« <code>BayesianPINN</code> Discretizer for PDESystems</a><a class="docs-footer-nextpage" href="../adaptive_losses/">Adaptive Loss Functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.11.4 on <span class="colophon-date" title="Tuesday 27 May 2025 18:58">Tuesday 27 May 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
