<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Improved PINNs for Inverse problems in ODEs · NeuralPDE.jl</title><meta name="title" content="Improved PINNs for Inverse problems in ODEs · NeuralPDE.jl"/><meta property="og:title" content="Improved PINNs for Inverse problems in ODEs · NeuralPDE.jl"/><meta property="twitter:title" content="Improved PINNs for Inverse problems in ODEs · NeuralPDE.jl"/><meta name="description" content="Documentation for NeuralPDE.jl."/><meta property="og:description" content="Documentation for NeuralPDE.jl."/><meta property="twitter:description" content="Documentation for NeuralPDE.jl."/><meta property="og:url" content="https://docs.sciml.ai/NeuralPDE/stable/tutorials/data_collocation_Inverse/"/><meta property="twitter:url" content="https://docs.sciml.ai/NeuralPDE/stable/tutorials/data_collocation_Inverse/"/><link rel="canonical" href="https://docs.sciml.ai/NeuralPDE/stable/tutorials/data_collocation_Inverse/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralPDE.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralPDE.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)</a></li><li><span class="tocitem">ODE PINN Tutorials</span><ul><li><a class="tocitem" href="../ode/">Introduction to NeuralPDE for ODEs</a></li><li><a class="tocitem" href="../Lotka_Volterra_BPINNs/">Bayesian PINNs for Coupled ODEs</a></li><li><a class="tocitem" href="../dae/">PINNs DAEs</a></li><li><a class="tocitem" href="../ode_parameter_estimation/">Parameter Estimation with PINNs for ODEs</a></li><li class="is-active"><a class="tocitem" href>Improved PINNs for Inverse problems in ODEs</a></li><li><a class="tocitem" href="../pino_ode/">Physics informed Neural Operator ODEs</a></li><li><a class="tocitem" href="../dgm/">Deep Galerkin Method</a></li></ul></li><li><span class="tocitem">PDE PINN Tutorials</span><ul><li><a class="tocitem" href="../pdesystem/">Introduction to NeuralPDE for PDEs</a></li><li><a class="tocitem" href="../low_level_2/">Bayesian PINNs for PDEs</a></li><li><a class="tocitem" href="../gpu/">Using GPUs</a></li><li><a class="tocitem" href="../systems/">Defining Systems of PDEs</a></li><li><a class="tocitem" href="../constraints/">Imposing Constraints</a></li><li><a class="tocitem" href="../low_level/">The symbolic_discretize Interface</a></li><li><a class="tocitem" href="../param_estim/">Optimising Parameters (Solving Inverse Problems)</a></li><li><a class="tocitem" href="../integro_diff/">Solving Integro Differential Equations</a></li><li><a class="tocitem" href="../neural_adapter/">Transfer Learning with Neural Adapter</a></li><li><a class="tocitem" href="../derivative_neural_network/">The Derivative Neural Network Approximation</a></li></ul></li><li><span class="tocitem">Extended Examples</span><ul><li><a class="tocitem" href="../../examples/wave/">1D Wave Equation with Dirichlet boundary conditions</a></li><li><a class="tocitem" href="../../examples/3rd/">ODE with a 3rd-Order Derivative</a></li><li><a class="tocitem" href="../../examples/ks/">Kuramoto–Sivashinsky equation</a></li><li><a class="tocitem" href="../../examples/heterogeneous/">PDEs with Dependent Variables on Heterogeneous Domains</a></li><li><a class="tocitem" href="../../examples/linear_parabolic/">Linear parabolic system of PDEs</a></li><li><a class="tocitem" href="../../examples/nonlinear_elliptic/">Nonlinear elliptic system of PDEs</a></li><li><a class="tocitem" href="../../examples/nonlinear_hyperbolic/">Nonlinear hyperbolic system of PDEs</a></li><li><a class="tocitem" href="../../examples/complex/">Complex Equations with PINNs</a></li></ul></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../../manual/ode/">ODE-Specialized Physics-Informed Neural Network (PINN) Solver</a></li><li><a class="tocitem" href="../../manual/dae/">Differential Algebraic Equation Specialized Physics-Informed Neural Solver</a></li><li><a class="tocitem" href="../../manual/pinns/"><code>PhysicsInformedNN</code> Discretizer for PDESystems</a></li><li><a class="tocitem" href="../../manual/bpinns/"><code>BayesianPINN</code> Discretizer for PDESystems</a></li><li><a class="tocitem" href="../../manual/training_strategies/">Training Strategies</a></li><li><a class="tocitem" href="../../manual/adaptive_losses/">Adaptive Loss Functions</a></li><li><a class="tocitem" href="../../manual/logging/">Logging Utilities</a></li><li><a class="tocitem" href="../../manual/neural_adapters/">Transfer Learning with neural_adapter</a></li><li><a class="tocitem" href="../../manual/pino_ode/">Physics-Informed Neural Operator (PINO) for ODEs</a></li></ul></li><li><span class="tocitem">Developer Documentation</span><ul><li><a class="tocitem" href="../../developer/debugging/">Debugging PINN Solutions</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">ODE PINN Tutorials</a></li><li class="is-active"><a href>Improved PINNs for Inverse problems in ODEs</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Improved PINNs for Inverse problems in ODEs</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/NeuralPDE.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/NeuralPDE.jl/blob/master/docs/src/tutorials/data_collocation_Inverse.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Model-Improvement-in-Physics-Informed-Neural-Networks-for-solving-Inverse-problems-in-ODEs."><a class="docs-heading-anchor" href="#Model-Improvement-in-Physics-Informed-Neural-Networks-for-solving-Inverse-problems-in-ODEs.">Model Improvement in Physics-Informed Neural Networks for solving Inverse problems in ODEs.</a><a id="Model-Improvement-in-Physics-Informed-Neural-Networks-for-solving-Inverse-problems-in-ODEs.-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Improvement-in-Physics-Informed-Neural-Networks-for-solving-Inverse-problems-in-ODEs." title="Permalink"></a></h1><p>Consider an Inverse problem setting for the  <a href="https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations">lotka volterra system</a>. Here we want to optimize parameters <span>$\alpha$</span>, <span>$\beta$</span>, <span>$\gamma$</span> and <span>$\delta$</span> and also solve a parametric Lotka Volterra system. PINNs are especially useful in these types of problems and are preferred over conventional solvers, due to their ability to learn from observations - the underlying physics governing the distribution of observations.</p><p>We start by defining the problem, with a random and non informative initialization for parameters:</p><pre><code class="language-julia hljs">using NeuralPDE, OrdinaryDiffEq, Lux, Random, OptimizationOptimJL, LineSearches,
      Distributions, Plots
using FastGaussQuadrature

function lv(u, p, t)
    u₁, u₂ = u
    α, β, γ, δ = p
    du₁ = α * u₁ - β * u₁ * u₂
    du₂ = δ * u₁ * u₂ - γ * u₂
    [du₁, du₂]
end

tspan = (0.0, 5.0)
u0 = [5.0, 5.0]
initialization = [-5.0, 8.0, 5.0, -7.0]
prob = ODEProblem(lv, u0, tspan, initialization)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi"><span class="sgr38_2" style="color:#56b6c2">ODEProblem</span> with uType <span class="sgr38_2" style="color:#56b6c2">Vector{Float64}</span> and tType <span class="sgr38_2" style="color:#56b6c2">Float64</span>. In-place: <span class="sgr38_2" style="color:#56b6c2">false</span>
Non-trivial mass matrix: <span class="sgr38_2" style="color:#56b6c2">false</span>
timespan: (0.0, 5.0)
u0: 2-element Vector{Float64}:
 5.0
 5.0</code></pre><p>We require a set of observations before we train the PINN. Considering we want robust results even for cases where measurement values are sparse and limited in number. We simulate a system that uses the true parameter <code>true_p</code> values and record phenomena/solution (<code>u</code>) values algorithmically at only <code>N=20</code> pre-decided timepoints in the system&#39;s time domain.</p><p>The value for <code>N</code> can be incremented based on the non linearity (~ <code>N</code> degree polynomial) in the measured phenomenon, this tutorial&#39;s setting shows that even with minimal but systematically chosen data-points we can extract excellent results.</p><pre><code class="language-julia hljs">true_p = [1.5, 1.0, 3.0, 1.0]
prob_data = remake(prob, p = true_p)

N = 20
x, w = gausslobatto(N)
a = tspan[1]
b = tspan[2]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5.0</code></pre><p>Now scale the weights and the gauss-lobatto/clenshaw-curtis/gauss-legendre quadrature points to fit in <code>tspan</code>.</p><pre><code class="language-julia hljs">t = map((x) -&gt; (x * (b - a) + (b + a)) / 2, x)
W = map((x) -&gt; x * (b - a) / 2, w)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">20-element Vector{Float64}:
 0.013157894736842105
 0.08059280797122265
 0.1429545053189173
 0.20157940999029922
 0.2549787492486272
 0.3017730690716866
 0.34075120589681046
 0.3709038851772921
 0.3914502566186888
 0.4018582159696143
 0.4018582159696143
 0.3914502566186888
 0.3709038851772921
 0.34075120589681046
 0.3017730690716866
 0.2549787492486272
 0.20157940999029922
 0.1429545053189173
 0.08059280797122265
 0.013157894736842105</code></pre><p>We now have our dataset of <code>20</code> measurements in our <code>tspan</code> and corresponding weights. Using this we can now use the Data Quadrature loss function by passing <code>estim_collocate</code> = <code>true</code> in <a href="../../manual/ode/#NeuralPDE.NNODE"><code>NNODE</code></a>.</p><pre><code class="language-julia hljs">sol_data = solve(prob_data, Tsit5(); saveat = t)
t_ = sol_data.t
u_ = sol_data.u
u1_ = [u_[i][1] for i in eachindex(t_)]
u2_ = [u_[i][2] for i in eachindex(t_)]
dataset = [u1_, u2_, t_, W]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">4-element Vector{Vector{Float64}}:
 [5.0, 4.181603611078826, 2.645397810891132, 1.3912498015703647, 0.8210526163681184, 0.6722098478776918, 0.7611053094812956, 1.0772907174319215, 1.7299869127510388, 2.9470253265117505, 5.049007710901019, 7.796206693042137, 5.5936246049244325, 1.4464064682616247, 0.7465251501572414, 0.6800898950063358, 0.7600175010829118, 0.8812968395300225, 0.9887672013131465, 1.0424843918922717]
 [5.0, 5.396689381837423, 5.620089682747766, 4.668051098875351, 2.9860691512818804, 1.5808704215183293, 0.7539189127225489, 0.3564866998395664, 0.19132741119848423, 0.14380368289069814, 0.20915289174224483, 0.8200396835925662, 4.616390757346223, 4.732955149243002, 2.4829239251518236, 1.3047202433563063, 0.7730246691232083, 0.5302342006306647, 0.42063858049750924, 0.38230716879691223]
 [0.0, 0.04814073776521477, 0.1601637529683364, 0.33280505477512445, 0.5615793476198603, 0.8405589942742218, 1.1625178399202842, 1.5191170407152268, 1.9011207351925337, 2.2986351569029453, 2.7013648430970547, 3.0988792648074663, 3.480882959284773, 3.8374821600797158, 4.159441005725778, 4.43842065238014, 4.6671949452248755, 4.839836247031664, 4.951859262234786, 5.0]
 [0.013157894736842105, 0.08059280797122265, 0.1429545053189173, 0.20157940999029922, 0.2549787492486272, 0.3017730690716866, 0.34075120589681046, 0.3709038851772921, 0.3914502566186888, 0.4018582159696143, 0.4018582159696143, 0.3914502566186888, 0.3709038851772921, 0.34075120589681046, 0.3017730690716866, 0.2549787492486272, 0.20157940999029922, 0.1429545053189173, 0.08059280797122265, 0.013157894736842105]</code></pre><p>Now, let&#39;s define a neural network for the PINN using <a href="https://lux.csail.mit.edu/">Lux.jl</a>.</p><pre><code class="language-julia hljs">rng = Random.default_rng()
Random.seed!(rng, 0)
n = 7
chain = Chain(Dense(1, n, tanh), Dense(n, n, tanh), Dense(n, 2))
ps, st = Lux.setup(rng, chain) |&gt; f64</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">((layer_1 = (weight = [-0.08216114342212677; -0.5444445610046387; … ; 2.0908210277557373; -1.323716640472412;;], bias = [-0.38509929180145264, 0.32322537899017334, -0.32623517513275146, -0.7673453092575073, 0.7302734851837158, -0.7812288999557495, -0.16844713687896729]), layer_2 = (weight = [-0.857661783695221 0.07281388342380524 … -0.22586384415626526 0.7762117981910706; -0.1741035431623459 0.4331861138343811 … 0.36346158385276794 1.0857038497924805; … ; -0.1376570612192154 0.8151324391365051 … 0.21547959744930267 -0.17118430137634277; 0.8868374824523926 -0.2730417251586914 … 1.0372037887573242 0.9000675082206726], bias = [-0.3254864513874054, 0.2545594573020935, 0.3401867151260376, 0.3638579249382019, 0.27597615122795105, -0.07990346103906631, -0.2069253772497177]), layer_3 = (weight = [-0.43004998564720154 0.1675717830657959 … 0.43563809990882874 0.16299796104431152; 0.17348821461200714 -0.5971973538398743 … -0.4388984143733978 0.24340596795082092], bias = [-0.12740696966648102, 0.10087139904499054])), (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple()))</code></pre><div class="admonition is-info" id="Note-975bb418a86919e7"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-975bb418a86919e7" title="Permalink"></a></header><div class="admonition-body"><p>While solving Inverse problems, when we specify <code>param_estim = true</code> in <a href="../../manual/ode/#NeuralPDE.NNODE"><code>NNODE</code></a> or <a href="../../manual/ode/#NeuralPDE.BNNODE"><code>BNNODE</code></a>, an L2 loss function measuring how the neural network&#39;s predictions fit the provided <code>dataset</code> is used internally during Maximum Likelihood Estimation. Therefore, the <code>additional_loss</code> mentioned in the <a href="https://docs.sciml.ai/NeuralPDE/stable/tutorials/ode_parameter_estimation/">ODE parameter estimation tutorial</a> is not limited to an L2 loss function against data.</p></div></div><p>We now define the optimizer and <a href="../../manual/ode/#NeuralPDE.NNODE"><code>NNODE</code></a> - the ODE solving PINN algorithm, for the old PINN model and the proposed new PINN formulation which uses a Data Quadrature loss. This optimizer and respective algorithms are plugged into the <code>solve</code> calls for comparing results between the new and old PINN models.</p><pre><code class="language-julia hljs">opt = LBFGS(linesearch = BackTracking())

alg_old = NNODE(
    chain, opt; strategy = GridTraining(0.01), dataset = dataset, param_estim = true)

alg_new = NNODE(chain, opt; strategy = GridTraining(0.01), param_estim = true,
    dataset = dataset, estim_collocate = true)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">NNODE{Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(tanh), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(tanh), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Optim.LBFGS{Nothing, LineSearches.InitialStatic{Float64}, LineSearches.BackTracking{Float64, Int64}, Returns{Nothing}}, Nothing, Bool, GridTraining{Float64}, Bool, Nothing, Vector{Vector{Float64}}, Base.Pairs{Symbol, Union{}, Tuple{}, @NamedTuple{}}}(Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(tanh), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(tanh), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(1 =&gt; 7, tanh), layer_2 = Dense(7 =&gt; 7, tanh), layer_3 = Dense(7 =&gt; 2)), nothing), Optim.LBFGS{Nothing, LineSearches.InitialStatic{Float64}, LineSearches.BackTracking{Float64, Int64}, Returns{Nothing}}(10, LineSearches.InitialStatic{Float64}
  alpha: Float64 1.0
  scaled: Bool false
, LineSearches.BackTracking{Float64, Int64}
  c_1: Float64 0.0001
  ρ_hi: Float64 0.5
  ρ_lo: Float64 0.1
  iterations: Int64 1000
  order: Int64 3
  maxstep: Float64 Inf
  cache: Nothing nothing
, nothing, Returns{Nothing}(nothing), Optim.Flat(), true), nothing, false, true, GridTraining{Float64}(0.01), true, nothing, [[5.0, 4.181603611078826, 2.645397810891132, 1.3912498015703647, 0.8210526163681184, 0.6722098478776918, 0.7611053094812956, 1.0772907174319215, 1.7299869127510388, 2.9470253265117505, 5.049007710901019, 7.796206693042137, 5.5936246049244325, 1.4464064682616247, 0.7465251501572414, 0.6800898950063358, 0.7600175010829118, 0.8812968395300225, 0.9887672013131465, 1.0424843918922717], [5.0, 5.396689381837423, 5.620089682747766, 4.668051098875351, 2.9860691512818804, 1.5808704215183293, 0.7539189127225489, 0.3564866998395664, 0.19132741119848423, 0.14380368289069814, 0.20915289174224483, 0.8200396835925662, 4.616390757346223, 4.732955149243002, 2.4829239251518236, 1.3047202433563063, 0.7730246691232083, 0.5302342006306647, 0.42063858049750924, 0.38230716879691223], [0.0, 0.04814073776521477, 0.1601637529683364, 0.33280505477512445, 0.5615793476198603, 0.8405589942742218, 1.1625178399202842, 1.5191170407152268, 1.9011207351925337, 2.2986351569029453, 2.7013648430970547, 3.0988792648074663, 3.480882959284773, 3.8374821600797158, 4.159441005725778, 4.43842065238014, 4.6671949452248755, 4.839836247031664, 4.951859262234786, 5.0], [0.013157894736842105, 0.08059280797122265, 0.1429545053189173, 0.20157940999029922, 0.2549787492486272, 0.3017730690716866, 0.34075120589681046, 0.3709038851772921, 0.3914502566186888, 0.4018582159696143, 0.4018582159696143, 0.3914502566186888, 0.3709038851772921, 0.34075120589681046, 0.3017730690716866, 0.2549787492486272, 0.20157940999029922, 0.1429545053189173, 0.08059280797122265, 0.013157894736842105]], true, Base.Pairs{Symbol, Union{}, Tuple{}, @NamedTuple{}}())</code></pre><p>Now we have all the pieces to solve the optimization problem.</p><pre><code class="language-julia hljs">sol_old = solve(
    prob, alg_old; verbose = true, abstol = 1e-12, maxiters = 5000, saveat = 0.01)

sol_new = solve(
    prob, alg_new; verbose = true, abstol = 1e-12, maxiters = 5000, saveat = 0.01)

sol = solve(prob_data, Tsit5(); saveat = 0.01)
sol_points = hcat(sol.u...)
sol_old_points = hcat(sol_old.u...)
sol_new_points = hcat(sol_new.u...)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×501 Matrix{Float64}:
 5.0  4.81778  4.64051  4.46815  4.30067  …  0.99151   0.995239  0.99885
 5.0  5.10393  5.1993   5.28594  5.36376     0.352015  0.339185  0.326444</code></pre><p>Let&#39;s plot the predictions from the PINN models, data used and compare it to the ideal system solution. First the old model.</p><pre><code class="language-julia hljs">plot(sol, labels = [&quot;u1&quot; &quot;u2&quot;])
plot!(sol_old, labels = [&quot;u1_pinn_old&quot; &quot;u2_pinn_old&quot;])
scatter!(sol_data, labels = [&quot;u1_data&quot; &quot;u2_data&quot;])</code></pre><img src="093be66d.svg" alt="Example block output"/><p>Clearly the old model cannot optimize given a realistic, tougher initialization of parameters especially with such limited data. It only seems to work when initial values are close to <code>true_p</code> and we have around <code>500</code> points for our <code>tspan</code>, as seen in the <a href="https://docs.sciml.ai/NeuralPDE/stable/tutorials/ode_parameter_estimation/">ODE parameter estimation tutorial</a>.</p><p>Lets move on to the proposed new model...</p><pre><code class="language-julia hljs">plot(sol, labels = [&quot;u1&quot; &quot;u2&quot;])
plot!(sol_new, labels = [&quot;u1_pinn_new&quot; &quot;u2_pinn_new&quot;])
scatter!(sol_data, labels = [&quot;u1_data&quot; &quot;u2_data&quot;])</code></pre><img src="e1c2455b.svg" alt="Example block output"/><p>We can see that it is a good fit! Now let&#39;s examine what the estimated parameters of the equation tell us in both cases. We also test for the following: the old model&#39;s estimates have at least one parameter value deviating from it&#39;s true value by more than <code>50%</code> while all the new model&#39;s estimates must be within <code>2%</code> of the <code>true_p</code> values.</p><pre><code class="language-julia hljs">sol_old.k.u.p</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">4-element view(::Vector{Float64}, 87:90) with eltype Float64:
  0.7520099906689697
  0.5005725738602825
 -0.2649352840592679
 -0.32117749700843545</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi"><span class="sgr32"><span class="sgr1">Test Passed</span></span></code></pre><p>This is nowhere near the true [1.5, 1.0, 3.0, 1.0]. But the new model gives :</p><pre><code class="language-julia hljs">sol_new.k.u.p</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">4-element view(::Vector{Float64}, 87:90) with eltype Float64:
 1.4964284939955685
 0.9964274269956848
 3.0519881715421984
 1.020470905167828</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi"><span class="sgr32"><span class="sgr1">Test Passed</span></span></code></pre><p>This is indeed very close to the true ODE parameter values [1.5, 1.0, 3.0, 1.0].</p><div class="admonition is-info" id="Note-61e90971b2cc8364"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-61e90971b2cc8364" title="Permalink"></a></header><div class="admonition-body"><p>This feature for using a Data collocation loss is also available for BPINNs solving Inverse problems in ODEs. Use a <code>dataset</code> of the form as described in this tutorial and set <code>estim_collocate</code>=<code>true</code> and you are good to go.</p></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../ode_parameter_estimation/">« Parameter Estimation with PINNs for ODEs</a><a class="docs-footer-nextpage" href="../pino_ode/">Physics informed Neural Operator ODEs »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.11.4 on <span class="colophon-date" title="Thursday 29 May 2025 19:59">Thursday 29 May 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
