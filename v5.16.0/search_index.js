var documenterSearchIndex = {"docs":
[{"location":"manual/ode/#ODE-Specialized-Physics-Informed-Neural-Network-(PINN)-Solver","page":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","title":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","text":"","category":"section"},{"location":"manual/ode/","page":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","title":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","text":"NNODE","category":"page"},{"location":"manual/ode/#NeuralPDE.NNODE","page":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","title":"NeuralPDE.NNODE","text":"NNODE(chain, opt, init_params = nothing; autodiff = false, batch = 0, additional_loss = nothing, kwargs...)\n\nAlgorithm for solving ordinary differential equations using a neural network. This is a specialization of the physics-informed neural network which is used as a solver for a standard ODEProblem.\n\nwarn: Warn\nNote that NNODE only supports ODEs which are written in the out-of-place form, i.e. du = f(u,p,t), and not f(du,u,p,t). If not declared out-of-place, then the NNODE will exit with an error.\n\nPositional Arguments\n\nchain: A neural network architecture, defined as a Lux.AbstractExplicitLayer or Flux.Chain.         Flux.Chain will be converted to Lux using adapt(FromFluxAdaptor(false, false), chain).\nopt: The optimizer to train the neural network.\ninit_params: The initial parameter of the neural network. By default, this is nothing                which thus uses the random initialization provided by the neural network library.\n\nKeyword Arguments\n\nadditional_loss: A function additional_loss(phi, θ) where phi are the neural network trial solutions,                    θ are the weights of the neural network(s).\nautodiff: The switch between automatic and numerical differentiation for             the PDE operators. The reverse mode of the loss function is always             automatic differentiation (via Zygote), this is only for the derivative             in the loss function (the derivative with respect to time).\nbatch: The batch size for the loss computation. Defaults to true, means the neural network is applied at a row vector of values          t simultaneously, i.e. it's the batch size for the neural network evaluations. This requires a neural network compatible with batched data.          false means which means the application of the neural network is done at individual time points one at a time.          This is not applicable to QuadratureTraining where batch is passed in the strategy which is the number of points it can parallelly compute the integrand.\nparam_estim: Boolean to indicate whether parameters of the differential equations are learnt along with parameters of the neural network.\nstrategy: The training strategy used to choose the points for the evaluations.             Default of nothing means that QuadratureTraining with QuadGK is used if no             dt is given, and GridTraining is used with dt if given.\nkwargs: Extra keyword arguments are splatted to the Optimization.jl solve call.\n\nExamples\n\nu0 = [1.0, 1.0]\nts = [t for t in 1:100]\n(u_, t_) = (analytical_func(ts), ts)\nfunction additional_loss(phi, θ)\n    return sum(sum(abs2, [phi(t, θ) for t in t_] .- u_)) / length(u_)\nend\nalg = NNODE(chain, opt, additional_loss = additional_loss)\n\nf(u,p,t) = cos(2pi*t)\ntspan = (0.0, 1.0)\nu0 = 0.0\nprob = ODEProblem(linear, u0 ,tspan)\nchain = Lux.Chain(Lux.Dense(1, 5, Lux.σ), Lux.Dense(5, 1))\nopt = OptimizationOptimisers.Adam(0.1)\nsol = solve(prob, NNODE(chain, opt), verbose = true, abstol = 1e-10, maxiters = 200)\n\nSolution Notes\n\nNote that the solution is evaluated at fixed time points according to standard output handlers such as saveat and dt. However, the neural network is a fully continuous solution so sol(t) is an accurate interpolation (up to the neural network training result). In addition, the OptimizationSolution is returned as sol.k for further analysis.\n\nReferences\n\nLagaris, Isaac E., Aristidis Likas, and Dimitrios I. Fotiadis. \"Artificial neural networks for solving ordinary and partial differential equations.\" IEEE Transactions on Neural Networks 9, no. 5 (1998): 987-1000.\n\n\n\n\n\n","category":"type"},{"location":"manual/ode/#Bayesian-inference-with-PINNs","page":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","title":"Bayesian inference with PINNs","text":"","category":"section"},{"location":"manual/ode/","page":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","title":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","text":"BNNODE","category":"page"},{"location":"manual/ode/#NeuralPDE.BNNODE","page":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","title":"NeuralPDE.BNNODE","text":"BNNODE(chain, Kernel = HMC; strategy = nothing, draw_samples = 2000,\n                    priorsNNw = (0.0, 2.0), param = [nothing], l2std = [0.05],\n                    phystd = [0.05], dataset = [nothing], physdt = 1 / 20.0,\n                    MCMCargs = (n_leapfrog=30), nchains = 1, init_params = nothing,\n                    Adaptorkwargs = (Adaptor = StanHMCAdaptor, targetacceptancerate = 0.8, Metric = DiagEuclideanMetric),\n                    Integratorkwargs = (Integrator = Leapfrog,), autodiff = false,\n                    progress = false, verbose = false)\n\nAlgorithm for solving ordinary differential equations using a Bayesian neural network. This is a specialization of the physics-informed neural network which is used as a solver for a standard ODEProblem.\n\nwarn: Warn\nNote that BNNODE only supports ODEs which are written in the out-of-place form, i.e. du = f(u,p,t), and not f(du,u,p,t). If not declared out-of-place, then the BNNODE will exit with an error.\n\nPositional Arguments\n\nchain: A neural network architecture, defined as a Lux.AbstractExplicitLayer.\nKernel: Choice of MCMC Sampling Algorithm. Defaults to AdvancedHMC.HMC\n\nKeyword Arguments\n\n(refer NeuralPDE.ahmc_bayesian_pinn_ode keyword arguments.)\n\nExample\n\nlinear = (u, p, t) -> -u / p[1] + exp(t / p[2]) * cos(t)\ntspan = (0.0, 10.0)\nu0 = 0.0\np = [5.0, -5.0]\nprob = ODEProblem(linear, u0, tspan, p)\nlinear_analytic = (u0, p, t) -> exp(-t / 5) * (u0 + sin(t))\n\nsol = solve(prob, Tsit5(); saveat = 0.05)\nu = sol.u[1:100]\ntime = sol.t[1:100]\nx̂ = u .+ (u .* 0.2) .* randn(size(u))\ndataset = [x̂, time]\n\nchainlux = Lux.Chain(Lux.Dense(1, 6, tanh), Lux.Dense(6, 6, tanh), Lux.Dense(6, 1))\n\nalg = BNNODE(chainlux, draw_samples = 2000,\n                       l2std = [0.05], phystd = [0.05],\n                       priorsNNw = (0.0, 3.0), progress = true)\n\nsol_lux = solve(prob, alg)\n\n# with parameter estimation\nalg = BNNODE(chainlux,dataset = dataset,\n                draw_samples = 2000,l2std = [0.05],\n                phystd = [0.05],priorsNNw = (0.0, 10.0),\n                param = [Normal(6.5, 0.5), Normal(-3, 0.5)],\n                progress = true)\n\nsol_lux_pestim = solve(prob, alg)\n\nSolution Notes\n\nNote that the solution is evaluated at fixed time points according to the strategy chosen. ensemble solution is evaluated and given at steps of saveat. Dataset should only be provided when ODE parameter Estimation is being done. The neural network is a fully continuous solution so BPINNsolution is an accurate interpolation (up to the neural network training result). In addition, the BPINNstats is returned as sol.fullsolution for further analysis.\n\nReferences\n\nLiu Yanga, Xuhui Menga, George Em Karniadakis. \"B-PINNs: Bayesian Physics-Informed Neural Networks for Forward and Inverse PDE Problems with Noisy Data\".\n\nKevin Linka, Amelie Schäfer, Xuhui Meng, Zongren Zou, George Em Karniadakis, Ellen Kuhl \"Bayesian Physics Informed Neural Networks for real-world nonlinear dynamical systems\".\n\n\n\n\n\n","category":"type"},{"location":"tutorials/low_level/#Investigating-symbolic_discretize-with-the-PhysicsInformedNN-Discretizer-for-the-1-D-Burgers'-Equation","page":"The symbolic_discretize Interface","title":"Investigating symbolic_discretize with the PhysicsInformedNN Discretizer for the 1-D Burgers' Equation","text":"","category":"section"},{"location":"tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"Let's consider the Burgers' equation:","category":"page"},{"location":"tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"begingather*\n_t u + u _x u - (001  pi) _x^2 u = 0   quad x in -1 1 t in 0 1   \nu(0 x) = - sin(pi x)   \nu(t -1) = u(t 1) = 0  \nendgather*","category":"page"},{"location":"tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"with Physics-Informed Neural Networks. Here is an example of using the low-level API:","category":"page"},{"location":"tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, LineSearches\nusing ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u(..)\nDt = Differential(t)\nDx = Differential(x)\nDxx = Differential(x)^2\n\n#2D PDE\neq = Dt(u(t, x)) + u(t, x) * Dx(u(t, x)) - (0.01 / pi) * Dxx(u(t, x)) ~ 0\n\n# Initial and boundary conditions\nbcs = [u(0, x) ~ -sin(pi * x),\n    u(t, -1) ~ 0.0,\n    u(t, 1) ~ 0.0,\n    u(t, -1) ~ u(t, 1)]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, 1.0),\n    x ∈ Interval(-1.0, 1.0)]\n\n# Neural network\nchain = Lux.Chain(Dense(2, 16, Lux.σ), Dense(16, 16, Lux.σ), Dense(16, 1))\nstrategy = NeuralPDE.QuadratureTraining(; abstol = 1e-6, reltol = 1e-6, batch = 200)\n\nindvars = [t, x]\ndepvars = [u(t, x)]\n@named pde_system = PDESystem(eq, bcs, domains, indvars, depvars)\n\ndiscretization = PhysicsInformedNN(chain, strategy)\nsym_prob = symbolic_discretize(pde_system, discretization)\n\nphi = sym_prob.phi\n\npde_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbc_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p.u), pde_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p.u), bc_loss_functions))\n    return false\nend\n\nloss_functions = [pde_loss_functions; bc_loss_functions]\n\nfunction loss_function(θ, p)\n    sum(map(l -> l(θ), loss_functions))\nend\n\nf_ = OptimizationFunction(loss_function, Optimization.AutoZygote())\nprob = Optimization.OptimizationProblem(f_, sym_prob.flat_init_params)\n\nres = Optimization.solve(prob, BFGS(linesearch = BackTracking()); maxiters = 3000)","category":"page"},{"location":"tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"And some analysis:","category":"page"},{"location":"tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"using Plots\n\nts, xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\nu_predict_contourf = reshape([first(phi([t, x], res.u)) for t in ts for x in xs],\n    length(xs), length(ts))\nplot(ts, xs, u_predict_contourf, linetype = :contourf, title = \"predict\")\n\nu_predict = [[first(phi([t, x], res.u)) for x in xs] for t in ts]\np1 = plot(xs, u_predict[3], title = \"t = 0.1\");\np2 = plot(xs, u_predict[11], title = \"t = 0.5\");\np3 = plot(xs, u_predict[end], title = \"t = 1\");\nplot(p1, p2, p3)","category":"page"},{"location":"tutorials/dgm/#Solving-PDEs-using-Deep-Galerkin-Method","page":"Deep Galerkin Method","title":"Solving PDEs using Deep Galerkin Method","text":"","category":"section"},{"location":"tutorials/dgm/#Overview","page":"Deep Galerkin Method","title":"Overview","text":"","category":"section"},{"location":"tutorials/dgm/","page":"Deep Galerkin Method","title":"Deep Galerkin Method","text":"Deep Galerkin Method is a meshless deep learning algorithm to solve high dimensional PDEs. The algorithm does so by approximating the solution of a PDE with a neural network. The loss function of the network is defined in the similar spirit as PINNs, composed of PDE loss and boundary condition loss.","category":"page"},{"location":"tutorials/dgm/","page":"Deep Galerkin Method","title":"Deep Galerkin Method","text":"In the following example, we demonstrate computing the loss function using Quasi-Random Sampling, a sampling technique that uses quasi-Monte Carlo sampling to generate low discrepancy random sequences in high dimensional spaces.","category":"page"},{"location":"tutorials/dgm/#Algorithm","page":"Deep Galerkin Method","title":"Algorithm","text":"","category":"section"},{"location":"tutorials/dgm/","page":"Deep Galerkin Method","title":"Deep Galerkin Method","text":"The authors of DGM suggest a network composed of LSTM-type layers that works well for most of the parabolic and quasi-parabolic PDEs.","category":"page"},{"location":"tutorials/dgm/","page":"Deep Galerkin Method","title":"Deep Galerkin Method","text":"beginalign*\nS^1 = sigma_1(W^1 vecx + b^1) \nZ^l = sigma_1(U^zl vecx + W^zl S^l + b^zl) quad l = 1 ldots L \nG^l = sigma_1(U^gl vecx + W^gl S_l + b^gl) quad l = 1 ldots L \nR^l = sigma_1(U^rl vecx + W^rl S^l + b^rl) quad l = 1 ldots L \nH^l = sigma_2(U^hl vecx + W^hl(S^l cdot R^l) + b^hl) quad l = 1 ldots L \nS^l+1 = (1 - G^l) cdot H^l + Z^l cdot S^l quad l = 1 ldots L \nf(t x theta) = sigma_out(W S^L+1 + b)\nendalign*","category":"page"},{"location":"tutorials/dgm/","page":"Deep Galerkin Method","title":"Deep Galerkin Method","text":"where vecx is the concatenated vector of (t x) and L is the number of LSTM type layers in the network.","category":"page"},{"location":"tutorials/dgm/#Example","page":"Deep Galerkin Method","title":"Example","text":"","category":"section"},{"location":"tutorials/dgm/","page":"Deep Galerkin Method","title":"Deep Galerkin Method","text":"Let's try to solve the following Burger's equation using Deep Galerkin Method for alpha = 005 and compare our solution with the finite difference method:","category":"page"},{"location":"tutorials/dgm/","page":"Deep Galerkin Method","title":"Deep Galerkin Method","text":"partial_t u(t x) + u(t x) partial_x u(t x) - alpha partial_xx u(t x) = 0 ","category":"page"},{"location":"tutorials/dgm/","page":"Deep Galerkin Method","title":"Deep Galerkin Method","text":"defined over","category":"page"},{"location":"tutorials/dgm/","page":"Deep Galerkin Method","title":"Deep Galerkin Method","text":"t in 0 1 x in -1 1 ","category":"page"},{"location":"tutorials/dgm/","page":"Deep Galerkin Method","title":"Deep Galerkin Method","text":"with boundary conditions","category":"page"},{"location":"tutorials/dgm/","page":"Deep Galerkin Method","title":"Deep Galerkin Method","text":"beginalign*\nu(t x)  = - sin(πx) \nu(t -1)  = 0 \nu(t 1)  = 0\nendalign*","category":"page"},{"location":"tutorials/dgm/#Copy-Pasteable-code","page":"Deep Galerkin Method","title":"Copy- Pasteable code","text":"","category":"section"},{"location":"tutorials/dgm/","page":"Deep Galerkin Method","title":"Deep Galerkin Method","text":"using NeuralPDE\nusing ModelingToolkit, Optimization, OptimizationOptimisers\nusing Lux: tanh, identity\nusing Distributions\nusing ModelingToolkit: Interval, infimum, supremum\nusing MethodOfLines, OrdinaryDiffEq\nusing Plots\n\n@parameters x t\n@variables u(..)\n\nDt = Differential(t)\nDx = Differential(x)\nDxx = Dx^2\nα = 0.05\n# Burger's equation\neq = Dt(u(t, x)) + u(t, x) * Dx(u(t, x)) - α * Dxx(u(t, x)) ~ 0\n\n# boundary conditions\nbcs = [\n    u(0.0, x) ~ -sin(π * x),\n    u(t, -1.0) ~ 0.0,\n    u(t, 1.0) ~ 0.0\n]\n\ndomains = [t ∈ Interval(0.0, 1.0), x ∈ Interval(-1.0, 1.0)]\n\n# MethodOfLines, for FD solution\ndx = 0.01\norder = 2\ndiscretization = MOLFiniteDifference([x => dx], t, saveat = 0.01)\n@named pde_system = PDESystem(eq, bcs, domains, [t, x], [u(t, x)])\nprob = discretize(pde_system, discretization)\nsol = solve(prob, Tsit5())\nts = sol[t]\nxs = sol[x]\n\nu_MOL = sol[u(t, x)]\n\n# NeuralPDE, using Deep Galerkin Method\nstrategy = QuasiRandomTraining(256, minibatch = 32)\ndiscretization = DeepGalerkin(2, 1, 50, 5, tanh, tanh, identity, strategy)\n@named pde_system = PDESystem(eq, bcs, domains, [t, x], [u(t, x)])\nprob = discretize(pde_system, discretization)\nglobal iter = 0\ncallback = function (p, l)\n    global iter += 1\n    if iter % 20 == 0\n        println(\"$iter => $l\")\n    end\n    return false\nend\n\nres = Optimization.solve(prob, Adam(0.1); maxiters = 100)\nprob = remake(prob, u0 = res.u)\nres = Optimization.solve(prob, Adam(0.01); maxiters = 500)\nphi = discretization.phi\n\nu_predict = [first(phi([t, x], res.minimizer)) for t in ts, x in xs]\n\ndiff_u = abs.(u_predict .- u_MOL)\ntgrid = 0.0:0.01:1.0\nxgrid = -1.0:0.01:1.0\n\np1 = plot(tgrid, xgrid, u_MOL', linetype = :contourf, title = \"FD\");\np2 = plot(tgrid, xgrid, u_predict', linetype = :contourf, title = \"predict\");\np3 = plot(tgrid, xgrid, diff_u', linetype = :contourf, title = \"error\");\nplot(p1, p2, p3)","category":"page"},{"location":"examples/linear_parabolic/#Linear-parabolic-system-of-PDEs","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"","category":"section"},{"location":"examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"We can use NeuralPDE to solve the linear parabolic system of PDEs:","category":"page"},{"location":"examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"beginaligned\nfracpartial upartial t = a * fracpartial^2 upartial x^2 + b_1 u + c_1 w \nfracpartial wpartial t = a * fracpartial^2 wpartial x^2 + b_2 u + c_2 w \nendaligned","category":"page"},{"location":"examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"with initial and boundary conditions:","category":"page"},{"location":"examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"beginaligned\nu(0 x) = fracb_1 - lambda_2b_2 (lambda_1 - lambda_2) cdot cos(fracxa) -  fracb_1 - lambda_1b_2 (lambda_1 - lambda_2) cdot cos(fracxa) \nw(0 x) = 0 \nu(t 0) = fracb_1 - lambda_2b_2 (lambda_1 - lambda_2) cdot e^lambda_1t -  fracb_1 - lambda_1b_2 (lambda_1 - lambda_2) cdot e^lambda_2t  w(t 0) = frace^lambda_1-e^lambda_2lambda_1 - lambda_2 \nu(t 1) = fracb_1 - lambda_2b_2 (lambda_1 - lambda_2) cdot e^lambda_1t cdot cos(fracxa) -  fracb_1 - lambda_1b_2 (lambda_1 - lambda_2) cdot e^lambda_2t * cos(fracxa) \nw(t 1) = frace^lambda_1 cos(fracxa)-e^lambda_2cos(fracxa)lambda_1 - lambda_2\nendaligned","category":"page"},{"location":"examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"with a physics-informed neural network.","category":"page"},{"location":"examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimisers,\n      OptimizationOptimJL, LineSearches\nusing Plots\nusing ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u(..), w(..)\nDxx = Differential(x)^2\nDt = Differential(t)\n\n# Constants\na = 1\nb1 = 4\nb2 = 2\nc1 = 3\nc2 = 1\nλ1 = (b1 + c2 + sqrt((b1 + c2)^2 + 4 * (b1 * c2 - b2 * c1))) / 2\nλ2 = (b1 + c2 - sqrt((b1 + c2)^2 + 4 * (b1 * c2 - b2 * c1))) / 2\n\n# Analytic solution\nθ(t, x) = exp(-t) * cos(x / a)\nfunction u_analytic(t, x)\n    (b1 - λ2) / (b2 * (λ1 - λ2)) * exp(λ1 * t) * θ(t, x) -\n    (b1 - λ1) / (b2 * (λ1 - λ2)) * exp(λ2 * t) * θ(t, x)\nend\nw_analytic(t, x) = 1 / (λ1 - λ2) * (exp(λ1 * t) * θ(t, x) - exp(λ2 * t) * θ(t, x))\n\n# Second-order constant-coefficient linear parabolic system\neqs = [Dt(u(x, t)) ~ a * Dxx(u(x, t)) + b1 * u(x, t) + c1 * w(x, t),\n    Dt(w(x, t)) ~ a * Dxx(w(x, t)) + b2 * u(x, t) + c2 * w(x, t)]\n\n# Boundary conditions\nbcs = [u(0, x) ~ u_analytic(0, x),\n    w(0, x) ~ w_analytic(0, x),\n    u(t, 0) ~ u_analytic(t, 0),\n    w(t, 0) ~ w_analytic(t, 0),\n    u(t, 1) ~ u_analytic(t, 1),\n    w(t, 1) ~ w_analytic(t, 1)]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n    t ∈ Interval(0.0, 1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain = [Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, 1)) for _ in 1:2]\n\nstrategy = StochasticTraining(500)\ndiscretization = PhysicsInformedNN(chain, strategy)\n\n@named pdesystem = PDESystem(eqs, bcs, domains, [t, x], [u(t, x), w(t, x)])\nprob = discretize(pdesystem, discretization)\nsym_prob = symbolic_discretize(pdesystem, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\nglobal iteration = 0\ncallback = function (p, l)\n    if iteration % 10 == 0\n        println(\"loss: \", l)\n        println(\"pde_losses: \", map(l_ -> l_(p.u), pde_inner_loss_functions))\n        println(\"bcs_losses: \", map(l_ -> l_(p.u), bcs_inner_loss_functions))\n    end\n    global iteration += 1\n    return false\nend\n\nres = Optimization.solve(prob, OptimizationOptimisers.Adam(1e-2); maxiters = 10000)\n\nphi = discretization.phi\n\n# Analysis\nts, xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\ndepvars = [:u, :w]\nminimizers_ = [res.u.depvar[depvars[i]] for i in 1:length(chain)]\n\nanalytic_sol_func(t, x) = [u_analytic(t, x), w_analytic(t, x)]\nu_real = [[analytic_sol_func(t, x)[i] for t in ts for x in xs] for i in 1:2]\nu_predict = [[phi[i]([t, x], minimizers_[i])[1] for t in ts for x in xs] for i in 1:2]\ndiff_u = [abs.(u_real[i] .- u_predict[i]) for i in 1:2]\nps = []\nfor i in 1:2\n    p1 = plot(ts, xs, u_real[i], linetype = :contourf, title = \"u$i, analytic\")\n    p2 = plot(ts, xs, u_predict[i], linetype = :contourf, title = \"predict\")\n    p3 = plot(ts, xs, diff_u[i], linetype = :contourf, title = \"error\")\n    push!(ps, plot(p1, p2, p3))\nend","category":"page"},{"location":"examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"ps[1]","category":"page"},{"location":"examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"ps[2]","category":"page"},{"location":"tutorials/gpu/#Using-GPUs-to-train-Physics-Informed-Neural-Networks-(PINNs)","page":"Using GPUs","title":"Using GPUs to train Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"the 2-dimensional PDE:","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"_t u(x y t) = ^2_x u(x y t) + ^2_y u(x y t)  ","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"with the initial and boundary conditions:","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"beginalign*\nu(x y 0) = e^x+y cos(x + y)       \nu(0 y t) = e^y   cos(y + 4t)      \nu(2 y t) = e^2+y cos(2 + y + 4t)  \nu(x 0 t) = e^x   cos(x + 4t)      \nu(x 2 t) = e^x+2 cos(x + 2 + 4t)  \nendalign*","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"on the space and time domain:","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"x in 0 2   y in 0 2    t in 0 2  ","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"with physics-informed neural networks. The only major difference from the CPU case is that we must ensure that our initial parameters for the neural network are on the GPU. If that is done, then the internal computations will all take place on the GPU. This is done by using the gpu function on the initial parameters, like:","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"using Lux, LuxCUDA, ComponentArrays, Random\nconst gpud = gpu_device()\ninner = 25\nchain = Chain(Dense(3, inner, Lux.σ),\n    Dense(inner, inner, Lux.σ),\n    Dense(inner, inner, Lux.σ),\n    Dense(inner, inner, Lux.σ),\n    Dense(inner, 1))\nps = Lux.setup(Random.default_rng(), chain)[1]\nps = ps |> ComponentArray |> gpud .|> Float64","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"In total, this looks like:","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"using NeuralPDE, Lux, LuxCUDA, Random, ComponentArrays\nusing Optimization\nusing OptimizationOptimisers\nimport ModelingToolkit: Interval\nusing Plots\nusing Printf\n\n@parameters t x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\nDt = Differential(t)\nt_min = 0.0\nt_max = 2.0\nx_min = 0.0\nx_max = 2.0\ny_min = 0.0\ny_max = 2.0\n\n# 2D PDE\neq = Dt(u(t, x, y)) ~ Dxx(u(t, x, y)) + Dyy(u(t, x, y))\n\nanalytic_sol_func(t, x, y) = exp(x + y) * cos(x + y + 4t)\n# Initial and boundary conditions\nbcs = [u(t_min, x, y) ~ analytic_sol_func(t_min, x, y),\n    u(t, x_min, y) ~ analytic_sol_func(t, x_min, y),\n    u(t, x_max, y) ~ analytic_sol_func(t, x_max, y),\n    u(t, x, y_min) ~ analytic_sol_func(t, x, y_min),\n    u(t, x, y_max) ~ analytic_sol_func(t, x, y_max)]\n\n# Space and time domains\ndomains = [t ∈ Interval(t_min, t_max),\n    x ∈ Interval(x_min, x_max),\n    y ∈ Interval(y_min, y_max)]\n\n# Neural network\ninner = 25\nchain = Chain(Dense(3, inner, Lux.σ),\n    Dense(inner, inner, Lux.σ),\n    Dense(inner, inner, Lux.σ),\n    Dense(inner, inner, Lux.σ),\n    Dense(inner, 1))\n\nstrategy = QuasiRandomTraining(100)\nps = Lux.setup(Random.default_rng(), chain)[1]\nps = ps |> ComponentArray |> gpud .|> Float64\ndiscretization = PhysicsInformedNN(chain,\n    strategy,\n    init_params = ps)\n\n@named pde_system = PDESystem(eq, bcs, domains, [t, x, y], [u(t, x, y)])\nprob = discretize(pde_system, discretization)\nsymprob = symbolic_discretize(pde_system, discretization)\n\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = Optimization.solve(prob, OptimizationOptimisers.Adam(1e-2); maxiters = 2500)","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"We then use the remake function to rebuild the PDE problem to start a new optimization at the optimized parameters, and continue with a lower learning rate:","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"prob = remake(prob, u0 = res.u)\nres = Optimization.solve(\n    prob, OptimizationOptimisers.Adam(1e-3); callback = callback, maxiters = 2500)","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"Finally, we inspect the solution:","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"phi = discretization.phi\nts, xs, ys = [infimum(d.domain):0.1:supremum(d.domain) for d in domains]\nu_real = [analytic_sol_func(t, x, y) for t in ts for x in xs for y in ys]\nu_predict = [first(Array(phi([t, x, y], res.u))) for t in ts for x in xs for y in ys]\n\nfunction plot_(res)\n    # Animate\n    anim = @animate for (i, t) in enumerate(0:0.05:t_max)\n        @info \"Animating frame $i...\"\n        u_real = reshape([analytic_sol_func(t, x, y) for x in xs for y in ys],\n            (length(xs), length(ys)))\n        u_predict = reshape([Array(phi([t, x, y], res.u))[1] for x in xs for y in ys],\n            length(xs), length(ys))\n        u_error = abs.(u_predict .- u_real)\n        title = @sprintf(\"predict, t = %.3f\", t)\n        p1 = plot(xs, ys, u_predict, st = :surface, label = \"\", title = title)\n        title = @sprintf(\"real\")\n        p2 = plot(xs, ys, u_real, st = :surface, label = \"\", title = title)\n        title = @sprintf(\"error\")\n        p3 = plot(xs, ys, u_error, st = :contourf, label = \"\", title = title)\n        plot(p1, p2, p3)\n    end\n    gif(anim, \"3pde.gif\", fps = 10)\nend\n\nplot_(res)","category":"page"},{"location":"tutorials/gpu/#Performance-benchmarks","page":"Using GPUs","title":"Performance benchmarks","text":"","category":"section"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"Here are some performance benchmarks for 2d-pde with various number of input points and the number of neurons in the hidden layer, measuring the time for 100 iterations. Comparing runtime with GPU and CPU.","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"julia> CUDA.device()\n","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"(Image: image)","category":"page"},{"location":"examples/complex/#Complex-Equations-with-PINNs","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"","category":"section"},{"location":"examples/complex/","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"NeuralPDE supports training PINNs with complex differential equations. This example will demonstrate how to use it for NNODE. Let us consider a system of bloch equations [1]. Note QuadratureTraining cannot be used with complex equations due to current limitations of computing quadratures.","category":"page"},{"location":"examples/complex/","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"As the input to this neural network is time which is real, we need to initialize the parameters of the neural network with complex values for it to output and train with complex values.","category":"page"},{"location":"examples/complex/","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"using Random, NeuralPDE\nusing OrdinaryDiffEq\nusing Lux, OptimizationOptimisers\nusing Plots\nrng = Random.default_rng()\nRandom.seed!(100)\n\nfunction bloch_equations(u, p, t)\n    Ω, Δ, Γ = p\n    γ = Γ / 2\n    ρ₁₁, ρ₂₂, ρ₁₂, ρ₂₁ = u\n    d̢ρ = [im * Ω * (ρ₁₂ - ρ₂₁) + Γ * ρ₂₂;\n           -im * Ω * (ρ₁₂ - ρ₂₁) - Γ * ρ₂₂;\n           -(γ + im * Δ) * ρ₁₂ - im * Ω * (ρ₂₂ - ρ₁₁);\n           conj(-(γ + im * Δ) * ρ₁₂ - im * Ω * (ρ₂₂ - ρ₁₁))]\n    return d̢ρ\nend\n\nu0 = zeros(ComplexF64, 4)\nu0[1] = 1.0\ntime_span = (0.0, 2.0)\nparameters = [2.0, 0.0, 1.0]\n\nproblem = ODEProblem(bloch_equations, u0, time_span, parameters)\n\nchain = Lux.Chain(\n    Lux.Dense(1, 16, tanh;\n        init_weight = (rng, a...) -> Lux.kaiming_normal(rng, ComplexF64, a...)),\n    Lux.Dense(\n        16, 4; init_weight = (rng, a...) -> Lux.kaiming_normal(rng, ComplexF64, a...))\n)\nps, st = Lux.setup(rng, chain)\n\nopt = OptimizationOptimisers.Adam(0.01)\nground_truth = solve(problem, Tsit5(), saveat = 0.01)\nalg = NNODE(chain, opt, ps; strategy = StochasticTraining(500))\nsol = solve(problem, alg, verbose = false, maxiters = 5000, saveat = 0.01)","category":"page"},{"location":"examples/complex/","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"Now, lets plot the predictions.","category":"page"},{"location":"examples/complex/","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"u1:","category":"page"},{"location":"examples/complex/","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"plot(sol.t, real.(reduce(hcat, sol.u)[1, :]));\nplot!(ground_truth.t, real.(reduce(hcat, ground_truth.u)[1, :]))","category":"page"},{"location":"examples/complex/","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"plot(sol.t, imag.(reduce(hcat, sol.u)[1, :]));\nplot!(ground_truth.t, imag.(reduce(hcat, ground_truth.u)[1, :]))","category":"page"},{"location":"examples/complex/","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"u2:","category":"page"},{"location":"examples/complex/","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"plot(sol.t, real.(reduce(hcat, sol.u)[2, :]));\nplot!(ground_truth.t, real.(reduce(hcat, ground_truth.u)[2, :]))","category":"page"},{"location":"examples/complex/","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"plot(sol.t, imag.(reduce(hcat, sol.u)[2, :]));\nplot!(ground_truth.t, imag.(reduce(hcat, ground_truth.u)[2, :]))","category":"page"},{"location":"examples/complex/","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"u3:","category":"page"},{"location":"examples/complex/","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"plot(sol.t, real.(reduce(hcat, sol.u)[3, :]));\nplot!(ground_truth.t, real.(reduce(hcat, ground_truth.u)[3, :]))","category":"page"},{"location":"examples/complex/","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"plot(sol.t, imag.(reduce(hcat, sol.u)[3, :]));\nplot!(ground_truth.t, imag.(reduce(hcat, ground_truth.u)[3, :]))","category":"page"},{"location":"examples/complex/","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"u4:","category":"page"},{"location":"examples/complex/","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"plot(sol.t, real.(reduce(hcat, sol.u)[4, :]));\nplot!(ground_truth.t, real.(reduce(hcat, ground_truth.u)[4, :]))","category":"page"},{"location":"examples/complex/","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"plot(sol.t, imag.(reduce(hcat, sol.u)[4, :]));\nplot!(ground_truth.t, imag.(reduce(hcat, ground_truth.u)[4, :]))","category":"page"},{"location":"examples/complex/","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"We can see it is able to learn the real parts of u1, u2 and imaginary parts of u3, u4.","category":"page"},{"location":"examples/complex/","page":"Complex Equations with PINNs","title":"Complex Equations with PINNs","text":"[1]: https://steck.us/alkalidata/","category":"page"},{"location":"tutorials/systems/#systems","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs for Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"In this example, we will solve the PDE system:","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"beginalign*\n_t^2 u_1(t x)  = _x^2 u_1(t x) + u_3(t x)  sin(pi x)  \n_t^2 u_2(t x)  = _x^2 u_2(t x) + u_3(t x)  cos(pi x)  \n0  = u_1(t x) sin(pi x) + u_2(t x) cos(pi x) - e^-t  \nendalign*","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"with the initial conditions:","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"beginalign*\nu_1(0 x)  = sin(pi x)  \n_t u_1(0 x)  = - sin(pi x)  \nu_2(0 x)  = cos(pi x)  \n_t u_2(0 x)  = - cos(pi x)  \nendalign*","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"and the boundary conditions:","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"beginalign*\nu_1(t 0)  = u_1(t 1) = 0  \nu_2(t 0)  = - u_2(t 1) = e^-t  \nendalign*","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"with physics-informed neural networks.","category":"page"},{"location":"tutorials/systems/#Solution","page":"Defining Systems of PDEs","title":"Solution","text":"","category":"section"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, LineSearches\nusing ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u1(..), u2(..), u3(..)\nDt = Differential(t)\nDtt = Differential(t)^2\nDx = Differential(x)\nDxx = Differential(x)^2\n\neqs = [Dtt(u1(t, x)) ~ Dxx(u1(t, x)) + u3(t, x) * sin(pi * x),\n    Dtt(u2(t, x)) ~ Dxx(u2(t, x)) + u3(t, x) * cos(pi * x),\n    0.0 ~ u1(t, x) * sin(pi * x) + u2(t, x) * cos(pi * x) - exp(-t)]\n\nbcs = [u1(0, x) ~ sin(pi * x),\n    u2(0, x) ~ cos(pi * x),\n    Dt(u1(0, x)) ~ -sin(pi * x),\n    Dt(u2(0, x)) ~ -cos(pi * x),\n    u1(t, 0) ~ 0.0,\n    u2(t, 0) ~ exp(-t),\n    u1(t, 1) ~ 0.0,\n    u2(t, 1) ~ -exp(-t)]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, 1.0),\n    x ∈ Interval(0.0, 1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain = [Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, 1)) for _ in 1:3]\n\nstrategy = QuadratureTraining(; batch = 200, abstol = 1e-6, reltol = 1e-6)\ndiscretization = PhysicsInformedNN(chain, strategy)\n\n@named pdesystem = PDESystem(eqs, bcs, domains, [t, x], [u1(t, x), u2(t, x), u3(t, x)])\nprob = discretize(pdesystem, discretization)\nsym_prob = symbolic_discretize(pdesystem, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p.u), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p.u), bcs_inner_loss_functions))\n    return false\nend\n\nres = solve(prob, LBFGS(linesearch = BackTracking()); maxiters = 1000)\nphi = discretization.phi","category":"page"},{"location":"tutorials/systems/#Direct-Construction-via-symbolic_discretize","page":"Defining Systems of PDEs","title":"Direct Construction via symbolic_discretize","text":"","category":"section"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"One can take apart the pieces and reassemble the loss functions using the symbolic_discretize interface. Here is an example using the components from symbolic_discretize to fully reproduce the discretize optimization:","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, LineSearches\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u1(..), u2(..), u3(..)\nDt = Differential(t)\nDtt = Differential(t)^2\nDx = Differential(x)\nDxx = Differential(x)^2\n\neqs = [Dtt(u1(t, x)) ~ Dxx(u1(t, x)) + u3(t, x) * sin(pi * x),\n    Dtt(u2(t, x)) ~ Dxx(u2(t, x)) + u3(t, x) * cos(pi * x),\n    0.0 ~ u1(t, x) * sin(pi * x) + u2(t, x) * cos(pi * x) - exp(-t)]\n\nbcs = [u1(0, x) ~ sin(pi * x),\n    u2(0, x) ~ cos(pi * x),\n    Dt(u1(0, x)) ~ -sin(pi * x),\n    Dt(u2(0, x)) ~ -cos(pi * x),\n    u1(t, 0) ~ 0.0,\n    u2(t, 0) ~ exp(-t),\n    u1(t, 1) ~ 0.0,\n    u2(t, 1) ~ -exp(-t)]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, 1.0),\n    x ∈ Interval(0.0, 1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain = [Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, 1)) for _ in 1:3]\n@named pdesystem = PDESystem(eqs, bcs, domains, [t, x], [u1(t, x), u2(t, x), u3(t, x)])\n\nstrategy = NeuralPDE.QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain, strategy)\nsym_prob = NeuralPDE.symbolic_discretize(pdesystem, discretization)\n\npde_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbc_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p.u), pde_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p.u), bc_loss_functions))\n    return false\nend\n\nloss_functions = [pde_loss_functions; bc_loss_functions]\n\nfunction loss_function(θ, p)\n    sum(map(l -> l(θ), loss_functions))\nend\n\nf_ = OptimizationFunction(loss_function, Optimization.AutoZygote())\nprob = Optimization.OptimizationProblem(f_, sym_prob.flat_init_params)\n\nres = Optimization.solve(\n    prob, OptimizationOptimJL.LBFGS(linesearch = BackTracking()); maxiters = 1000)","category":"page"},{"location":"tutorials/systems/#Solution-Representation","page":"Defining Systems of PDEs","title":"Solution Representation","text":"","category":"section"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"Now let's perform some analysis for both the symbolic_discretize and discretize APIs:","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"using Plots\n\nphi = discretization.phi\nts, xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\n\nminimizers_ = [res.u.depvar[sym_prob.depvars[i]] for i in 1:3]\n\nfunction analytic_sol_func(t, x)\n    [exp(-t) * sin(pi * x), exp(-t) * cos(pi * x), (1 + pi^2) * exp(-t)]\nend\nu_real = [[analytic_sol_func(t, x)[i] for t in ts for x in xs] for i in 1:3]\nu_predict = [[phi[i]([t, x], minimizers_[i])[1] for t in ts for x in xs] for i in 1:3]\ndiff_u = [abs.(u_real[i] .- u_predict[i]) for i in 1:3]\nps = []\nfor i in 1:3\n    p1 = plot(ts, xs, u_real[i], linetype = :contourf, title = \"u$i, analytic\")\n    p2 = plot(ts, xs, u_predict[i], linetype = :contourf, title = \"predict\")\n    p3 = plot(ts, xs, diff_u[i], linetype = :contourf, title = \"error\")\n    push!(ps, plot(p1, p2, p3))\nend","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"ps[1]","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"ps[2]","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"ps[3]","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"Notice here that the solution is represented in the OptimizationSolution with u as the parameters for the trained neural network. But, for the case where the neural network is from Lux.jl, it's given as a ComponentArray where res.u.depvar.x corresponds to the result for the neural network corresponding to the dependent variable x, i.e. res.u.depvar.u1 are the trained parameters for phi[1] in our example. For simpler indexing, you can use res.u.depvar[:u1] or res.u.depvar[Symbol(:u,1)] as shown here.","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"Subsetting the array also works, but is inelegant.","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"(If param_estim == true, then res.u.p are the fit parameters)","category":"page"},{"location":"tutorials/systems/#Note:-Solving-Matrices-of-PDEs","page":"Defining Systems of PDEs","title":"Note: Solving Matrices of PDEs","text":"","category":"section"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"Also, in addition to vector systems, we can use the matrix form of PDEs:","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"using ModelingToolkit, NeuralPDE\n@parameters x y\n@variables (u(..))[1:2, 1:2]\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\n# Initial and boundary conditions\nbcs = [u[1](x, 0) ~ x, u[2](x, 0) ~ 2, u[3](x, 0) ~ 3, u[4](x, 0) ~ 4]\n\n# matrix PDE\neqs = @. [(Dxx(u_(x, y)) + Dyy(u_(x, y))) for u_ in u] ~ -sin(pi * x) * sin(pi * y) *\n                                                         [0 1; 0 1]\n\nsize(eqs)","category":"page"},{"location":"manual/logging/#Logging-Utilities","page":"Logging Utilities","title":"Logging Utilities","text":"","category":"section"},{"location":"manual/logging/","page":"Logging Utilities","title":"Logging Utilities","text":"LogOptions","category":"page"},{"location":"manual/logging/#NeuralPDE.LogOptions","page":"Logging Utilities","title":"NeuralPDE.LogOptions","text":"???\n\n\n\n\n\n","category":"type"},{"location":"manual/pinns/#PhysicsInformedNN-Discretizer-for-PDESystems","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"","category":"section"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"Using the PINNs solver, we can solve general nonlinear PDEs:","category":"page"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"(Image: generalPDE)","category":"page"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"with suitable boundary conditions:","category":"page"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"(Image: bcs)","category":"page"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"where time t is a special component of x, and Ω contains the temporal domain.","category":"page"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"PDEs are defined using the ModelingToolkit.jl PDESystem:","category":"page"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"@named pde_system = PDESystem(eq, bcs, domains, param, var)","category":"page"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"Here, eq is the equation, bcs represents the boundary conditions, param is the parameter of the equation (like [x,y]), and var represents variables (like [u]). For more information, see the ModelingToolkit.jl PDESystem documentation.","category":"page"},{"location":"manual/pinns/#The-PhysicsInformedNN-Discretizer","page":"PhysicsInformedNN Discretizer for PDESystems","title":"The PhysicsInformedNN Discretizer","text":"","category":"section"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"NeuralPDE.PhysicsInformedNN\nNeuralPDE.Phi\nSciMLBase.discretize(::PDESystem, ::NeuralPDE.PhysicsInformedNN)","category":"page"},{"location":"manual/pinns/#NeuralPDE.PhysicsInformedNN","page":"PhysicsInformedNN Discretizer for PDESystems","title":"NeuralPDE.PhysicsInformedNN","text":"PhysicsInformedNN(chain,\n                strategy;\n                init_params = nothing,\n                phi = nothing,\n                param_estim = false,\n                additional_loss = nothing,\n                adaptive_loss = nothing,\n                logger = nothing,\n                log_options = LogOptions(),\n                iteration = nothing,\n                kwargs...)\n\nA discretize algorithm for the ModelingToolkit PDESystem interface, which transforms a PDESystem into an OptimizationProblem using the Physics-Informed Neural Networks (PINN) methodology.\n\nPositional Arguments\n\nchain: a vector of Lux/Flux chains with a d-dimensional input and a          1-dimensional output corresponding to each of the dependent variables. Note that this          specification respects the order of the dependent variables as specified in the PDESystem.          Flux chains will be converted to Lux internally using adapt(FromFluxAdaptor(false, false), chain).\nstrategy: determines which training strategy will be used. See the Training Strategy             documentation for more details.\n\nKeyword Arguments\n\ninit_params: the initial parameters of the neural networks. If init_params is not given, then the neural network default parameters are used. Note that for Lux, the default will convert to Float64.\nphi: a trial solution, specified as phi(x,p) where x is the coordinates vector for the dependent variable and p are the weights of the phi function (generally the weights of the neural network defining phi). By default, this is generated from the chain. This should only be used to more directly impose functional information in the training problem, for example imposing the boundary condition by the test function formulation.\nadaptive_loss: the choice for the adaptive loss function. See the adaptive loss page for more details. Defaults to no adaptivity.\nadditional_loss: a function additional_loss(phi, θ, p_) where phi are the neural network trial solutions, θ are the weights of the neural network(s), and p_ are the hyperparameters of the OptimizationProblem. If param_estim = true, then θ additionally contains the parameters of the differential equation appended to the end of the vector.\nparam_estim: whether the parameters of the differential equation should be included in the values sent to the additional_loss function. Defaults to false.\nlogger: ?? needs docs\nlog_options: ?? why is this separate from the logger?\niteration: used to control the iteration counter???\nkwargs: Extra keyword arguments which are splatted to the OptimizationProblem on solve.\n\n\n\n\n\n","category":"type"},{"location":"manual/pinns/#NeuralPDE.Phi","page":"PhysicsInformedNN Discretizer for PDESystems","title":"NeuralPDE.Phi","text":"An encoding of the test function phi that is used for calculating the PDE value at domain points x\n\nFields:\n\nf: A representation of the chain function.\nst: The state of the Lux.AbstractExplicitLayer. It should be updated on each call.\n\n\n\n\n\n","category":"type"},{"location":"manual/pinns/#SciMLBase.discretize-Tuple{PDESystem, PhysicsInformedNN}","page":"PhysicsInformedNN Discretizer for PDESystems","title":"SciMLBase.discretize","text":"prob = discretize(pde_system::PDESystem, discretization::PhysicsInformedNN)\n\nTransforms a symbolic description of a ModelingToolkit-defined PDESystem and generates an OptimizationProblem for Optimization.jl whose solution is the solution to the PDE.\n\n\n\n\n\n","category":"method"},{"location":"manual/pinns/#symbolic_discretize-for-PhysicsInformedNN-and-the-lower-level-interface","page":"PhysicsInformedNN Discretizer for PDESystems","title":"symbolic_discretize for PhysicsInformedNN and the lower-level interface","text":"","category":"section"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"SciMLBase.symbolic_discretize(::PDESystem, ::NeuralPDE.AbstractPINN)\nNeuralPDE.PINNRepresentation\nNeuralPDE.PINNLossFunctions","category":"page"},{"location":"manual/pinns/#SciMLBase.symbolic_discretize-Tuple{PDESystem, NeuralPDE.AbstractPINN}","page":"PhysicsInformedNN Discretizer for PDESystems","title":"SciMLBase.symbolic_discretize","text":"prob = symbolic_discretize(pde_system::PDESystem, discretization::AbstractPINN)\n\nsymbolic_discretize is the lower level interface to discretize for inspecting internals. It transforms a symbolic description of a ModelingToolkit-defined PDESystem into a PINNRepresentation which holds the pieces required to build an OptimizationProblem for Optimization.jl or a Likelihood Function used for HMC based Posterior Sampling Algorithms AdvancedHMC.jl which is later optimized upon to give Solution or the Solution Distribution of the PDE.\n\nFor more information, see discretize and PINNRepresentation.\n\n\n\n\n\n","category":"method"},{"location":"manual/pinns/#NeuralPDE.PINNRepresentation","page":"PhysicsInformedNN Discretizer for PDESystems","title":"NeuralPDE.PINNRepresentation","text":"PINNRepresentation`\n\nAn internal representation of a physics-informed neural network (PINN). This is the struct used internally and returned for introspection by symbolic_discretize.\n\nFields\n\neqs: The equations of the PDE\n\nbcs: The boundary condition equations\n\ndomains: The domains for each of the independent variables\n\neq_params: ???\n\ndefaults: ???\n\ndefault_p: ???\n\nparam_estim: Whether parameters are to be appended to the additional_loss\n\nadditional_loss: The additional_loss function as provided by the user\n\nadaloss: The adaptive loss function\n\ndepvars: The dependent variables of the system\n\nindvars: The independent variables of the system\n\ndict_indvars: A dictionary form of the independent variables. Define the structure ???\n\ndict_depvars: A dictionary form of the dependent variables. Define the structure ???\n\ndict_depvar_input: ???\n\nlogger: The logger as provided by the user\n\nmultioutput: Whether there are multiple outputs, i.e. a system of PDEs\n\niteration: The iteration counter used inside the cost function\n\ninit_params: The initial parameters as provided by the user. If the PDE is a system of PDEs, this will be an array of arrays. If Lux.jl is used, then this is an array of ComponentArrays.\n\nflat_init_params: The initial parameters as a flattened array. This is the array that is used in the construction of the OptimizationProblem. If a Lux.jl neural network is used, then this flattened form is a ComponentArray. If the equation is a system of equations, then flat_init_params.depvar.x are the parameters for the neural network corresponding to the dependent variable x, and i.e. if depvar[i] == :x then for phi[i]. If param_estim = true, then flat_init_params.p are the parameters and flat_init_params.depvar.x are the neural network parameters, so flat_init_params.depvar.x would be the parameters of the neural network for the dependent variable x if it's a system.\n\nphi: The representation of the test function of the PDE solution\n\nderivative: The function used for computing the derivative\n\nstrategy: The training strategy as provided by the user\n\npde_indvars: ???\n\nbc_indvars: ???\n\npde_integration_vars: ???\n\nbc_integration_vars: ???\n\nintegral: ???\n\nsymbolic_pde_loss_functions: The PDE loss functions as represented in Julia AST\n\nsymbolic_bc_loss_functions: The boundary condition loss functions as represented in Julia AST\n\nloss_functions: The PINNLossFunctions, i.e. the generated loss functions\n\n\n\n\n\n","category":"type"},{"location":"manual/pinns/#NeuralPDE.PINNLossFunctions","page":"PhysicsInformedNN Discretizer for PDESystems","title":"NeuralPDE.PINNLossFunctions","text":"PINNLossFunctions`\n\nThe generated functions from the PINNRepresentation\n\nFields\n\nbc_loss_functions: The boundary condition loss functions\n\npde_loss_functions: The PDE loss functions\n\nfull_loss_function: The full loss function, combining the PDE and boundary condition loss functions. This is the loss function that is used by the optimizer.\n\nadditional_loss_function: The wrapped additional_loss, as pieced together for the optimizer.\n\ndatafree_pde_loss_functions: The pre-data version of the PDE loss function\n\ndatafree_bc_loss_functions: The pre-data version of the BC loss function\n\n\n\n\n\n","category":"type"},{"location":"tutorials/neural_adapter/#Transfer-Learning-with-Neural-Adapter","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"","category":"section"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"Transfer learning is a machine learning technique where a model trained on one task is re-purposed on a second related task.","category":"page"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"neural_adapter is the method that trains a neural network using the results from an already obtained prediction.","category":"page"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"This allows reusing the obtained prediction results and pre-training states of the neural network to get a new prediction, or reuse the results of predictions to train a related task (for example, the same task with a different domain). It makes it possible to create more flexible training schemes.","category":"page"},{"location":"tutorials/neural_adapter/#Retrain-the-prediction","page":"Transfer Learning with Neural Adapter","title":"Retrain the prediction","text":"","category":"section"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"Using the example of 2D Poisson equation, it is shown how, using the method neural_adapter, to retrain the prediction of one neural network to another.","category":"page"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"(Image: image)","category":"page"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimisers\nusing ModelingToolkit: Interval, infimum, supremum\nusing Random, ComponentArrays\n\n@parameters x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\n# 2D PDE\neq = Dxx(u(x, y)) + Dyy(u(x, y)) ~ -sin(pi * x) * sin(pi * y)\n\n# Initial and boundary conditions\nbcs = [u(0, y) ~ 0.0, u(1, y) ~ -sin(pi * 1) * sin(pi * y),\n    u(x, 0) ~ 0.0, u(x, 1) ~ -sin(pi * x) * sin(pi * 1)]\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0), y ∈ Interval(0.0, 1.0)]\nquadrature_strategy = NeuralPDE.QuadratureTraining(reltol = 1e-3, abstol = 1e-6,\n    maxiters = 50, batch = 100)\ninner = 8\naf = Lux.tanh\nchain1 = Lux.Chain(Lux.Dense(2, inner, af),\n    Lux.Dense(inner, inner, af),\n    Lux.Dense(inner, 1))\n\ndiscretization = NeuralPDE.PhysicsInformedNN(chain1, quadrature_strategy)\n\n@named pde_system = PDESystem(eq, bcs, domains, [x, y], [u(x, y)])\nprob = NeuralPDE.discretize(pde_system, discretization)\nsym_prob = NeuralPDE.symbolic_discretize(pde_system, discretization)\n\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = Optimization.solve(prob, OptimizationOptimisers.Adam(5e-3); maxiters = 10000)\nphi = discretization.phi\n\ninner_ = 8\naf = Lux.tanh\nchain2 = Lux.Chain(Dense(2, inner_, af),\n    Dense(inner_, inner_, af),\n    Dense(inner_, inner_, af),\n    Dense(inner_, 1))\ninitp, st = Lux.setup(Random.default_rng(), chain2)\ninit_params2 = Float64.(ComponentArrays.ComponentArray(initp))\n\n# the rule by which the training will take place is described here in loss function\nfunction loss(cord, θ)\n    global st, chain2\n    ch2, st = chain2(cord, θ, st)\n    ch2 .- phi(cord, res.u)\nend\n\nstrategy = NeuralPDE.QuadratureTraining(; reltol = 1e-6, abstol = 1e-3)\n\nprob_ = NeuralPDE.neural_adapter(loss, init_params2, pde_system, strategy)\nres_ = Optimization.solve(prob_, OptimizationOptimisers.Adam(5e-3); maxiters = 10000)\n\nphi_ = PhysicsInformedNN(chain2, strategy; init_params = res_.u).phi\n\nxs, ys = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\nanalytic_sol_func(x, y) = (sin(pi * x) * sin(pi * y)) / (2pi^2)\n\nu_predict = reshape([first(phi([x, y], res.u)) for x in xs for y in ys],\n    (length(xs), length(ys)))\nu_predict_ = reshape([first(phi_([x, y], res_.u)) for x in xs for y in ys],\n    (length(xs), length(ys)))\nu_real = reshape([analytic_sol_func(x, y) for x in xs for y in ys],\n    (length(xs), length(ys)))\ndiff_u = u_predict .- u_real\ndiff_u_ = u_predict_ .- u_real\n\nusing Plots\np1 = plot(xs, ys, u_predict, linetype = :contourf, title = \"first predict\")\np2 = plot(xs, ys, u_predict_, linetype = :contourf, title = \"second predict\")\np3 = plot(xs, ys, u_real, linetype = :contourf, title = \"analytic\")\np4 = plot(xs, ys, diff_u, linetype = :contourf, title = \"error 1\")\np5 = plot(xs, ys, diff_u_, linetype = :contourf, title = \"error 2\")\nplot(p1, p2, p3, p4, p5)","category":"page"},{"location":"tutorials/neural_adapter/#Domain-decomposition","page":"Transfer Learning with Neural Adapter","title":"Domain decomposition","text":"","category":"section"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"In this example, we first obtain a prediction of 2D Poisson equation on subdomains. We split up full domain into 10 sub problems by x, and create separate neural networks for each sub interval. If x domain ∈ [x0, xend] so, it is decomposed on 10 part: sub x domains = {[x0, x1], ... [xi,xi+1], ..., x9,xend]}. And then using the method neural_adapter, we retrain the batch of 10 predictions to the one prediction for full domain of task.","category":"page"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"(Image: domain_decomposition)","category":"page"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimisers\nusing ModelingToolkit: Interval, infimum, supremum\n\n@parameters x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\neq = Dxx(u(x, y)) + Dyy(u(x, y)) ~ -sin(pi * x) * sin(pi * y)\n\nbcs = [u(0, y) ~ 0.0, u(1, y) ~ -sin(pi * 1) * sin(pi * y),\n    u(x, 0) ~ 0.0, u(x, 1) ~ -sin(pi * x) * sin(pi * 1)]\n\n# Space\nx_0 = 0.0\nx_end = 1.0\nx_domain = Interval(x_0, x_end)\ny_domain = Interval(0.0, 1.0)\ndomains = [x ∈ x_domain, y ∈ y_domain]\n\ncount_decomp = 10\n\n# Neural network\naf = Lux.tanh\ninner = 10\nchains = [Lux.Chain(Dense(2, inner, af), Dense(inner, inner, af), Dense(inner, 1))\n          for _ in 1:count_decomp]\ninit_params = map(\n    c -> Float64.(ComponentArray(Lux.setup(Random.default_rng(), c)[1])), chains)\n\nxs_ = infimum(x_domain):(1 / count_decomp):supremum(x_domain)\nxs_domain = [(xs_[i], xs_[i + 1]) for i in 1:(length(xs_) - 1)]\ndomains_map = map(xs_domain) do (xs_dom)\n    x_domain_ = Interval(xs_dom...)\n    domains_ = [x ∈ x_domain_,\n        y ∈ y_domain]\nend\n\nanalytic_sol_func(x, y) = (sin(pi * x) * sin(pi * y)) / (2pi^2)\nfunction create_bcs(x_domain_, phi_bound)\n    x_0, x_e = x_domain_.left, x_domain_.right\n    if x_0 == 0.0\n        bcs = [u(0, y) ~ 0.0,\n            u(x_e, y) ~ analytic_sol_func(x_e, y),\n            u(x, 0) ~ 0.0,\n            u(x, 1) ~ -sin(pi * x) * sin(pi * 1)]\n        return bcs\n    end\n    bcs = [u(x_0, y) ~ phi_bound(x_0, y),\n        u(x_e, y) ~ analytic_sol_func(x_e, y),\n        u(x, 0) ~ 0.0,\n        u(x, 1) ~ -sin(pi * x) * sin(pi * 1)]\n    bcs\nend\n\nreses = []\nphis = []\npde_system_map = []\n\nfor i in 1:count_decomp\n    println(\"decomposition $i\")\n    domains_ = domains_map[i]\n    phi_in(cord) = phis[i - 1](cord, reses[i - 1].u)\n    phi_bound(x, y) = phi_in(vcat(x, y))\n    @register_symbolic phi_bound(x, y)\n    Base.Broadcast.broadcasted(::typeof(phi_bound), x, y) = phi_bound(x, y)\n    bcs_ = create_bcs(domains_[1].domain, phi_bound)\n    @named pde_system_ = PDESystem(eq, bcs_, domains_, [x, y], [u(x, y)])\n    push!(pde_system_map, pde_system_)\n    strategy = NeuralPDE.QuadratureTraining(; reltol = 1e-6, abstol = 1e-3)\n\n    discretization = NeuralPDE.PhysicsInformedNN(chains[i], strategy;\n        init_params = init_params[i])\n\n    prob = NeuralPDE.discretize(pde_system_, discretization)\n    symprob = NeuralPDE.symbolic_discretize(pde_system_, discretization)\n    res_ = Optimization.solve(prob, OptimizationOptimisers.Adam(5e-3); maxiters = 10000)\n    phi = discretization.phi\n    push!(reses, res_)\n    push!(phis, phi)\nend\n\nfunction compose_result(dx)\n    u_predict_array = Float64[]\n    diff_u_array = Float64[]\n    ys = infimum(domains[2].domain):dx:supremum(domains[2].domain)\n    xs_ = infimum(x_domain):dx:supremum(x_domain)\n    xs = collect(xs_)\n    function index_of_interval(x_)\n        for (i, x_domain) in enumerate(xs_domain)\n            if x_ <= x_domain[2] && x_ >= x_domain[1]\n                return i\n            end\n        end\n    end\n    for x_ in xs\n        i = index_of_interval(x_)\n        u_predict_sub = [first(phis[i]([x_, y], reses[i].u)) for y in ys]\n        u_real_sub = [analytic_sol_func(x_, y) for y in ys]\n        diff_u_sub = abs.(u_predict_sub .- u_real_sub)\n        append!(u_predict_array, u_predict_sub)\n        append!(diff_u_array, diff_u_sub)\n    end\n    xs, ys = [infimum(d.domain):dx:supremum(d.domain) for d in domains]\n    u_predict = reshape(u_predict_array, (length(xs), length(ys)))\n    diff_u = reshape(diff_u_array, (length(xs), length(ys)))\n    u_predict, diff_u\nend\n\ndx = 0.01\nu_predict, diff_u = compose_result(dx)\n\ninner_ = 18\naf = Lux.tanh\nchain2 = Lux.Chain(Dense(2, inner_, af),\n    Dense(inner_, inner_, af),\n    Dense(inner_, inner_, af),\n    Dense(inner_, inner_, af),\n    Dense(inner_, 1))\n\ninitp, st = Lux.setup(Random.default_rng(), chain2)\ninit_params2 = Float64.(ComponentArrays.ComponentArray(initp))\n\n@named pde_system = PDESystem(eq, bcs, domains, [x, y], [u(x, y)])\n\nlosses = map(1:count_decomp) do i\n    function loss(cord, θ)\n        global st, chain2, phis, reses\n        ch2, st = chain2(cord, θ, st)\n        ch2 .- phis[i](cord, reses[i].u)\n    end\nend\n\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nprob_ = NeuralPDE.neural_adapter(losses, init_params2, pde_system_map,\n    NeuralPDE.QuadratureTraining(; reltol = 1e-6, abstol = 1e-3))\nres_ = Optimization.solve(prob_, OptimizationOptimisers.Adam(5e-3); maxiters = 5000)\nprob_ = NeuralPDE.neural_adapter(losses, res_.u, pde_system_map,\n    NeuralPDE.QuadratureTraining(; reltol = 1e-6, abstol = 1e-3))\nres_ = Optimization.solve(prob_, OptimizationOptimisers.Adam(5e-3); maxiters = 5000)\n\nphi_ = PhysicsInformedNN(chain2, strategy; init_params = res_.u).phi\n\nxs, ys = [infimum(d.domain):dx:supremum(d.domain) for d in domains]\nu_predict_ = reshape([first(phi_([x, y], res_.u)) for x in xs for y in ys],\n    (length(xs), length(ys)))\nu_real = reshape([analytic_sol_func(x, y) for x in xs for y in ys],\n    (length(xs), length(ys)))\ndiff_u_ = u_predict_ .- u_real\n\nusing Plots\n\np1 = plot(xs, ys, u_predict, linetype = :contourf, title = \"predict 1\");\np2 = plot(xs, ys, u_predict_, linetype = :contourf, title = \"predict 2\");\np3 = plot(xs, ys, u_real, linetype = :contourf, title = \"analytic\");\np4 = plot(xs, ys, diff_u, linetype = :contourf, title = \"error 1\");\np5 = plot(xs, ys, diff_u_, linetype = :contourf, title = \"error 2\");\nplot(p1, p2, p3, p4, p5)","category":"page"},{"location":"tutorials/ode/#Solving-ODEs-with-Physics-Informed-Neural-Networks-(PINNs)","page":"Introduction to NeuralPDE for ODEs","title":"Solving ODEs with Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"note: Note\nIt is highly recommended you first read the solving ordinary differential equations with DifferentialEquations.jl tutorial before reading this tutorial.","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"This tutorial is an introduction to using physics-informed neural networks (PINNs) for solving ordinary differential equations (ODEs). In contrast to the later parts of this documentation which use the symbolic interface, here we will focus on the simplified NNODE which uses the ODEProblem specification for the ODE.","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"Mathematically, the ODEProblem defines a problem:","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"u = f(upt)","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"for t in (t_0t_f) with an initial condition u(t_0) = u_0. With physics-informed neural networks, we choose a neural network architecture NN to represent the solution u and seek parameters p such that NN' = f(NN,p,t) for all points in the domain. When this is satisfied sufficiently closely, then NN is thus a solution to the differential equation.","category":"page"},{"location":"tutorials/ode/#Solving-an-ODE-with-NNODE","page":"Introduction to NeuralPDE for ODEs","title":"Solving an ODE with NNODE","text":"","category":"section"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"Let's solve a simple ODE:","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"u = cos(2pi t)","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"for t in (01) and u_0 = 0 with NNODE. First, we define an ODEProblem as we would for defining an ODE using DifferentialEquations.jl interface. This looks like:","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"using NeuralPDE\n\nlinear(u, p, t) = cos(t * 2 * pi)\ntspan = (0.0, 1.0)\nu0 = 0.0\nprob = ODEProblem(linear, u0, tspan)","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"Now, to define the NNODE solver, we must choose a neural network architecture. To do this, we will use the Lux.jl to define a multilayer perceptron (MLP) with one hidden layer of 5 nodes and a sigmoid activation function. This looks like:","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"using Lux, Random\n\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\nchain = Chain(Dense(1, 5, σ), Dense(5, 1))\nps, st = Lux.setup(rng, chain) |> Lux.f64","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"Now we must choose an optimizer to define the NNODE solver. A common choice is Adam, with a tunable rate, which we will set to 0.1. In general, this rate parameter should be decreased if the solver's loss tends to be unsteady (sometimes rise “too much”), but should be as large as possible for efficiency. We use Adam from OptimizationOptimisers. Thus, the definition of the NNODE solver is as follows:","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"using OptimizationOptimisers\n\nopt = Adam(0.1)\nalg = NNODE(chain, opt, init_params = ps)","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"Once these pieces are together, we call solve just like with any other ODEProblem. Let's turn on verbose so we can see the loss over time during the training process:","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"sol = solve(prob, alg, verbose = true, maxiters = 2000, saveat = 0.01)","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"Now lets compare the predictions from the learned network with the ground truth which we can obtain by numerically solving the ODE.","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"using OrdinaryDiffEq, Plots\n\nground_truth = solve(prob, Tsit5(), saveat = 0.01)\n\nplot(ground_truth, label = \"ground truth\")\nplot!(sol.t, sol.u, label = \"pred\")","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"And that's it: the neural network solution was computed by training the neural network and returned in the standard DifferentialEquations.jl ODESolution format. For more information on handling the solution, consult here.","category":"page"},{"location":"tutorials/integro_diff/#Solving-Integro-Differential-Equations-with-Physics-Informed-Neural-Networks-(PINNs)","page":"Solving Integro Differential Equations","title":"Solving Integro-Differential Equations with Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"The integral of function u(x),","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"int_0^tu(x)dx","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"where x is variable of integral and t is variable of integro-differential equation,","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"is defined as","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"using ModelingToolkit\n@parameters t\n@variables i(..)\nIi = Symbolics.Integral(t in DomainSets.ClosedInterval(0, t))","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"In multidimensional case,","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"Ix = Integral((x, y) in DomainSets.UnitSquare())","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"The UnitSquare domain ranges both x and y from 0 to 1. Similarly, a rectangular or cuboidal domain can be defined using ProductDomain of ClosedIntervals.","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"Ix = Integral((x, y) in DomainSets.ProductDomain(ClosedInterval(0, 1), ClosedInterval(0, x)))","category":"page"},{"location":"tutorials/integro_diff/#1-dimensional-example","page":"Solving Integro Differential Equations","title":"1-dimensional example","text":"","category":"section"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"Let's take an example of an integro-differential equation:","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"fract u(t)  + 2u(t) + 5 int_0^tu(x)dx = 1  textfor  t geq 0","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"and boundary condition","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"u(0) = 0","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, DomainSets\nusing ModelingToolkit: Interval, infimum, supremum\nusing Plots\n\n@parameters t\n@variables i(..)\nDi = Differential(t)\nIi = Integral(t in DomainSets.ClosedInterval(0, t))\neq = Di(i(t)) + 2 * i(t) + 5 * Ii(i(t)) ~ 1\nbcs = [i(0.0) ~ 0.0]\ndomains = [t ∈ Interval(0.0, 2.0)]\nchain = Lux.Chain(Lux.Dense(1, 15, Lux.σ), Lux.Dense(15, 1))\n\nstrategy_ = QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain,\n    strategy_)\n@named pde_system = PDESystem(eq, bcs, domains, [t], [i(t)])\nprob = NeuralPDE.discretize(pde_system, discretization)\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\nres = Optimization.solve(prob, BFGS(); maxiters = 100)","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"Plotting the final solution and analytical solution","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"ts = [infimum(d.domain):0.01:supremum(d.domain) for d in domains][1]\nphi = discretization.phi\nu_predict = [first(phi([t], res.u)) for t in ts]\n\nanalytic_sol_func(t) = 1 / 2 * (exp(-t)) * (sin(2 * t))\nu_real = [analytic_sol_func(t) for t in ts]\nplot(ts, u_real, label = \"Analytical Solution\")\nplot!(ts, u_predict, label = \"PINN Solution\")","category":"page"},{"location":"examples/3rd/#ODE-with-a-3rd-Order-Derivative","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"","category":"section"},{"location":"examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"Let's consider the ODE with a 3rd-order derivative:","category":"page"},{"location":"examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"beginalign*\n^3_x u(x) = cos(pi x)  \nu(0) = 0  \nu(1) = cos(pi)  \n_x u(0) = 1  \nx in 0 1  \nendalign*","category":"page"},{"location":"examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"We will use physics-informed neural networks.","category":"page"},{"location":"examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"using NeuralPDE, Lux, ModelingToolkit\nusing Optimization, OptimizationOptimJL, OptimizationOptimisers\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x\n@variables u(..)\n\nDxxx = Differential(x)^3\nDx = Differential(x)\n# ODE\neq = Dxxx(u(x)) ~ cos(pi * x)\n\n# Initial and boundary conditions\nbcs = [u(0.0) ~ 0.0,\n    u(1.0) ~ cos(pi),\n    Dx(u(1.0)) ~ 1.0]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0)]\n\n# Neural network\nchain = Lux.Chain(Dense(1, 8, Lux.σ), Dense(8, 1))\n\ndiscretization = PhysicsInformedNN(chain, QuasiRandomTraining(20))\n@named pde_system = PDESystem(eq, bcs, domains, [x], [u(x)])\nprob = discretize(pde_system, discretization)\n\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = Optimization.solve(prob, OptimizationOptimisers.Adam(0.01); maxiters = 2000)\nphi = discretization.phi","category":"page"},{"location":"examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"We can plot the predicted solution of the ODE and its analytical solution.","category":"page"},{"location":"examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"using Plots\n\nanalytic_sol_func(x) = (π * x * (-x + (π^2) * (2 * x - 3) + 1) - sin(π * x)) / (π^3)\n\ndx = 0.05\nxs = [infimum(d.domain):(dx / 10):supremum(d.domain) for d in domains][1]\nu_real = [analytic_sol_func(x) for x in xs]\nu_predict = [first(phi(x, res.u)) for x in xs]\n\nx_plot = collect(xs)\nplot(x_plot, u_real, title = \"real\")\nplot!(x_plot, u_predict, title = \"predict\")","category":"page"},{"location":"manual/dae/#Differential-Algebraic-Equation-Specialized-Physics-Informed-Neural-Solver","page":"Differential Algebraic Equation Specialized Physics-Informed Neural Solver","title":"Differential Algebraic Equation Specialized Physics-Informed Neural Solver","text":"","category":"section"},{"location":"manual/dae/","page":"Differential Algebraic Equation Specialized Physics-Informed Neural Solver","title":"Differential Algebraic Equation Specialized Physics-Informed Neural Solver","text":"NNDAE","category":"page"},{"location":"manual/dae/#NeuralPDE.NNDAE","page":"Differential Algebraic Equation Specialized Physics-Informed Neural Solver","title":"NeuralPDE.NNDAE","text":"NNDAE(chain,\n    OptimizationOptimisers.Adam(0.1),\n    init_params = nothing;\n    autodiff = false,\n    kwargs...)\n\nAlgorithm for solving differential algebraic equationsusing a neural network. This is a specialization of the physics-informed neural network which is used as a solver for a standard DAEProblem.\n\nwarn: Warn\nNote that NNDAE only supports DAEs which are written in the out-of-place form, i.e. du = f(du,u,p,t), and not f(out,du,u,p,t). If not declared out-of-place, then the NNDAE will exit with an error.\n\nPositional Arguments\n\nchain: A neural network architecture, defined as either a Flux.Chain or a Lux.AbstractExplicitLayer.\nopt: The optimizer to train the neural network.\ninit_params: The initial parameter of the neural network. By default, this is nothing which thus uses the random initialization provided by the neural network library.\n\nKeyword Arguments\n\nautodiff: The switch between automatic(not supported yet) and numerical differentiation for             the PDE operators. The reverse mode of the loss function is always             automatic differentiation (via Zygote), this is only for the derivative             in the loss function (the derivative with respect to time).\nstrategy: The training strategy used to choose the points for the evaluations.             By default, GridTraining is used with dt if given.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/param_estim/#Optimizing-Parameters-(Solving-Inverse-Problems)-with-Physics-Informed-Neural-Networks-(PINNs)","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimizing Parameters (Solving Inverse Problems) with Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"Consider a Lorenz System,","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"beginalign*\n    fracmathrmd xmathrmdt = sigma (y -x)  \n    fracmathrmd ymathrmdt = x (rho - z) - y  \n    fracmathrmd zmathrmdt = x y - beta z  \nendalign*","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"with Physics-Informed Neural Networks. Now we would consider the case where we want to optimize the parameters sigma, beta, and rho.","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"We start by defining the problem,","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, OrdinaryDiffEq,\n      Plots, LineSearches\nusing ModelingToolkit: Interval, infimum, supremum\n@parameters t, σ_, β, ρ\n@variables x(..), y(..), z(..)\nDt = Differential(t)\neqs = [Dt(x(t)) ~ σ_ * (y(t) - x(t)),\n    Dt(y(t)) ~ x(t) * (ρ - z(t)) - y(t),\n    Dt(z(t)) ~ x(t) * y(t) - β * z(t)]\n\nbcs = [x(0) ~ 1.0, y(0) ~ 0.0, z(0) ~ 0.0]\ndomains = [t ∈ Interval(0.0, 1.0)]\ndt = 0.01","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"And the neural networks as,","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"input_ = length(domains)\nn = 8\nchain1 = Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, n, Lux.σ),\n    Dense(n, 1))\nchain2 = Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, n, Lux.σ),\n    Dense(n, 1))\nchain3 = Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, n, Lux.σ),\n    Dense(n, 1))","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"We will add another loss term based on the data that we have to optimize the parameters.","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"Here we simply calculate the solution of the Lorenz system with OrdinaryDiffEq.jl based on the adaptivity of the ODE solver. This is used to introduce non-uniformity to the time series.","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"function lorenz!(du, u, p, t)\n    du[1] = 10.0 * (u[2] - u[1])\n    du[2] = u[1] * (28.0 - u[3]) - u[2]\n    du[3] = u[1] * u[2] - (8 / 3) * u[3]\nend\n\nu0 = [1.0; 0.0; 0.0]\ntspan = (0.0, 1.0)\nprob = ODEProblem(lorenz!, u0, tspan)\nsol = solve(prob, Tsit5(), dt = 0.1)\nts = [infimum(d.domain):0.01:supremum(d.domain) for d in domains][1]\nfunction getData(sol)\n    data = []\n    us = hcat(sol(ts).u...)\n    ts_ = hcat(sol(ts).t...)\n    return [us, ts_]\nend\ndata = getData(sol)\n\n(u_, t_) = data\nlen = length(data[2])","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"Then we define the additional loss function additional_loss(phi, θ , p), the function has three arguments:","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"phi the trial solution\nθ the parameters of neural networks\nthe hyperparameters p .","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"For a Lux neural network, the composed function will present itself as having θ as a ComponentArray subsets θ.x, which can also be dereferenced like θ[:x]. Thus, the additional loss looks like:","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"depvars = [:x, :y, :z]\nfunction additional_loss(phi, θ, p)\n    return sum(sum(abs2, phi[i](t_, θ[depvars[i]]) .- u_[[i], :]) / len for i in 1:1:3)\nend","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"Then finally defining and optimizing using the PhysicsInformedNN interface.","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"discretization = NeuralPDE.PhysicsInformedNN([chain1, chain2, chain3],\n    NeuralPDE.QuadratureTraining(; abstol = 1e-6, reltol = 1e-6, batch = 200), param_estim = true,\n    additional_loss = additional_loss)\n@named pde_system = PDESystem(eqs, bcs, domains, [t], [x(t), y(t), z(t)], [σ_, ρ, β],\n    defaults = Dict([p .=> 1.0 for p in [σ_, ρ, β]]))\nprob = NeuralPDE.discretize(pde_system, discretization)\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\nres = Optimization.solve(prob, BFGS(linesearch = BackTracking()); maxiters = 1000)\np_ = res.u[(end - 2):end] # p_ = [9.93, 28.002, 2.667]","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"And then finally some analysis by plotting.","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"minimizers = [res.u.depvar[depvars[i]] for i in 1:3]\nts = [infimum(d.domain):(0.001):supremum(d.domain) for d in domains][1]\nu_predict = [[discretization.phi[i]([t], minimizers[i])[1] for t in ts] for i in 1:3]\nplot(sol)\nplot!(ts, u_predict, label = [\"x(t)\" \"y(t)\" \"z(t)\"])","category":"page"},{"location":"examples/nonlinear_elliptic/#Nonlinear-elliptic-system-of-PDEs","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"","category":"section"},{"location":"examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"We can also solve nonlinear systems, such as the system of nonlinear elliptic PDEs","category":"page"},{"location":"examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"beginaligned\nfracpartial^2upartial x^2 + fracpartial^2upartial y^2 = uf(fracuw) + fracuwh(fracuw) \nfracpartial^2wpartial x^2 + fracpartial^2wpartial y^2 = wg(fracuw) + h(fracuw) \nendaligned","category":"page"},{"location":"examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"where f, g, h are arbitrary functions. With initial and boundary conditions:","category":"page"},{"location":"examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"beginaligned\nu(0y) = y + 1 \nw(1 y) = cosh(sqrtf(k)) + sinh(sqrtf(k))cdot(y + 1) \nw(x0) = cosh(sqrtf(k)) + sinh(sqrtf(k)) \nw(0y) = k(y + 1) \nu(1 y) = kcosh(sqrtf(k)) + sinh(sqrtf(k))cdot(y + 1) \nu(x0) = kcosh(sqrtf(k)) + sinh(sqrtf(k)) \nendaligned","category":"page"},{"location":"examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"where k is a root of the algebraic (transcendental) equation f(k) = g(k).","category":"page"},{"location":"examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"This is done using a derivative neural network approximation.","category":"page"},{"location":"examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, Roots\nusing Plots\nusing ModelingToolkit: Interval, infimum, supremum\n\n@parameters x, y\nDx = Differential(x)\nDy = Differential(y)\n@variables Dxu(..), Dyu(..), Dxw(..), Dyw(..)\n@variables u(..), w(..)\n\n# Arbitrary functions\nf(x) = sin(x)\ng(x) = cos(x)\nh(x) = x\nroot(x) = f(x) - g(x)\n\n# Analytic solution\nk = find_zero(root, (0, 1), Bisection())                            # k is a root of the algebraic (transcendental) equation f(x) = g(x)\nθ(x, y) = (cosh(sqrt(f(k)) * x) + sinh(sqrt(f(k)) * x)) * (y + 1)   # Analytical solution to Helmholtz equation\nw_analytic(x, y) = θ(x, y) - h(k) / f(k)\nu_analytic(x, y) = k * w_analytic(x, y)\n\n# Nonlinear Steady-State Systems of Two Reaction-Diffusion Equations with 3 arbitrary function f, g, h\neqs_ = [\n    Dx(Dxu(x, y)) + Dy(Dyu(x, y)) ~ u(x, y) * f(u(x, y) / w(x, y)) +\n                                    u(x, y) / w(x, y) * h(u(x, y) / w(x, y)),\n    Dx(Dxw(x, y)) + Dy(Dyw(x, y)) ~ w(x, y) * g(u(x, y) / w(x, y)) + h(u(x, y) / w(x, y))]\n\n# Boundary conditions\nbcs_ = [u(0, y) ~ u_analytic(0, y),\n    u(1, y) ~ u_analytic(1, y),\n    u(x, 0) ~ u_analytic(x, 0),\n    w(0, y) ~ w_analytic(0, y),\n    w(1, y) ~ w_analytic(1, y),\n    w(x, 0) ~ w_analytic(x, 0)]\n\nder_ = [Dy(u(x, y)) ~ Dyu(x, y),\n    Dy(w(x, y)) ~ Dyw(x, y),\n    Dx(u(x, y)) ~ Dxu(x, y),\n    Dx(w(x, y)) ~ Dxw(x, y)]\n\nbcs__ = [bcs_; der_]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n    y ∈ Interval(0.0, 1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain = [Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, 1)) for _ in 1:6] # 1:number of @variables\n\nstrategy = GridTraining(0.01)\ndiscretization = PhysicsInformedNN(chain, strategy)\n\nvars = [u(x, y), w(x, y), Dxu(x, y), Dyu(x, y), Dxw(x, y), Dyw(x, y)]\n@named pdesystem = PDESystem(eqs_, bcs__, domains, [x, y], vars)\nprob = NeuralPDE.discretize(pdesystem, discretization)\nsym_prob = NeuralPDE.symbolic_discretize(pdesystem, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions[1:6]\naprox_derivative_loss_functions = sym_prob.loss_functions.bc_loss_functions[7:end]\n\nglobal iteration = 0\ncallback = function (p, l)\n    if iteration % 10 == 0\n        println(\"loss: \", l)\n        println(\"pde_losses: \", map(l_ -> l_(p.u), pde_inner_loss_functions))\n        println(\"bcs_losses: \", map(l_ -> l_(p.u), bcs_inner_loss_functions))\n        println(\"der_losses: \", map(l_ -> l_(p.u), aprox_derivative_loss_functions))\n    end\n    global iteration += 1\n    return false\nend\n\nres = Optimization.solve(prob, BFGS(); maxiters = 100)\n\nphi = discretization.phi\n\n# Analysis\nxs, ys = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\ndepvars = [:u, :w]\nminimizers_ = [res.u.depvar[depvars[i]] for i in 1:2]\n\nanalytic_sol_func(x, y) = [u_analytic(x, y), w_analytic(x, y)]\nu_real = [[analytic_sol_func(x, y)[i] for x in xs for y in ys] for i in 1:2]\nu_predict = [[phi[i]([x, y], minimizers_[i])[1] for x in xs for y in ys] for i in 1:2]\ndiff_u = [abs.(u_real[i] .- u_predict[i]) for i in 1:2]\nps = []\nfor i in 1:2\n    p1 = plot(xs, ys, u_real[i], linetype = :contourf, title = \"u$i, analytic\")\n    p2 = plot(xs, ys, u_predict[i], linetype = :contourf, title = \"predict\")\n    p3 = plot(xs, ys, diff_u[i], linetype = :contourf, title = \"error\")\n    push!(ps, plot(p1, p2, p3))\nend","category":"page"},{"location":"examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"ps[1]","category":"page"},{"location":"examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"ps[2]","category":"page"},{"location":"manual/neural_adapters/#Transfer-Learning-with-neural_adapter","page":"Transfer Learning with neural_adapter","title":"Transfer Learning with neural_adapter","text":"","category":"section"},{"location":"manual/neural_adapters/","page":"Transfer Learning with neural_adapter","title":"Transfer Learning with neural_adapter","text":"NeuralPDE.neural_adapter","category":"page"},{"location":"manual/neural_adapters/#NeuralPDE.neural_adapter","page":"Transfer Learning with neural_adapter","title":"NeuralPDE.neural_adapter","text":"neural_adapter(loss, init_params, pde_system, strategy)\n\nTrains a neural network using the results from one already obtained prediction.\n\nPositional Arguments\n\nloss: the body of loss function,\ninit_params: the initial parameter of the neural network,\npde_system: PDEs are defined using the ModelingToolkit.jl,\nstrategy: determines which training strategy will be used.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/ode_parameter_estimation/#Parameter-Estimation-with-Physics-Informed-Neural-Networks-for-ODEs","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with Physics-Informed Neural Networks for ODEs","text":"","category":"section"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"Consider the lotka volterra system","category":"page"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"with Physics-Informed Neural Networks. Now we would consider the case where we want to optimize the parameters alpha, beta, gamma and delta.","category":"page"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"We start by defining the problem:","category":"page"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"using NeuralPDE, OrdinaryDiffEq\nusing Lux, Random\nusing OptimizationOptimJL, LineSearches\nusing Plots\nusing Test # hide\n\nfunction lv(u, p, t)\n    u₁, u₂ = u\n    α, β, γ, δ = p\n    du₁ = α * u₁ - β * u₁ * u₂\n    du₂ = δ * u₁ * u₂ - γ * u₂\n    [du₁, du₂]\nend\n\ntspan = (0.0, 5.0)\nu0 = [5.0, 5.0]\nprob = ODEProblem(lv, u0, tspan, [1.0, 1.0, 1.0, 1.0])","category":"page"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"As we want to estimate the parameters as well, lets get some data.","category":"page"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"true_p = [1.5, 1.0, 3.0, 1.0]\nprob_data = remake(prob, p = true_p)\nsol_data = solve(prob_data, Tsit5(), saveat = 0.01)\nt_ = sol_data.t\nu_ = reduce(hcat, sol_data.u)","category":"page"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"Now, lets define a neural network for the PINN using Lux.jl.","category":"page"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"rng = Random.default_rng()\nRandom.seed!(rng, 0)\nn = 15\nchain = Lux.Chain(\n    Lux.Dense(1, n, Lux.σ),\n    Lux.Dense(n, n, Lux.σ),\n    Lux.Dense(n, n, Lux.σ),\n    Lux.Dense(n, 2)\n)\nps, st = Lux.setup(rng, chain) |> Lux.f64","category":"page"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"Next we define an additional loss term to in the total loss which measures how the neural network's predictions is fitting the data.","category":"page"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"function additional_loss(phi, θ)\n    return sum(abs2, phi(t_, θ) .- u_) / size(u_, 2)\nend","category":"page"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"Next we define the optimizer and NNODE which is then plugged into the solve call.","category":"page"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"opt = LBFGS(linesearch = BackTracking())\nalg = NNODE(chain, opt, ps; strategy = WeightedIntervalTraining([0.7, 0.2, 0.1], 500),\n    param_estim = true, additional_loss = additional_loss)","category":"page"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"Now we have all the pieces to solve the optimization problem.","category":"page"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"sol = solve(prob, alg, verbose = true, abstol = 1e-8, maxiters = 5000, saveat = t_)\n@test sol.k.u.p≈true_p rtol=1e-2 # hide","category":"page"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"Let's plot the predictions from the PINN and compare it to the data.","category":"page"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"plot(sol, labels = [\"u1_pinn\" \"u2_pinn\"])\nplot!(sol_data, labels = [\"u1_data\" \"u2_data\"])","category":"page"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"We can see it is a good fit! Now lets see if we have the parameters of the equation also estimated correctly or not.","category":"page"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"sol.k.u.p","category":"page"},{"location":"tutorials/ode_parameter_estimation/","page":"Parameter Estimation with PINNs for ODEs","title":"Parameter Estimation with PINNs for ODEs","text":"We can see it is indeed close to the true values [1.5, 1.0, 3.0, 1.0].","category":"page"},{"location":"examples/wave/#1D-Wave-Equation-with-Dirichlet-boundary-conditions","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"","category":"section"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"Let's solve this 1-dimensional wave equation:","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"beginalign*\n^2_t u(x t) = c^2 ^2_x u(x t) quad  textsffor all  0  x  1 text and  t  0   \nu(0 t) = u(1 t) = 0 quad  textsffor all  t  0   \nu(x 0) = x (1-x)     quad  textsffor all  0  x  1   \n_t u(x 0) = 0       quad  textsffor all  0  x  1   \nendalign*","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"with grid discretization dx = 0.1 and physics-informed neural networks.","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"Further, the solution of this equation with the given boundary conditions is presented.","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"using NeuralPDE, Lux, Optimization, OptimizationOptimJL\nusing ModelingToolkit: Interval\n\n@parameters t, x\n@variables u(..)\nDxx = Differential(x)^2\nDtt = Differential(t)^2\nDt = Differential(t)\n\n#2D PDE\nC = 1\neq = Dtt(u(t, x)) ~ C^2 * Dxx(u(t, x))\n\n# Initial and boundary conditions\nbcs = [u(t, 0) ~ 0.0,# for all t > 0\n    u(t, 1) ~ 0.0,# for all t > 0\n    u(0, x) ~ x * (1.0 - x), #for all 0 < x < 1\n    Dt(u(0, x)) ~ 0.0] #for all  0 < x < 1]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, 1.0),\n    x ∈ Interval(0.0, 1.0)]\n# Discretization\ndx = 0.1\n\n# Neural network\nchain = Lux.Chain(Dense(2, 16, Lux.σ), Dense(16, 16, Lux.σ), Dense(16, 1))\ndiscretization = PhysicsInformedNN(chain, GridTraining(dx))\n\n@named pde_system = PDESystem(eq, bcs, domains, [t, x], [u(t, x)])\nprob = discretize(pde_system, discretization)\n\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\n# optimizer\nopt = OptimizationOptimJL.BFGS()\nres = Optimization.solve(prob, opt; callback = callback, maxiters = 1200)\nphi = discretization.phi","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"We can plot the predicted solution of the PDE and compare it with the analytical solution to plot the relative error.","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"using Plots\n\nts, xs = [infimum(d.domain):dx:supremum(d.domain) for d in domains]\nfunction analytic_sol_func(t, x)\n    sum([(8 / (k^3 * pi^3)) * sin(k * pi * x) * cos(C * k * pi * t) for k in 1:2:50000])\nend\n\nu_predict = reshape([first(phi([t, x], res.u)) for t in ts for x in xs],\n    (length(ts), length(xs)))\nu_real = reshape([analytic_sol_func(t, x) for t in ts for x in xs],\n    (length(ts), length(xs)))\n\ndiff_u = abs.(u_predict .- u_real)\np1 = plot(ts, xs, u_real, linetype = :contourf, title = \"analytic\");\np2 = plot(ts, xs, u_predict, linetype = :contourf, title = \"predict\");\np3 = plot(ts, xs, diff_u, linetype = :contourf, title = \"error\");\nplot(p1, p2, p3)","category":"page"},{"location":"examples/wave/#1D-Damped-Wave-Equation-with-Dirichlet-boundary-conditions","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Damped Wave Equation with Dirichlet boundary conditions","text":"","category":"section"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"Now let's solve the 1-dimensional wave equation with damping.","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"beginaligned\nfracpartial^2 u(tx)partial x^2 = frac1c^2 fracpartial^2 u(tx)partial t^2 + v fracpartial u(tx)partial t \nu(t 0) = u(t L) = 0 \nu(0 x) = x(1-x) \nu_t(0 x) = 1 - 2x \nendaligned","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"with grid discretization dx = 0.05 and physics-informed neural networks. Here, we take advantage of adaptive derivative to increase accuracy.","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nusing Plots, Printf\nusing ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u(..) Dxu(..) Dtu(..) O1(..) O2(..)\nDxx = Differential(x)^2\nDtt = Differential(t)^2\nDx = Differential(x)\nDt = Differential(t)\n\n# Constants\nv = 3\nb = 2\nL = 1.0\n@assert b > 0 && b < 2π / (L * v)\n\n# 1D damped wave\neq = Dx(Dxu(t, x)) ~ 1 / v^2 * Dt(Dtu(t, x)) + b * Dtu(t, x)\n\n# Initial and boundary conditions\nbcs_ = [u(t, 0) ~ 0.0,# for all t > 0\n    u(t, L) ~ 0.0,# for all t > 0\n    u(0, x) ~ x * (1.0 - x), # for all 0 < x < 1\n    Dtu(0, x) ~ 1 - 2x # for all  0 < x < 1\n]\n\nep = (cbrt(eps(eltype(Float64))))^2 / 6\n\nder = [Dxu(t, x) ~ Dx(u(t, x)) + ep * O1(t, x),\n    Dtu(t, x) ~ Dt(u(t, x)) + ep * O2(t, x)]\n\nbcs = [bcs_; der]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, L),\n    x ∈ Interval(0.0, L)]\n\n# Neural network\ninn = 25\ninnd = 4\nchain = [[Lux.Chain(Dense(2, inn, Lux.tanh),\n              Dense(inn, inn, Lux.tanh),\n              Dense(inn, inn, Lux.tanh),\n              Dense(inn, 1)) for _ in 1:3]\n         [Lux.Chain(Dense(2, innd, Lux.tanh), Dense(innd, 1)) for _ in 1:2]]\n\nstrategy = GridTraining(0.02)\ndiscretization = PhysicsInformedNN(chain, strategy;)\n\n@named pde_system = PDESystem(eq, bcs, domains, [t, x],\n    [u(t, x), Dxu(t, x), Dtu(t, x), O1(t, x), O2(t, x)])\nprob = discretize(pde_system, discretization)\nsym_prob = NeuralPDE.symbolic_discretize(pde_system, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p.u), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p.u), bcs_inner_loss_functions))\n    return false\nend\n\nres = Optimization.solve(prob, BFGS(); maxiters = 2000)\nprob = remake(prob, u0 = res.u)\nres = Optimization.solve(prob, BFGS(); maxiters = 2000)\n\nphi = discretization.phi[1]\n\n# Analysis\nts, xs = [infimum(d.domain):0.05:supremum(d.domain) for d in domains]\n\nμ_n(k) = (v * sqrt(4 * k^2 * π^2 - b^2 * L^2 * v^2)) / (2 * L)\nfunction b_n(k)\n    2 / L * -(L^2 *\n      ((2 * π * L - π) * k * sin(π * k) + ((π^2 - π^2 * L) * k^2 + 2 * L) * cos(π * k) -\n       2 * L)) / (π^3 * k^3)\nend # vegas((x, ϕ) -> ϕ[1] = sin(k * π * x[1]) * f(x[1])).integral[1]\nfunction a_n(k)\n    2 / -(L * μ_n(k)) * (L * (((2 * π * L^2 - π * L) * b * k * sin(π * k) +\n       ((π^2 * L - π^2 * L^2) * b * k^2 + 2 * L^2 * b) * cos(π * k) - 2 * L^2 * b) * v^2 +\n      4 * π * L * k * sin(π * k) + (2 * π^2 - 4 * π^2 * L) * k^2 * cos(π * k) -\n      2 * π^2 * k^2)) / (2 * π^3 * k^3)\nend\n\n# Plot\nfunction analytic_sol_func(t, x)\n    sum([sin((k * π * x) / L) * exp(-v^2 * b * t / 2) *\n         (a_n(k) * sin(μ_n(k) * t) + b_n(k) * cos(μ_n(k) * t)) for k in 1:2:100])\nend # TODO replace 10 with 500\nanim = @animate for t in ts\n    @info \"Time $t...\"\n    sol = [analytic_sol_func(t, x) for x in xs]\n    sol_p = [first(phi([t, x], res.u.depvar.u)) for x in xs]\n    plot(sol, label = \"analytic\", ylims = [0, 0.1])\n    title = @sprintf(\"t = %.3f\", t)\n    plot!(sol_p, label = \"predict\", ylims = [0, 0.1], title = title)\nend\ngif(anim, \"1Dwave_damped_adaptive.gif\", fps = 200)\n\n# Surface plot\nts, xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\nu_predict = reshape(\n    [first(phi([t, x], res.u.depvar.u)) for\n     t in ts for x in xs], (length(ts), length(xs)))\nu_real = reshape([analytic_sol_func(t, x) for t in ts for x in xs],\n    (length(ts), length(xs)))\n\ndiff_u = abs.(u_predict .- u_real)\np1 = plot(ts, xs, u_real, linetype = :contourf, title = \"analytic\");\np2 = plot(ts, xs, u_predict, linetype = :contourf, title = \"predict\");\np3 = plot(ts, xs, diff_u, linetype = :contourf, title = \"error\");\nplot(p1, p2, p3)","category":"page"},{"location":"tutorials/dae/#Solving-DAEs-with-Physics-Informed-Neural-Networks-(PINNs)","page":"PINNs DAEs","title":"Solving DAEs with Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"tutorials/dae/","page":"PINNs DAEs","title":"PINNs DAEs","text":"note: Note\nIt is highly recommended you first read the solving ordinary differential equations with DifferentialEquations.jl tutorial before reading this tutorial.","category":"page"},{"location":"tutorials/dae/","page":"PINNs DAEs","title":"PINNs DAEs","text":"This tutorial is an introduction to using physics-informed neural networks (PINNs) for solving differential algebraic equations (DAEs).","category":"page"},{"location":"tutorials/dae/#Solving-an-DAE-with-PINNs","page":"PINNs DAEs","title":"Solving an DAE with PINNs","text":"","category":"section"},{"location":"tutorials/dae/","page":"PINNs DAEs","title":"PINNs DAEs","text":"Let's solve a simple DAE system:","category":"page"},{"location":"tutorials/dae/","page":"PINNs DAEs","title":"PINNs DAEs","text":"using NeuralPDE\nusing Random\nusing OrdinaryDiffEq, Statistics\nusing Lux, OptimizationOptimisers\n\nexample = (du, u, p, t) -> [cos(2pi * t) - du[1], u[2] + cos(2pi * t) - du[2]]\nu₀ = [1.0, -1.0]\ndu₀ = [0.0, 0.0]\ntspan = (0.0, 1.0)","category":"page"},{"location":"tutorials/dae/","page":"PINNs DAEs","title":"PINNs DAEs","text":"Differential_vars is a logical array which declares which variables are the differential (non-algebraic) vars","category":"page"},{"location":"tutorials/dae/","page":"PINNs DAEs","title":"PINNs DAEs","text":"differential_vars = [true, false]","category":"page"},{"location":"tutorials/dae/","page":"PINNs DAEs","title":"PINNs DAEs","text":"prob = DAEProblem(example, du₀, u₀, tspan; differential_vars = differential_vars)\nchain = Lux.Chain(Lux.Dense(1, 15, cos), Lux.Dense(15, 15, sin), Lux.Dense(15, 2))\nopt = OptimizationOptimisers.Adam(0.1)\nalg = NNDAE(chain, opt; autodiff = false)\nsol = solve(prob, alg, verbose = true, dt = 1 / 100.0, maxiters = 3000, abstol = 1e-10)","category":"page"},{"location":"tutorials/dae/","page":"PINNs DAEs","title":"PINNs DAEs","text":"Now lets compare the predictions from the learned network with the ground truth which we can obtain by numerically solving the DAE.","category":"page"},{"location":"tutorials/dae/","page":"PINNs DAEs","title":"PINNs DAEs","text":"function example1(du, u, p, t)\n    du[1] = cos(2pi * t)\n    du[2] = u[2] + cos(2pi * t)\n    nothing\nend\nM = [1.0 0.0; 0.0 0.0]\nf = ODEFunction(example1, mass_matrix = M)\nprob_mm = ODEProblem(f, u₀, tspan)\nground_sol = solve(prob_mm, Rodas5(), reltol = 1e-8, abstol = 1e-8)","category":"page"},{"location":"tutorials/dae/","page":"PINNs DAEs","title":"PINNs DAEs","text":"using Plots\nplot(ground_sol, tspan = tspan, layout = (2, 1), label = \"ground truth\")\nplot!(sol, tspan = tspan, layout = (2, 1), label = \"dae with pinns\")","category":"page"},{"location":"tutorials/low_level_2/#Using-ahmc_bayesian_pinn_pde-with-the-BayesianPINN-Discretizer-for-the-Kuramoto–Sivashinsky-equation","page":"Bayesian PINNs for PDEs","title":"Using ahmc_bayesian_pinn_pde with the BayesianPINN Discretizer for the Kuramoto–Sivashinsky equation","text":"","category":"section"},{"location":"tutorials/low_level_2/","page":"Bayesian PINNs for PDEs","title":"Bayesian PINNs for PDEs","text":"Consider the Kuramoto–Sivashinsky equation:","category":"page"},{"location":"tutorials/low_level_2/","page":"Bayesian PINNs for PDEs","title":"Bayesian PINNs for PDEs","text":"_t u(x t) + u(x t) _x u(x t) + alpha ^2_x u(x t) + beta ^3_x u(x t) + gamma ^4_x u(x t) =  0  ","category":"page"},{"location":"tutorials/low_level_2/","page":"Bayesian PINNs for PDEs","title":"Bayesian PINNs for PDEs","text":"where alpha = gamma = 1 and beta = 4. The exact solution is:","category":"page"},{"location":"tutorials/low_level_2/","page":"Bayesian PINNs for PDEs","title":"Bayesian PINNs for PDEs","text":"u_e(x t) = 11 + 15 tanh theta - 15 tanh^2 theta - 15 tanh^3 theta  ","category":"page"},{"location":"tutorials/low_level_2/","page":"Bayesian PINNs for PDEs","title":"Bayesian PINNs for PDEs","text":"where theta = t - x2 and with initial and boundary conditions:","category":"page"},{"location":"tutorials/low_level_2/","page":"Bayesian PINNs for PDEs","title":"Bayesian PINNs for PDEs","text":"beginalign*\n    u(  x 0) =     u_e(  x 0)  \n    u( 10 t) =     u_e( 10 t)  \n    u(-10 t) =     u_e(-10 t)  \n_x u( 10 t) = _x u_e( 10 t)  \n_x u(-10 t) = _x u_e(-10 t)  \nendalign*","category":"page"},{"location":"tutorials/low_level_2/","page":"Bayesian PINNs for PDEs","title":"Bayesian PINNs for PDEs","text":"With Bayesian Physics-Informed Neural Networks, here is an example of using BayesianPINN discretization with ahmc_bayesian_pinn_pde :","category":"page"},{"location":"tutorials/low_level_2/","page":"Bayesian PINNs for PDEs","title":"Bayesian PINNs for PDEs","text":"using NeuralPDE, Flux, Lux, ModelingToolkit, LinearAlgebra, AdvancedHMC\nimport ModelingToolkit: Interval, infimum, supremum, Distributions\nusing Plots, MonteCarloMeasurements\n\n@parameters x, t, α\n@variables u(..)\nDt = Differential(t)\nDx = Differential(x)\nDx2 = Differential(x)^2\nDx3 = Differential(x)^3\nDx4 = Differential(x)^4\n\n# α = 1\nβ = 4\nγ = 1\neq = Dt(u(x, t)) + u(x, t) * Dx(u(x, t)) + α * Dx2(u(x, t)) + β * Dx3(u(x, t)) + γ * Dx4(u(x, t)) ~ 0\n\nu_analytic(x, t; z = -x / 2 + t) = 11 + 15 * tanh(z) - 15 * tanh(z)^2 - 15 * tanh(z)^3\ndu(x, t; z = -x / 2 + t) = 15 / 2 * (tanh(z) + 1) * (3 * tanh(z) - 1) * sech(z)^2\n\nbcs = [u(x, 0) ~ u_analytic(x, 0),\n    u(-10, t) ~ u_analytic(-10, t),\n    u(10, t) ~ u_analytic(10, t),\n    Dx(u(-10, t)) ~ du(-10, t),\n    Dx(u(10, t)) ~ du(10, t)]\n\n# Space and time domains\ndomains = [x ∈ Interval(-10.0, 10.0),\n    t ∈ Interval(0.0, 1.0)]\n\n# Discretization\ndx = 0.4;\ndt = 0.2;\n\n# Function to compute analytical solution at a specific point (x, t)\nfunction u_analytic_point(x, t)\n    z = -x / 2 + t\n    return 11 + 15 * tanh(z) - 15 * tanh(z)^2 - 15 * tanh(z)^3\nend\n\n# Function to generate the dataset matrix\nfunction generate_dataset_matrix(domains, dx, dt)\n    x_values = -10:dx:10\n    t_values = 0.0:dt:1.0\n\n    dataset = []\n\n    for t in t_values\n        for x in x_values\n            u_value = u_analytic_point(x, t)\n            push!(dataset, [u_value, x, t])\n        end\n    end\n\n    return vcat([data' for data in dataset]...)\nend\n\ndatasetpde = [generate_dataset_matrix(domains, dx, dt)]\n\n# noise to dataset\nnoisydataset = deepcopy(datasetpde)\nnoisydataset[1][:, 1] = noisydataset[1][:, 1] .+\n                        randn(size(noisydataset[1][:, 1])) .* 5 / 100 .*\n                        noisydataset[1][:, 1]","category":"page"},{"location":"tutorials/low_level_2/","page":"Bayesian PINNs for PDEs","title":"Bayesian PINNs for PDEs","text":"Plotting dataset, added noise is set at 5%.","category":"page"},{"location":"tutorials/low_level_2/","page":"Bayesian PINNs for PDEs","title":"Bayesian PINNs for PDEs","text":"plot(datasetpde[1][:, 2], datasetpde[1][:, 1], title = \"Dataset from Analytical Solution\")\nplot!(noisydataset[1][:, 2], noisydataset[1][:, 1])","category":"page"},{"location":"tutorials/low_level_2/","page":"Bayesian PINNs for PDEs","title":"Bayesian PINNs for PDEs","text":"# Neural network\nchain = Lux.Chain(Lux.Dense(2, 8, Lux.tanh),\n    Lux.Dense(8, 8, Lux.tanh),\n    Lux.Dense(8, 1))\n\ndiscretization = NeuralPDE.BayesianPINN([chain],\n    GridTraining([dx, dt]), param_estim = true, dataset = [noisydataset, nothing])\n\n@named pde_system = PDESystem(eq,\n    bcs,\n    domains,\n    [x, t],\n    [u(x, t)],\n    [α],\n    defaults = Dict([α => 0.5]))\n\nsol1 = ahmc_bayesian_pinn_pde(pde_system,\n    discretization;\n    draw_samples = 100, Kernel = AdvancedHMC.NUTS(0.8),\n    bcstd = [0.2, 0.2, 0.2, 0.2, 0.2],\n    phystd = [1.0], l2std = [0.05], param = [Distributions.LogNormal(0.5, 2)],\n    priorsNNw = (0.0, 10.0),\n    saveats = [1 / 100.0, 1 / 100.0], progress = true)","category":"page"},{"location":"tutorials/low_level_2/","page":"Bayesian PINNs for PDEs","title":"Bayesian PINNs for PDEs","text":"And some analysis:","category":"page"},{"location":"tutorials/low_level_2/","page":"Bayesian PINNs for PDEs","title":"Bayesian PINNs for PDEs","text":"phi = discretization.phi[1]\nxs, ts = [infimum(d.domain):dx:supremum(d.domain)\n          for (d, dx) in zip(domains, [dx / 10, dt])]\nu_predict = [[first(pmean(phi([x, t], sol1.estimated_nn_params[1]))) for x in xs]\n             for t in ts]\nu_real = [[u_analytic(x, t) for x in xs] for t in ts]\ndiff_u = [[abs(u_analytic(x, t) - first(pmean(phi([x, t], sol1.estimated_nn_params[1]))))\n           for x in xs]\n          for t in ts]\n\np1 = plot(xs, u_predict, title = \"predict\")\np2 = plot(xs, u_real, title = \"analytic\")\np3 = plot(xs, diff_u, title = \"error\")\nplot(p1, p2, p3)","category":"page"},{"location":"manual/training_strategies/#Training-Strategies","page":"Training Strategies","title":"Training Strategies","text":"","category":"section"},{"location":"manual/training_strategies/","page":"Training Strategies","title":"Training Strategies","text":"Training strategies are the choices for how the points are sampled for the definition of the physics-informed loss.","category":"page"},{"location":"manual/training_strategies/#Recommendations","page":"Training Strategies","title":"Recommendations","text":"","category":"section"},{"location":"manual/training_strategies/","page":"Training Strategies","title":"Training Strategies","text":"QuasiRandomTraining with its default LatinHyperCubeSample() is a well-rounded training strategy which can be used for most situations. It scales well for high dimensional spaces and is GPU-compatible. QuadratureTraining can lead to faster or more robust convergence with one of the H-Cubature or P-Cubature methods, but are not currently GPU compatible. For very high dimensional cases, QuadratureTraining with an adaptive Monte Carlo quadrature method, such as CubaVegas, can be beneficial for difficult or stiff problems.","category":"page"},{"location":"manual/training_strategies/","page":"Training Strategies","title":"Training Strategies","text":"GridTraining should only be used for testing purposes and should not be relied upon for real training cases. StochasticTraining achieves a lower convergence rate in the quasi-Monte Carlo methods and thus QuasiRandomTraining should be preferred in most cases. WeightedIntervalTraining can only be used with ODEs (NNODE).","category":"page"},{"location":"manual/training_strategies/#API","page":"Training Strategies","title":"API","text":"","category":"section"},{"location":"manual/training_strategies/","page":"Training Strategies","title":"Training Strategies","text":"GridTraining\nStochasticTraining\nQuasiRandomTraining\nQuadratureTraining\nWeightedIntervalTraining","category":"page"},{"location":"manual/training_strategies/#NeuralPDE.GridTraining","page":"Training Strategies","title":"NeuralPDE.GridTraining","text":"GridTraining(dx)\n\nA training strategy that uses the grid points in a multidimensional grid with spacings dx. If the grid is multidimensional, then dx is expected to be an array of dx values matching the dimension of the domain, corresponding to the grid spacing in each dimension.\n\nPositional Arguments\n\ndx: the discretization of the grid.\n\n\n\n\n\n","category":"type"},{"location":"manual/training_strategies/#NeuralPDE.StochasticTraining","page":"Training Strategies","title":"NeuralPDE.StochasticTraining","text":"StochasticTraining(points; bcs_points = points)\n\nPositional Arguments\n\npoints: number of points in random select training set\n\nKeyword Arguments\n\nbcs_points: number of points in random select training set for boundary conditions (by default, it equals points).\n\n\n\n\n\n","category":"type"},{"location":"manual/training_strategies/#NeuralPDE.QuasiRandomTraining","page":"Training Strategies","title":"NeuralPDE.QuasiRandomTraining","text":"QuasiRandomTraining(points; bcs_points = points,\n                            sampling_alg = LatinHypercubeSample(), resampling = true,\n                            minibatch = 0)\n\nA training strategy which uses quasi-Monte Carlo sampling for low discrepancy sequences that accelerate the convergence in high dimensional spaces over pure random sequences.\n\nPositional Arguments\n\npoints:  the number of quasi-random points in a sample\n\nKeyword Arguments\n\nbcs_points: the number of quasi-random points in a sample for boundary conditions (by default, it equals points),\nsampling_alg: the quasi-Monte Carlo sampling algorithm,\nresampling: if it's false - the full training set is generated in advance before training,  and at each iteration, one subset is randomly selected out of the batch.  Ff it's true - the training set isn't generated beforehand, and one set of quasi-random  points is generated directly at each iteration in runtime. In this case, minibatch has no effect,\nminibatch: the number of subsets, if resampling == false.\n\nFor more information, see QuasiMonteCarlo.jl.\n\n\n\n\n\n","category":"type"},{"location":"manual/training_strategies/#NeuralPDE.QuadratureTraining","page":"Training Strategies","title":"NeuralPDE.QuadratureTraining","text":"QuadratureTraining(; quadrature_alg = CubatureJLh(),\n                    reltol = 1e-6, abstol = 1e-3,\n                    maxiters = 1_000, batch = 100)\n\nA training strategy which treats the loss function as the integral of ||condition|| over the domain. Uses an Integrals.jl algorithm for computing the (adaptive) quadrature of this loss with respect to the chosen tolerances, with a batching batch corresponding to the maximum number of points to evaluate in a given integrand call.\n\nKeyword Arguments\n\nquadrature_alg: quadrature algorithm,\nreltol: relative tolerance,\nabstol: absolute tolerance,\nmaxiters: the maximum number of iterations in quadrature algorithm,\nbatch: the preferred number of points to batch.\n\nFor more information on the argument values and algorithm choices, see Integrals.jl.\n\n\n\n\n\n","category":"type"},{"location":"manual/training_strategies/#NeuralPDE.WeightedIntervalTraining","page":"Training Strategies","title":"NeuralPDE.WeightedIntervalTraining","text":"WeightedIntervalTraining(weights, samples)\n\nA training strategy that generates points for training based on the given inputs.  We split the timespan into equal segments based on the number of weights,  then sample points in each segment based on that segments corresponding weight, such that the total number of sampled points is equivalent to the given samples\n\nPositional Arguments\n\nweights: A vector of weights that should sum to 1, representing the proportion of samples at each interval.\npoints: the total number of samples that we want, across the entire time span\n\nLimitations\n\nThis training strategy can only be used with ODEs (NNODE).\n\n\n\n\n\n","category":"type"},{"location":"examples/heterogeneous/#PDEs-with-Dependent-Variables-on-Heterogeneous-Domains","page":"PDEs with Dependent Variables on Heterogeneous Domains","title":"PDEs with Dependent Variables on Heterogeneous Domains","text":"","category":"section"},{"location":"examples/heterogeneous/","page":"PDEs with Dependent Variables on Heterogeneous Domains","title":"PDEs with Dependent Variables on Heterogeneous Domains","text":"A differential equation is said to have heterogeneous domains when its dependent variables depend on different independent variables:","category":"page"},{"location":"examples/heterogeneous/","page":"PDEs with Dependent Variables on Heterogeneous Domains","title":"PDEs with Dependent Variables on Heterogeneous Domains","text":"u(x) + w(x v) = fracpartial w(x v)partial w","category":"page"},{"location":"examples/heterogeneous/","page":"PDEs with Dependent Variables on Heterogeneous Domains","title":"PDEs with Dependent Variables on Heterogeneous Domains","text":"Here, we write an arbitrary heterogeneous system:","category":"page"},{"location":"examples/heterogeneous/","page":"PDEs with Dependent Variables on Heterogeneous Domains","title":"PDEs with Dependent Variables on Heterogeneous Domains","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nimport ModelingToolkit: Interval\n\n@parameters x y\n@variables p(..) q(..) r(..) s(..)\nDx = Differential(x)\nDy = Differential(y)\n\n# 2D PDE\neq = p(x) + q(y) + Dx(r(x, y)) + Dy(s(y, x)) ~ 0\n\n# Initial and boundary conditions\nbcs = [p(1) ~ 0.0f0, q(-1) ~ 0.0f0,\n    r(x, -1) ~ 0.0f0, r(1, y) ~ 0.0f0,\n    s(y, 1) ~ 0.0f0, s(-1, x) ~ 0.0f0]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n    y ∈ Interval(0.0, 1.0)]\n\nnumhid = 3\nchains = [[Lux.Chain(Dense(1, numhid, Lux.σ), Dense(numhid, numhid, Lux.σ),\n               Dense(numhid, 1)) for i in 1:2]\n          [Lux.Chain(Dense(2, numhid, Lux.σ), Dense(numhid, numhid, Lux.σ),\n               Dense(numhid, 1)) for i in 1:2]]\ndiscretization = NeuralPDE.PhysicsInformedNN(chains, QuadratureTraining())\n\n@named pde_system = PDESystem(eq, bcs, domains, [x, y], [p(x), q(y), r(x, y), s(y, x)])\nprob = SciMLBase.discretize(pde_system, discretization)\n\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = Optimization.solve(prob, BFGS(); maxiters = 100)","category":"page"},{"location":"examples/ks/#Kuramoto–Sivashinsky-equation","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"","category":"section"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"Let's consider the Kuramoto–Sivashinsky equation, which contains a 4th-order derivative:","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"_t u(x t) + u(x t) _x u(x t) + alpha ^2_x u(x t) + beta ^3_x u(x t) + gamma ^4_x u(x t) =  0  ","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"where alpha = gamma = 1 and beta = 4. The exact solution is:","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"u_e(x t) = 11 + 15 tanh theta - 15 tanh^2 theta - 15 tanh^3 theta  ","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"where theta = t - x2 and with initial and boundary conditions:","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"beginalign*\n    u(  x 0) =     u_e(  x 0)  \n    u( 10 t) =     u_e( 10 t)  \n    u(-10 t) =     u_e(-10 t)  \n_x u( 10 t) = _x u_e( 10 t)  \n_x u(-10 t) = _x u_e(-10 t)  \nendalign*","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"We use physics-informed neural networks.","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x, t\n@variables u(..)\nDt = Differential(t)\nDx = Differential(x)\nDx2 = Differential(x)^2\nDx3 = Differential(x)^3\nDx4 = Differential(x)^4\n\nα = 1\nβ = 4\nγ = 1\neq = Dt(u(x, t)) + u(x, t) * Dx(u(x, t)) + α * Dx2(u(x, t)) + β * Dx3(u(x, t)) + γ * Dx4(u(x, t)) ~ 0\n\nu_analytic(x, t; z = -x / 2 + t) = 11 + 15 * tanh(z) - 15 * tanh(z)^2 - 15 * tanh(z)^3\ndu(x, t; z = -x / 2 + t) = 15 / 2 * (tanh(z) + 1) * (3 * tanh(z) - 1) * sech(z)^2\n\nbcs = [u(x, 0) ~ u_analytic(x, 0),\n    u(-10, t) ~ u_analytic(-10, t),\n    u(10, t) ~ u_analytic(10, t),\n    Dx(u(-10, t)) ~ du(-10, t),\n    Dx(u(10, t)) ~ du(10, t)]\n\n# Space and time domains\ndomains = [x ∈ Interval(-10.0, 10.0),\n    t ∈ Interval(0.0, 1.0)]\n# Discretization\ndx = 0.4;\ndt = 0.2;\n\n# Neural network\nchain = Lux.Chain(Dense(2, 12, Lux.σ), Dense(12, 12, Lux.σ), Dense(12, 1))\n\ndiscretization = PhysicsInformedNN(chain, GridTraining([dx, dt]))\n@named pde_system = PDESystem(eq, bcs, domains, [x, t], [u(x, t)])\nprob = discretize(pde_system, discretization)\n\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nopt = OptimizationOptimJL.BFGS()\nres = Optimization.solve(prob, opt; maxiters = 2000)\nphi = discretization.phi","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"And some analysis:","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"using Plots\n\nxs, ts = [infimum(d.domain):dx:supremum(d.domain)\n          for (d, dx) in zip(domains, [dx / 10, dt])]\n\nu_predict = [[first(phi([x, t], res.u)) for x in xs] for t in ts]\nu_real = [[u_analytic(x, t) for x in xs] for t in ts]\ndiff_u = [[abs(u_analytic(x, t) - first(phi([x, t], res.u))) for x in xs] for t in ts]\n\np1 = plot(xs, u_predict, title = \"predict\")\np2 = plot(xs, u_real, title = \"analytic\")\np3 = plot(xs, diff_u, title = \"error\")\nplot(p1, p2, p3)","category":"page"},{"location":"examples/nonlinear_hyperbolic/#Nonlinear-hyperbolic-system-of-PDEs","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"","category":"section"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"We may also solve hyperbolic systems like the following","category":"page"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"beginaligned\nfracpartial^2upartial t^2 = fracax^n fracpartialpartial x(x^n fracpartial upartial x) + u f(fracuw)  \nfracpartial^2wpartial t^2 = fracbx^n fracpartialpartial x(x^n fracpartial upartial x) + w g(fracuw)  \nendaligned","category":"page"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"where f and g are arbitrary functions. With initial and boundary conditions:","category":"page"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"beginaligned\nu(0x) = k * j0(ξ(0 x)) + y0(ξ(0 x)) \nu(t0) = k * j0(ξ(t 0)) + y0(ξ(t 0)) \nu(t1) = k * j0(ξ(t 1)) + y0(ξ(t 1)) \nw(0x) = j0(ξ(0 x)) + y0(ξ(0 x)) \nw(t0) = j0(ξ(t 0)) + y0(ξ(t 0)) \nw(t1) = j0(ξ(t 0)) + y0(ξ(t 0)) \nendaligned","category":"page"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"where k is a root of the algebraic (transcendental) equation f(k) = g(k), j0 and y0 are the Bessel functions, and ξ(t, x) is:","category":"page"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"beginaligned\nfracsqrtf(k)sqrtfracax^nsqrtfracax^n(t+1)^2 - (x+1)^2\nendaligned","category":"page"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"We solve this with Neural:","category":"page"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, Roots,\n      LineSearches\nusing SpecialFunctions\nusing Plots\nusing ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u(..), w(..)\nDx = Differential(x)\nDt = Differential(t)\nDtt = Differential(t)^2\n\n# Constants\na = 16\nb = 16\nn = 0\n\n# Arbitrary functions\nf(x) = x^2\ng(x) = 4 * cos(π * x)\nroot(x) = g(x) - f(x)\n\n# Analytic solution\nk = find_zero(root, (0, 1), Roots.Bisection())                # k is a root of the algebraic (transcendental) equation f(x) = g(x)\nξ(t, x) = sqrt(f(k)) / sqrt(a) * sqrt(a * (t + 1)^2 - (x + 1)^2)\nθ(t, x) = besselj0(ξ(t, x)) + bessely0(ξ(t, x))                     # Analytical solution to Klein-Gordon equation\nw_analytic(t, x) = θ(t, x)\nu_analytic(t, x) = k * θ(t, x)\n\n# Nonlinear system of hyperbolic equations\neqs = [Dtt(u(t, x)) ~ a / (x^n) * Dx(x^n * Dx(u(t, x))) + u(t, x) * f(u(t, x) / w(t, x)),\n    Dtt(w(t, x)) ~ b / (x^n) * Dx(x^n * Dx(w(t, x))) + w(t, x) * g(u(t, x) / w(t, x))]\n\n# Boundary conditions\nbcs = [u(0, x) ~ u_analytic(0, x),\n    w(0, x) ~ w_analytic(0, x),\n    u(t, 0) ~ u_analytic(t, 0),\n    w(t, 0) ~ w_analytic(t, 0),\n    u(t, 1) ~ u_analytic(t, 1),\n    w(t, 1) ~ w_analytic(t, 1)]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, 1.0),\n    x ∈ Interval(0.0, 1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain = [Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, 1)) for _ in 1:2]\n\nstrategy = QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain, strategy)\n\n@named pdesystem = PDESystem(eqs, bcs, domains, [t, x], [u(t, x), w(t, x)])\nprob = discretize(pdesystem, discretization)\nsym_prob = symbolic_discretize(pdesystem, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p.u), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p.u), bcs_inner_loss_functions))\n    return false\nend\n\nres = Optimization.solve(prob, BFGS(linesearch = BackTracking()); maxiters = 200)\n\nphi = discretization.phi\n\n# Analysis\nts, xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\ndepvars = [:u, :w]\nminimizers_ = [res.u.depvar[depvars[i]] for i in 1:length(chain)]\n\nanalytic_sol_func(t, x) = [u_analytic(t, x), w_analytic(t, x)]\nu_real = [[analytic_sol_func(t, x)[i] for t in ts for x in xs] for i in 1:2]\nu_predict = [[phi[i]([t, x], minimizers_[i])[1] for t in ts for x in xs] for i in 1:2]\ndiff_u = [abs.(u_real[i] .- u_predict[i]) for i in 1:2]\nps = []\nfor i in 1:2\n    p1 = plot(ts, xs, u_real[i], linetype = :contourf, title = \"u$i, analytic\")\n    p2 = plot(ts, xs, u_predict[i], linetype = :contourf, title = \"predict\")\n    p3 = plot(ts, xs, diff_u[i], linetype = :contourf, title = \"error\")\n    push!(ps, plot(p1, p2, p3))\nend","category":"page"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"ps[1]","category":"page"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"ps[2]","category":"page"},{"location":"tutorials/derivative_neural_network/#The-Derivative-neural-network-approximation","page":"The Derivative Neural Network Approximation","title":"The Derivative neural network approximation","text":"","category":"section"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"The accuracy and stability of numerical derivative decreases with each successive order. The accuracy of the entire solution is determined by the worst accuracy of one of the variables, in our case, the highest degree of the derivative. Meanwhile, the computational cost of automatic differentiation for higher orders grows as O(n^d), making even numerical differentiation much more efficient! Given these two bad choices, there exists an alternative which can improve training speed and accuracy: using a system to represent the derivatives directly.","category":"page"},{"location":"tutorials/derivative_neural_network/#Demonstration","page":"The Derivative Neural Network Approximation","title":"Demonstration","text":"","category":"section"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"Take the PDE system:","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"beginalign*\n_t^2 u_1(t x)  = _x^2 u_1(t x) + u_3(t x)  sin(pi x)  \n_t^2 u_2(t x)  = _x^2 u_2(t x) + u_3(t x)  cos(pi x)  \n0  = u_1(t x) sin(pi x) + u_2(t x) cos(pi x) - e^-t  \nendalign*","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"with the initial conditions:","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"beginalign*\nu_1(0 x)  = sin(pi x)  \n_t u_1(0 x)  = - sin(pi x)  \nu_2(0 x)  = cos(pi x)  \n_t u_2(0 x)  = - cos(pi x)  \nendalign*","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"and the boundary conditions:","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"beginalign*\nu_1(t 0)  = u_1(t 1) = 0  \nu_2(t 0)  = - u_2(t 1) = e^-t  \nendalign*","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"This is the same system as the system of equations example","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"The derivative neural network approximation is such an approach that using lower-order numeric derivatives and estimates higher-order derivatives with a neural network, so that allows an increase in the marginal precision for all optimization. Since u3 is only in the first and second equations, its accuracy during training is determined by the accuracy of the second numerical derivative u3(t,x) ~ (Dtt(u1(t,x)) -Dxx(u1(t,x))) / sin(pi*x).","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"We approximate the derivative of the neural network with another neural network Dt(u1(t,x)) ~ Dtu1(t,x) and train it along with other equations, and thus we avoid using the second numeric derivative Dt(Dtu1(t,x)).","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"using NeuralPDE, Lux, ModelingToolkit\nusing Optimization, OptimizationOptimisers, OptimizationOptimJL, LineSearches\nusing Plots\nusing ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\nDt = Differential(t)\nDx = Differential(x)\n@variables u1(..), u2(..), u3(..)\n@variables Dxu1(..) Dtu1(..) Dxu2(..) Dtu2(..)\n\neqs_ = [Dt(Dtu1(t, x)) ~ Dx(Dxu1(t, x)) + u3(t, x) * sin(pi * x),\n    Dt(Dtu2(t, x)) ~ Dx(Dxu2(t, x)) + u3(t, x) * cos(pi * x),\n    exp(-t) ~ u1(t, x) * sin(pi * x) + u2(t, x) * cos(pi * x)]\n\nbcs_ = [u1(0.0, x) ~ sin(pi * x),\n    u2(0.0, x) ~ cos(pi * x),\n    Dt(u1(0, x)) ~ -sin(pi * x),\n    Dt(u2(0, x)) ~ -cos(pi * x),\n    u1(t, 0.0) ~ 0.0,\n    u2(t, 0.0) ~ exp(-t),\n    u1(t, 1.0) ~ 0.0,\n    u2(t, 1.0) ~ -exp(-t)]\n\nder_ = [Dt(u1(t, x)) ~ Dtu1(t, x),\n    Dt(u2(t, x)) ~ Dtu2(t, x),\n    Dx(u1(t, x)) ~ Dxu1(t, x),\n    Dx(u2(t, x)) ~ Dxu2(t, x)]\n\nbcs__ = [bcs_; der_]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, 1.0),\n    x ∈ Interval(0.0, 1.0)]\n\ninput_ = length(domains)\nn = 15\nchain = [Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, 1)) for _ in 1:7]\n\ntraining_strategy = NeuralPDE.QuadratureTraining(;\n    batch = 200, reltol = 1e-6, abstol = 1e-6)\ndiscretization = NeuralPDE.PhysicsInformedNN(chain, training_strategy)\n\nvars = [u1(t, x), u2(t, x), u3(t, x), Dxu1(t, x), Dtu1(t, x), Dxu2(t, x), Dtu2(t, x)]\n@named pdesystem = PDESystem(eqs_, bcs__, domains, [t, x], vars)\nprob = NeuralPDE.discretize(pdesystem, discretization)\nsym_prob = NeuralPDE.symbolic_discretize(pdesystem, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions[1:7]\naprox_derivative_loss_functions = sym_prob.loss_functions.bc_loss_functions[9:end]\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p.u), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p.u), bcs_inner_loss_functions))\n    println(\"der_losses: \", map(l_ -> l_(p.u), aprox_derivative_loss_functions))\n    return false\nend\n\nres = Optimization.solve(prob, OptimizationOptimisers.Adam(0.01); maxiters = 2000)\nprob = remake(prob, u0 = res.u)\nres = Optimization.solve(prob, LBFGS(linesearch = BackTracking()); maxiters = 200)\n\nphi = discretization.phi","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"And some analysis:","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"using Plots\n\nts, xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\nminimizers_ = [res.u.depvar[sym_prob.depvars[i]] for i in 1:length(chain)]\n\nu1_real(t, x) = exp(-t) * sin(pi * x)\nu2_real(t, x) = exp(-t) * cos(pi * x)\nu3_real(t, x) = (1 + pi^2) * exp(-t)\nDxu1_real(t, x) = pi * exp(-t) * cos(pi * x)\nDtu1_real(t, x) = -exp(-t) * sin(pi * x)\nDxu2_real(t, x) = -pi * exp(-t) * sin(pi * x)\nDtu2_real(t, x) = -exp(-t) * cos(pi * x)\n\nfunction analytic_sol_func_all(t, x)\n    [u1_real(t, x), u2_real(t, x), u3_real(t, x),\n        Dxu1_real(t, x), Dtu1_real(t, x), Dxu2_real(t, x), Dtu2_real(t, x)]\nend\n\nu_real = [[analytic_sol_func_all(t, x)[i] for t in ts for x in xs] for i in 1:7]\nu_predict = [[phi[i]([t, x], minimizers_[i])[1] for t in ts for x in xs] for i in 1:7]\ndiff_u = [abs.(u_real[i] .- u_predict[i]) for i in 1:7]\nps = []\ntitles = [\"u1\", \"u2\", \"u3\", \"Dtu1\", \"Dtu2\", \"Dxu1\", \"Dxu2\"]\nfor i in 1:7\n    p1 = plot(ts, xs, u_real[i], linetype = :contourf, title = \"$(titles[i]), analytic\")\n    p2 = plot(ts, xs, u_predict[i], linetype = :contourf, title = \"predict\")\n    p3 = plot(ts, xs, diff_u[i], linetype = :contourf, title = \"error\")\n    push!(ps, plot(p1, p2, p3))\nend","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"ps[1]","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"ps[2]","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"ps[3]","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"ps[4]","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"ps[5]","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"ps[6]","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"ps[7]","category":"page"},{"location":"manual/bpinns/#BayesianPINN-Discretizer-for-PDESystems","page":"BayesianPINN Discretizer for PDESystems","title":"BayesianPINN Discretizer for PDESystems","text":"","category":"section"},{"location":"manual/bpinns/","page":"BayesianPINN Discretizer for PDESystems","title":"BayesianPINN Discretizer for PDESystems","text":"Using the Bayesian PINN solvers, we can solve general nonlinear PDEs, ODEs and also simultaneously perform parameter estimation on them.","category":"page"},{"location":"manual/bpinns/","page":"BayesianPINN Discretizer for PDESystems","title":"BayesianPINN Discretizer for PDESystems","text":"Note: The BPINN PDE solver also works for ODEs defined using ModelingToolkit, ModelingToolkit.jl PDESystem documentation. Despite this, the ODE specific BPINN solver BNNODE refer exists and uses NeuralPDE.ahmc_bayesian_pinn_ode at a lower level.","category":"page"},{"location":"manual/bpinns/#BayesianPINN-Discretizer-for-PDESystems-and-lower-level-Bayesian-PINN-Solver-calls-for-PDEs-and-ODEs.","page":"BayesianPINN Discretizer for PDESystems","title":"BayesianPINN Discretizer for PDESystems and lower level Bayesian PINN Solver calls for PDEs and ODEs.","text":"","category":"section"},{"location":"manual/bpinns/","page":"BayesianPINN Discretizer for PDESystems","title":"BayesianPINN Discretizer for PDESystems","text":"NeuralPDE.BayesianPINN\nNeuralPDE.ahmc_bayesian_pinn_ode\nNeuralPDE.ahmc_bayesian_pinn_pde","category":"page"},{"location":"manual/bpinns/#NeuralPDE.BayesianPINN","page":"BayesianPINN Discretizer for PDESystems","title":"NeuralPDE.BayesianPINN","text":"BayesianPINN(chain,\n              strategy;\n              init_params = nothing,\n              phi = nothing,\n              param_estim = false,\n              additional_loss = nothing,\n              adaptive_loss = nothing,\n              logger = nothing,\n              log_options = LogOptions(),\n              iteration = nothing,\n              dataset = nothing,\n              kwargs...)\n\nA discretize algorithm for the ModelingToolkit PDESystem interface, which transforms a PDESystem into a likelihood function used for HMC based Posterior Sampling Algorithms AdvancedHMC.jl which is later optimized upon to give the Solution Distribution of the PDE, using the Physics-Informed Neural Networks (PINN) methodology.\n\nPositional Arguments\n\nchain: a vector of Lux.jl chains with a d-dimensional input and a 1-dimensional output corresponding to each of the dependent variables. Note that this specification respects the order of the dependent variables as specified in the PDESystem.\nstrategy: determines which training strategy will be used. See the Training Strategy documentation for more details.\n\nKeyword Arguments\n\nDataset: A vector of matrix, each matrix for ith dependant variable and first col in matrix is for dependant variables, remaining columns for independent variables. Needed for inverse problem solving.\ninit_params: the initial parameters of the neural networks. If init_params is not given, then the neural network default parameters are used. Note that for Lux, the default will convert to Float64.\nphi: a trial solution, specified as phi(x,p) where x is the coordinates vector for the dependent variable and p are the weights of the phi function (generally the weights of the neural network defining phi). By default, this is generated from the chain. This should only be used to more directly impose functional information in the training problem, for example imposing the boundary condition by the test function formulation.\nadaptive_loss: (STILL WIP), the choice for the adaptive loss function. See the adaptive loss page for more details. Defaults to no adaptivity.\nadditional_loss: a function additional_loss(phi, θ, p_) where phi are the neural network trial solutions, θ are the weights of the neural network(s), and p_ are the hyperparameters . If param_estim = true, then θ additionally contains the parameters of the differential equation appended to the end of the vector.\nparam_estim: whether the parameters of the differential equation should be included in the values sent to the additional_loss function. Defaults to false.\nlogger: ?? needs docs\nlog_options: ?? why is this separate from the logger?\niteration: used to control the iteration counter???\nkwargs: Extra keyword arguments.\n\n\n\n\n\n","category":"type"},{"location":"manual/bpinns/#NeuralPDE.ahmc_bayesian_pinn_ode","page":"BayesianPINN Discretizer for PDESystems","title":"NeuralPDE.ahmc_bayesian_pinn_ode","text":"ahmc_bayesian_pinn_ode(prob, chain; strategy = GridTraining,\n                    dataset = [nothing],init_params = nothing,\n                    draw_samples = 1000, physdt = 1 / 20.0f0,l2std = [0.05],\n                    phystd = [0.05], priorsNNw = (0.0, 2.0),\n                    param = [], nchains = 1, autodiff = false, Kernel = HMC,\n                    Adaptorkwargs = (Adaptor = StanHMCAdaptor,\n                                     Metric = DiagEuclideanMetric,\n                                     targetacceptancerate = 0.8),\n                    Integratorkwargs = (Integrator = Leapfrog,),\n                    MCMCkwargs = (n_leapfrog = 30,),\n                    progress = false, verbose = false)\n\nwarn: Warn\nNote that ahmc_bayesian_pinn_ode() only supports ODEs which are written in the out-of-place form, i.e. du = f(u,p,t), and not f(du,u,p,t). If not declared out-of-place, then the ahmc_bayesian_pinn_ode() will exit with an error.\n\nExample\n\nlinear = (u, p, t) -> -u / p[1] + exp(t / p[2]) * cos(t)\ntspan = (0.0, 10.0)\nu0 = 0.0\np = [5.0, -5.0]\nprob = ODEProblem(linear, u0, tspan, p)\n\n### CREATE DATASET (Necessity for accurate Parameter estimation)\nsol = solve(prob, Tsit5(); saveat = 0.05)\nu = sol.u[1:100]\ntime = sol.t[1:100]\n\n### dataset and BPINN create\nx̂ = collect(Float64, Array(u) + 0.05 * randn(size(u)))\ndataset = [x̂, time]\n\nchain1 = Lux.Chain(Lux.Dense(1, 5, tanh), Lux.Dense(5, 5, tanh), Lux.Dense(5, 1)\n\n### simply solving ode here hence better to not pass dataset(uses ode params specified in prob)\nfh_mcmc_chain1, fhsamples1, fhstats1 = ahmc_bayesian_pinn_ode(prob, chain1,\n                                                            dataset = dataset,\n                                                            draw_samples = 1500,\n                                                            l2std = [0.05],\n                                                            phystd = [0.05],\n                                                            priorsNNw = (0.0,3.0))\n\n### solving ode + estimating parameters hence dataset needed to optimize parameters upon + Pior Distributions for ODE params\nfh_mcmc_chain2, fhsamples2, fhstats2 = ahmc_bayesian_pinn_ode(prob, chain1,\n                                                            dataset = dataset,\n                                                            draw_samples = 1500,\n                                                            l2std = [0.05],\n                                                            phystd = [0.05],\n                                                            priorsNNw = (0.0,3.0),\n                                                            param = [Normal(6.5,0.5), Normal(-3,0.5)])\n\nNOTES\n\nDataset is required for accurate Parameter estimation + solving equations Incase you are only solving the Equations for solution, do not provide dataset\n\nPositional Arguments\n\nprob: DEProblem(out of place and the function signature should be f(u,p,t).\nchain: Lux Neural Netork which would be made the Bayesian PINN.\n\nKeyword Arguments\n\nstrategy: The training strategy used to choose the points for the evaluations. By default GridTraining is used with given physdt discretization.\ninit_params: initial parameter values for BPINN (ideally for multiple chains different initializations preferred)\nnchains: number of chains you want to sample\ndraw_samples: number of samples to be drawn in the MCMC algorithms (warmup samples are ~2/3 of draw samples)\nl2std: standard deviation of BPINN prediction against L2 losses/Dataset\nphystd: standard deviation of BPINN prediction against Chosen Underlying ODE System\npriorsNNw: Tuple of (mean, std) for BPINN Network parameters. Weights and Biases of BPINN are Normal Distributions by default.\nparam: Vector of chosen ODE parameters Distributions in case of Inverse problems.\nautodiff: Boolean Value for choice of Derivative Backend(default is numerical)\nphysdt: Timestep for approximating ODE in it's Time domain. (1/20.0 by default)\nKernel: Choice of MCMC Sampling Algorithm (AdvancedHMC.jl implementations HMC/NUTS/HMCDA)\nIntegratorkwargs: Integrator, jitter_rate, tempering_rate. Refer: https://turinglang.org/AdvancedHMC.jl/stable/\nAdaptorkwargs: Adaptor, Metric, targetacceptancerate. Refer: https://turinglang.org/AdvancedHMC.jl/stable/   Note: Target percentage(in decimal) of iterations in which the proposals are accepted (0.8 by default)\nMCMCargs: A NamedTuple containing all the chosen MCMC kernel's(HMC/NUTS/HMCDA) Arguments, as follows :\nn_leapfrog: number of leapfrog steps for HMC\nδ: target acceptance probability for NUTS and HMCDA\nλ: target trajectory length for HMCDA\nmax_depth: Maximum doubling tree depth (NUTS)\nΔ_max: Maximum divergence during doubling tree (NUTS)\nRefer: https://turinglang.org/AdvancedHMC.jl/stable/\nprogress: controls whether to show the progress meter or not.\nverbose: controls the verbosity. (Sample call args in AHMC)\n\nWarnings\n\nAdvancedHMC.jl is still developing convenience structs so might need changes on new releases.\n\n\n\n\n\n","category":"function"},{"location":"manual/bpinns/#NeuralPDE.ahmc_bayesian_pinn_pde","page":"BayesianPINN Discretizer for PDESystems","title":"NeuralPDE.ahmc_bayesian_pinn_pde","text":"ahmc_bayesian_pinn_pde(pde_system, discretization;\n        draw_samples = 1000,\n        bcstd = [0.01], l2std = [0.05],\n        phystd = [0.05], priorsNNw = (0.0, 2.0),\n        param = [], nchains = 1, Kernel = HMC(0.1, 30),\n        Adaptorkwargs = (Adaptor = StanHMCAdaptor,\n            Metric = DiagEuclideanMetric, targetacceptancerate = 0.8),\n        Integratorkwargs = (Integrator = Leapfrog,), saveats = [1 / 10.0],\n        numensemble = floor(Int, draw_samples / 3), progress = false, verbose = false)\n\nNOTES\n\nDataset is required for accurate Parameter estimation + solving equations.\nReturned solution is a BPINNsolution consisting of Ensemble solution, estimated PDE and NN parameters for chosen saveats grid spacing and last n = numensemble samples in Chain. the complete set of samples in the MCMC chain is returned as fullsolution,  refer BPINNsolution for more details.\n\nPositional Arguments\n\npde_system: ModelingToolkit defined PDE equation or system of equations.\ndiscretization: BayesianPINN discretization for the given pde_system, Neural Network and training strategy.\n\nKeyword Arguments\n\ndraw_samples: number of samples to be drawn in the MCMC algorithms (warmup samples are ~2/3 of draw samples)\nbcstd: Vector of standard deviations of BPINN prediction against Initial/Boundary Condition equations.\nl2std: Vector of standard deviations of BPINN prediction against L2 losses/Dataset for each dependant variable of interest.\nphystd: Vector of standard deviations of BPINN prediction against Chosen Underlying PDE equations.\npriorsNNw: Tuple of (mean, std) for BPINN Network parameters. Weights and Biases of BPINN are Normal Distributions by default.\nparam: Vector of chosen PDE's parameter's Distributions in case of Inverse problems.\nnchains: number of chains you want to sample.\nKernel: Choice of MCMC Sampling Algorithm object HMC/NUTS/HMCDA (AdvancedHMC.jl implementations).\nAdaptorkwargs: Adaptor, Metric, targetacceptancerate. Refer: https://turinglang.org/AdvancedHMC.jl/stable/  Note: Target percentage(in decimal) of iterations in which the proposals are accepted (0.8 by default).\nIntegratorkwargs: Integrator, jitter_rate, tempering_rate. Refer: https://turinglang.org/AdvancedHMC.jl/stable/\nsaveats: Grid spacing for each independent variable for evaluation of ensemble solution, estimated parameters.\nnumensemble: Number of last samples to take for creation of ensemble solution, estimated parameters.\nprogress: controls whether to show the progress meter or not.\nverbose: controls the verbosity. (Sample call args in AHMC).\n\nWarnings\n\nAdvancedHMC.jl is still developing convenience structs so might need changes on new releases.\n\n\n\n\n\n","category":"function"},{"location":"manual/bpinns/#symbolic_discretize-for-BayesianPINN-and-lower-level-interface.","page":"BayesianPINN Discretizer for PDESystems","title":"symbolic_discretize for BayesianPINN and lower level interface.","text":"","category":"section"},{"location":"manual/bpinns/","page":"BayesianPINN Discretizer for PDESystems","title":"BayesianPINN Discretizer for PDESystems","text":"SciMLBase.symbolic_discretize(::PDESystem, ::NeuralPDE.AbstractPINN)","category":"page"},{"location":"manual/bpinns/#SciMLBase.symbolic_discretize-Tuple{PDESystem, NeuralPDE.AbstractPINN}-manual-bpinns","page":"BayesianPINN Discretizer for PDESystems","title":"SciMLBase.symbolic_discretize","text":"prob = symbolic_discretize(pde_system::PDESystem, discretization::AbstractPINN)\n\nsymbolic_discretize is the lower level interface to discretize for inspecting internals. It transforms a symbolic description of a ModelingToolkit-defined PDESystem into a PINNRepresentation which holds the pieces required to build an OptimizationProblem for Optimization.jl or a Likelihood Function used for HMC based Posterior Sampling Algorithms AdvancedHMC.jl which is later optimized upon to give Solution or the Solution Distribution of the PDE.\n\nFor more information, see discretize and PINNRepresentation.\n\n\n\n\n\n","category":"method"},{"location":"manual/bpinns/","page":"BayesianPINN Discretizer for PDESystems","title":"BayesianPINN Discretizer for PDESystems","text":"NeuralPDE.BPINNstats\nNeuralPDE.BPINNsolution","category":"page"},{"location":"manual/bpinns/#NeuralPDE.BPINNstats","page":"BayesianPINN Discretizer for PDESystems","title":"NeuralPDE.BPINNstats","text":"Contains ahmc_bayesian_pinn_ode() function output:\n\nA MCMCChains.jl chain object for sampled parameters.\nThe set of all sampled parameters.\nStatistics like:\nn_steps\nacceptance_rate\nlog_density\nhamiltonian_energy\nhamiltonianenergyerror\nnumerical_error\nstep_size\nnomstepsize\n\n\n\n\n\n","category":"type"},{"location":"manual/bpinns/#NeuralPDE.BPINNsolution","page":"BayesianPINN Discretizer for PDESystems","title":"NeuralPDE.BPINNsolution","text":"BPINN Solution contains the original solution from AdvancedHMC.jl sampling (BPINNstats contains fields related to that).\n\nensemblesol is the Probabilistic Estimate (MonteCarloMeasurements.jl Particles type) of Ensemble solution from All Neural Network's (made using all sampled parameters) output's.\nestimated_nn_params - Probabilistic Estimate of NN params from sampled weights, biases.\nestimated_de_params - Probabilistic Estimate of DE params from sampled unknown DE parameters.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/Lotka_Volterra_BPINNs/#Bayesian-Physics-informed-Neural-Network-ODEs-Solvers","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian Physics informed Neural Network ODEs Solvers","text":"","category":"section"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"Bayesian inference for PINNs provides an approach to ODE solution finding and parameter estimation with quantified uncertainty.","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/#The-Lotka-Volterra-Model","page":"Bayesian PINNs for Coupled ODEs","title":"The Lotka-Volterra Model","text":"","category":"section"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"The Lotka–Volterra equations, also known as the predator–prey equations, are a pair of first-order nonlinear differential equations. These differential equations are frequently used to describe the dynamics of biological systems in which two species interact, one as a predator and the other as prey. The populations change through time according to the pair of equations:","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"beginaligned\nfracmathrmdxmathrmdt = (alpha - beta y(t))x(t) \nfracmathrmdymathrmdt = (delta x(t) - gamma)y(t)\nendaligned","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"where x(t) and y(t) denote the populations of prey and predator at time t, respectively, and alpha beta gamma delta are positive parameters.","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"We implement the Lotka-Volterra model and simulate it with ideal parameters alpha = 15, beta = 1, gamma = 3, and delta = 1 and initial conditions x(0) = y(0) = 1.","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"We then solve the equations and estimate the parameters of the model with priors for alpha, beta, gamma and delta as  Normal(1,2), Normal(2,2), Normal(2,2) and Normal(0,2) using a neural network.","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"using NeuralPDE, Lux, Plots, OrdinaryDiffEq, Distributions, Random\n\nfunction lotka_volterra(u, p, t)\n    # Model parameters.\n    α, β, γ, δ = p\n    # Current state.\n    x, y = u\n\n    # Evaluate differential equations.\n    dx = (α - β * y) * x # prey\n    dy = (δ * x - γ) * y # predator\n\n    return [dx, dy]\nend\n\n# initial-value problem.\nu0 = [1.0, 1.0]\np = [1.5, 1.0, 3.0, 1.0]\ntspan = (0.0, 4.0)\nprob = ODEProblem(lotka_volterra, u0, tspan, p)","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"With the saveat argument, we can specify that the solution is stored only at saveat time units.","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"# Solve using OrdinaryDiffEq.jl solver\ndt = 0.01\nsolution = solve(prob, Tsit5(); saveat = dt)","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"We generate noisy observations to use for the parameter estimation task in this tutorial. To make the example more realistic we add random normally distributed noise to the simulation.","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"# Dataset creation for parameter estimation (30% noise)\ntime = solution.t\nu = hcat(solution.u...)\nx = u[1, :] + (u[1, :]) .* (0.3 .* randn(length(u[1, :])))\ny = u[2, :] + (u[2, :]) .* (0.3 .* randn(length(u[2, :])))\ndataset = [x, y, time]\n\n# Plotting the data which will be used\nplot(time, x, label = \"noisy x\")\nplot!(time, y, label = \"noisy y\")\nplot!(solution, labels = [\"x\" \"y\"])","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"Lets define a PINN.","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"# Neural Networks must have 2 outputs as u -> [dx,dy] in function lotka_volterra()\nchain = Lux.Chain(Lux.Dense(1, 6, tanh), Lux.Dense(6, 6, tanh),\n    Lux.Dense(6, 2))","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"The dataset we generated can be passed for doing parameter estimation using provided priors in param keyword argument for BNNODE.","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"alg = BNNODE(chain;\n    dataset = dataset,\n    draw_samples = 1000,\n    l2std = [0.1, 0.1],\n    phystd = [0.1, 0.1],\n    priorsNNw = (0.0, 3.0),\n    param = [\n        Normal(1, 2),\n        Normal(2, 2),\n        Normal(2, 2),\n        Normal(0, 2)], progress = false)\n\nsol_pestim = solve(prob, alg; saveat = dt)\n\nnothing #hide","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"The solution for the ODE is returned as a nested vector sol_flux_pestim.ensemblesol. Here, [x , y] would be returned.","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"# plotting solution for x,y for chain\nplot(time, sol_pestim.ensemblesol[1], label = \"estimated x\")\nplot!(time, sol_pestim.ensemblesol[2], label = \"estimated y\")\n\n# comparing it with the original solution\nplot!(solution, labels = [\"true x\" \"true y\"])","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"We can see the estimated ODE parameters by -","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"sol_pestim.estimated_de_params","category":"page"},{"location":"tutorials/Lotka_Volterra_BPINNs/","page":"Bayesian PINNs for Coupled ODEs","title":"Bayesian PINNs for Coupled ODEs","text":"We can see it is close to the true values of the parameters.","category":"page"},{"location":"manual/adaptive_losses/#adaptive_loss","page":"Adaptive Loss Functions","title":"Adaptive Loss Functions","text":"","category":"section"},{"location":"manual/adaptive_losses/","page":"Adaptive Loss Functions","title":"Adaptive Loss Functions","text":"The NeuralPDE discretize function allows for specifying adaptive loss function strategy which improve training performance by reweighing the equations as necessary to ensure the boundary conditions are well-satisfied, even in ill-conditioned scenarios. The following are the options for the adaptive_loss:","category":"page"},{"location":"manual/adaptive_losses/","page":"Adaptive Loss Functions","title":"Adaptive Loss Functions","text":"NeuralPDE.NonAdaptiveLoss\nNeuralPDE.GradientScaleAdaptiveLoss\nNeuralPDE.MiniMaxAdaptiveLoss","category":"page"},{"location":"manual/adaptive_losses/#NeuralPDE.NonAdaptiveLoss","page":"Adaptive Loss Functions","title":"NeuralPDE.NonAdaptiveLoss","text":"NonAdaptiveLoss(; pde_loss_weights = 1.0,\n                  bc_loss_weights = 1.0,\n                  additional_loss_weights = 1.0)\n\nA way of loss weighting the components of the loss function in the total sum that does not change during optimization\n\n\n\n\n\n","category":"type"},{"location":"manual/adaptive_losses/#NeuralPDE.GradientScaleAdaptiveLoss","page":"Adaptive Loss Functions","title":"NeuralPDE.GradientScaleAdaptiveLoss","text":"GradientScaleAdaptiveLoss(reweight_every;\n                        weight_change_inertia = 0.9,\n                        pde_loss_weights = 1.0,\n                        bc_loss_weights = 1.0,\n                        additional_loss_weights = 1.0)\n\nA way of adaptively reweighting the components of the loss function in the total sum such that BCi loss weights are scaled by the exponential moving average of max(|∇pdeloss|) / mean(|∇bciloss|)).\n\nPositional Arguments\n\nreweight_every: how often to reweight the BC loss functions, measured in iterations. Reweighting is somewhat expensive since it involves evaluating the gradient of each component loss function,\n\nKeyword Arguments\n\nweight_change_inertia: a real number that represents the inertia of the exponential moving average of the BC weight changes,\n\nReferences\n\nUnderstanding and mitigating gradient pathologies in physics-informed neural networks Sifan Wang, Yujun Teng, Paris Perdikaris https://arxiv.org/abs/2001.04536v1\n\nWith code reference: https://github.com/PredictiveIntelligenceLab/GradientPathologiesPINNs\n\n\n\n\n\n","category":"type"},{"location":"manual/adaptive_losses/#NeuralPDE.MiniMaxAdaptiveLoss","page":"Adaptive Loss Functions","title":"NeuralPDE.MiniMaxAdaptiveLoss","text":"function MiniMaxAdaptiveLoss(reweight_every;\n                            pde_max_optimiser = OptimizationOptimisers.Adam(1e-4),\n                            bc_max_optimiser = OptimizationOptimisers.Adam(0.5),\n                            pde_loss_weights = 1,\n                            bc_loss_weights = 1,\n                            additional_loss_weights = 1)\n\nA way of adaptively reweighting the components of the loss function in the total sum such that the loss weights are maximized by an internal optimizer, which leads to a behavior where loss functions that have not been satisfied get a greater weight.\n\nPositional Arguments\n\nreweight_every: how often to reweight the PDE and BC loss functions, measured in iterations.  Reweighting is cheap since it re-uses the value of loss functions generated during the main optimization loop.\n\nKeyword Arguments\n\npde_max_optimiser: a OptimizationOptimisers optimiser that is used internally to maximize the weights of the PDE loss functions.\nbc_max_optimiser: a OptimizationOptimisers optimiser that is used internally to maximize the weights of the BC loss functions.\n\nReferences\n\nSelf-Adaptive Physics-Informed Neural Networks using a Soft Attention Mechanism Levi McClenny, Ulisses Braga-Neto https://arxiv.org/abs/2009.04544\n\n\n\n\n\n","category":"type"},{"location":"tutorials/pdesystem/#Specifying-and-Solving-PDESystems-with-Physics-Informed-Neural-Networks-(PINNs)","page":"Introduction to NeuralPDE for PDEs","title":"Specifying and Solving PDESystems with Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"In this example, we will solve a Poisson equation:","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"^2_x u(x y) + ^2_y u(x y) = - sin(pi x) sin(pi y)  ","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"with the boundary conditions:","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"beginalign*\nu(0 y) = 0  \nu(1 y) = 0  \nu(x 0) = 0  \nu(x 1) = 0  \nendalign*","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"on the space domain:","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"x in 0 1    y in 0 1  ","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"Using physics-informed neural networks.","category":"page"},{"location":"tutorials/pdesystem/#Copy-Pasteable-Code","page":"Introduction to NeuralPDE for PDEs","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"using NeuralPDE, Lux, Optimization, OptimizationOptimJL\nusing LineSearches\nusing ModelingToolkit: Interval\nusing Plots\n\n@parameters x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\n# 2D PDE\neq = Dxx(u(x, y)) + Dyy(u(x, y)) ~ -sin(pi * x) * sin(pi * y)\n\n# Boundary conditions\nbcs = [u(0, y) ~ 0.0, u(1, y) ~ 0.0,\n    u(x, 0) ~ 0.0, u(x, 1) ~ 0.0]\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n    y ∈ Interval(0.0, 1.0)]\n\n# Neural network\ndim = 2 # number of dimensions\nchain = Lux.Chain(Dense(dim, 16, Lux.σ), Dense(16, 16, Lux.σ), Dense(16, 1))\n\n# Discretization\ndiscretization = PhysicsInformedNN(\n    chain, QuadratureTraining(; batch = 200, abstol = 1e-6, reltol = 1e-6))\n\n@named pde_system = PDESystem(eq, bcs, domains, [x, y], [u(x, y)])\nprob = discretize(pde_system, discretization)\n\n#Callback function\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\n# Optimizer\nopt = OptimizationOptimJL.LBFGS(linesearch = BackTracking())\nres = solve(prob, opt, maxiters = 1000)\nphi = discretization.phi\n\ndx = 0.05\nxs, ys = [infimum(d.domain):(dx / 10):supremum(d.domain) for d in domains]\nanalytic_sol_func(x, y) = (sin(pi * x) * sin(pi * y)) / (2pi^2)\n\nu_predict = reshape([first(phi([x, y], res.u)) for x in xs for y in ys],\n    (length(xs), length(ys)))\nu_real = reshape([analytic_sol_func(x, y) for x in xs for y in ys],\n    (length(xs), length(ys)))\ndiff_u = abs.(u_predict .- u_real)\n\np1 = plot(xs, ys, u_real, linetype = :contourf, title = \"analytic\");\np2 = plot(xs, ys, u_predict, linetype = :contourf, title = \"predict\");\np3 = plot(xs, ys, diff_u, linetype = :contourf, title = \"error\");\nplot(p1, p2, p3)","category":"page"},{"location":"tutorials/pdesystem/#Detailed-Description","page":"Introduction to NeuralPDE for PDEs","title":"Detailed Description","text":"","category":"section"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"The ModelingToolkit PDE interface for this example looks like this:","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nusing ModelingToolkit: Interval\nusing Plots\n\n@parameters x y\n@variables u(..)\n@derivatives Dxx'' ~ x\n@derivatives Dyy'' ~ y\n\n# 2D PDE\neq = Dxx(u(x, y)) + Dyy(u(x, y)) ~ -sin(pi * x) * sin(pi * y)\n\n# Boundary conditions\nbcs = [u(0, y) ~ 0.0, u(1, y) ~ 0.0,\n    u(x, 0) ~ 0.0, u(x, 1) ~ 0.0]\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n    y ∈ Interval(0.0, 1.0)]","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"Here, we define the neural network, where the input of NN equals the number of dimensions and output equals the number of equations in the system.","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"# Neural network\ndim = 2 # number of dimensions\nchain = Lux.Chain(Dense(dim, 16, Lux.σ), Dense(16, 16, Lux.σ), Dense(16, 1))","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"Here, we build PhysicsInformedNN algorithm where dx is the step of discretization where strategy stores information for choosing a training strategy.","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"discretization = PhysicsInformedNN(\n    chain, QuadratureTraining(; batch = 200, abstol = 1e-6, reltol = 1e-6))","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"As described in the API docs, we now need to define the PDESystem and create PINNs problem using the discretize method.","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"@named pde_system = PDESystem(eq, bcs, domains, [x, y], [u(x, y)])\nprob = discretize(pde_system, discretization)","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"Here, we define the callback function and the optimizer. And now we can solve the PDE using PINNs (with the number of epochs maxiters=1000).","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"#Optimizer\nopt = OptimizationOptimJL.LBFGS(linesearch = BackTracking())\n\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\n# We can pass the callback function in the solve. Not doing here as the output would be very long.\nres = Optimization.solve(prob, opt, maxiters = 1000)\nphi = discretization.phi","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"We can plot the predicted solution of the PDE and compare it with the analytical solution to plot the relative error.","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"dx = 0.05\nxs, ys = [infimum(d.domain):(dx / 10):supremum(d.domain) for d in domains]\nanalytic_sol_func(x, y) = (sin(pi * x) * sin(pi * y)) / (2pi^2)\n\nu_predict = reshape([first(phi([x, y], res.u)) for x in xs for y in ys],\n    (length(xs), length(ys)))\nu_real = reshape([analytic_sol_func(x, y) for x in xs for y in ys],\n    (length(xs), length(ys)))\ndiff_u = abs.(u_predict .- u_real)\n\np1 = plot(xs, ys, u_real, linetype = :contourf, title = \"analytic\");\np2 = plot(xs, ys, u_predict, linetype = :contourf, title = \"predict\");\np3 = plot(xs, ys, diff_u, linetype = :contourf, title = \"error\");\nplot(p1, p2, p3)","category":"page"},{"location":"manual/nnrode/#Random-Ordinary-Differential-Equation-Specialized-Physics-Informed-Neural-Solver","page":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","title":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","text":"","category":"section"},{"location":"manual/nnrode/","page":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","title":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","text":"TODO","category":"page"},{"location":"#NeuralPDE.jl:-Automatic-Physics-Informed-Neural-Networks-(PINNs)","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"NeuralPDE.jl is a solver package which consists of neural network solvers for partial differential equations using physics-informed neural networks (PINNs) and the ability to generate neural networks which both approximate physical laws and real data simultaneously.","category":"page"},{"location":"#Features","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"Features","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"Physics-Informed Neural Networks for ODE, SDE, RODE, and PDE solving.\nAbility to define extra loss functions to mix xDE solving with data fitting (scientific machine learning).\nAutomated construction of Physics-Informed loss functions from a high-level symbolic interface.\nSophisticated techniques like quadrature training strategies, adaptive loss functions, and neural adapters to accelerate training.\nIntegrated logging suite for handling connections to TensorBoard.\nHandling of (partial) integro-differential equations and various stochastic equations.\nSpecialized forms for solving ODEProblems with neural networks.\nCompatibility with Flux.jl and Lux.jl. for all the GPU-powered machine learning layers available from those libraries.\nCompatibility with NeuralOperators.jl for mixing DeepONets and other neural operators (Fourier Neural Operators, Graph Neural Operators, etc.) with physics-informed loss functions.","category":"page"},{"location":"#Installation","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"Installation","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"Assuming that you already have Julia correctly installed, it suffices to import NeuralPDE.jl in the standard way:","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"import Pkg\nPkg.add(\"NeuralPDE\")","category":"page"},{"location":"#Contributing","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"Contributing","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"page"},{"location":"#Citation","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"Citation","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"If you use NeuralPDE.jl in your research, please cite this paper:","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"@misc{https://doi.org/10.48550/arxiv.2107.09443,\n  doi = {10.48550/ARXIV.2107.09443},\n  url = {https://arxiv.org/abs/2107.09443},\n  author = {Zubov, Kirill and McCarthy, Zoe and Ma, Yingbo and Calisto, Francesco and Pagliarino, Valerio and Azeglio, Simone and Bottero, Luca and Luján, Emmanuel and Sulzer, Valentin and Bharambe, Ashutosh and Vinchhi, Nand and Balakrishnan, Kaushik and Upadhyay, Devesh and Rackauckas, Chris},\n  keywords = {Mathematical Software (cs.MS), Symbolic Computation (cs.SC), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  title = {NeuralPDE: Automating Physics-Informed Neural Networks (PINNs) with Error Approximations},\n  publisher = {arXiv},\n  year = {2021},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}\n}","category":"page"},{"location":"#Flux.jl-vs-Lux.jl","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"Flux.jl vs Lux.jl","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"Both Flux and Lux defined neural networks are supported by NeuralPDE.jl. However, Lux.jl neural networks are greatly preferred for many correctness reasons. Particularly, a Flux Chain does not respect Julia's type promotion rules. This causes major problems in that the restructuring of a Flux neural network will not respect the chosen types from the solver. Demonstration:","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"using Flux, Tracker\nx = [0.8; 0.8]\nann = Chain(Dense(2, 10, tanh), Dense(10, 1))\np, re = Flux.destructure(ann)\nz = re(Float64(p))","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"While one may think this recreates the neural network to act in Float64 precision, it does not and instead its values will silently downgrade everything to Float32. This is only fixed by Chain(Dense(2, 10, tanh), Dense(10, 1)) |> f64. Similar cases will lead to dropped gradients with complex numbers. This is not an issue with the automatic differentiation library commonly associated with Flux (Zygote.jl) but rather due to choices in the neural network library's decision for how to approach type handling and precision. Thus when using DiffEqFlux.jl with Flux, the user must be very careful to ensure that the precision of the arguments are correct, and anything that requires alternative types (like TrackerAdjoint tracked values, ForwardDiffSensitivity dual numbers, and TaylorDiff.jl differentiation) are suspect.","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"Lux.jl has none of these issues, is simpler to work with due to the parameters in its function calls being explicit rather than implicit global references, and achieves higher performance. It is built on the same foundations as Flux.jl, such as Zygote and NNLib, and thus it supports the same layers underneath and calls the same kernels. The better performance comes from not having the overhead of restructure required. Thus we highly recommend people use Lux instead and only use the Flux fallbacks for legacy code.","category":"page"},{"location":"#Reproducibility","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"</details>","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"</details>","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"using Pkg # hide\nPkg.status(; mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"</details>","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"using TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"page"},{"location":"tutorials/constraints/#Imposing-Constraints-on-Physics-Informed-Neural-Network-(PINN)-Solutions","page":"Imposing Constraints","title":"Imposing Constraints on Physics-Informed Neural Network (PINN) Solutions","text":"","category":"section"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"Let's consider the Fokker-Planck equation:","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"- fracx left  left( alpha x - beta x^3right) p(x)right  + fracsigma^22 frac^2x^2 p(x) = 0  ","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"which must satisfy the normalization condition:","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"Delta t  p(x) = 1","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"with the boundary conditions:","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"p(-22) = p(22) = 0","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"with Physics-Informed Neural Networks.","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, LineSearches\nusing Integrals, Cubature\nusing ModelingToolkit: Interval, infimum, supremum\n# the example is taken from this article https://arxiv.org/abs/1910.10503\n@parameters x\n@variables p(..)\nDx = Differential(x)\nDxx = Differential(x)^2\n\nα = 0.3\nβ = 0.5\n_σ = 0.5\nx_0 = -2.2\nx_end = 2.2\n\neq = Dx((α * x - β * x^3) * p(x)) ~ (_σ^2 / 2) * Dxx(p(x))\n\n# Initial and boundary conditions\nbcs = [p(x_0) ~ 0.0, p(x_end) ~ 0.0]\n\n# Space and time domains\ndomains = [x ∈ Interval(x_0, x_end)]\n\n# Neural network\ninn = 18\nchain = Lux.Chain(Dense(1, inn, Lux.σ),\n    Dense(inn, inn, Lux.σ),\n    Dense(inn, inn, Lux.σ),\n    Dense(inn, 1))\n\nlb = [x_0]\nub = [x_end]\nfunction norm_loss_function(phi, θ, p)\n    function inner_f(x, θ)\n        0.01 * phi(x, θ) .- 1\n    end\n    prob = IntegralProblem(inner_f, lb, ub, θ)\n    norm2 = solve(prob, HCubatureJL(), reltol = 1e-8, abstol = 1e-8, maxiters = 10)\n    abs(norm2[1])\nend\n\ndiscretization = PhysicsInformedNN(chain,\n    QuadratureTraining(),\n    additional_loss = norm_loss_function)\n\n@named pdesystem = PDESystem(eq, bcs, domains, [x], [p(x)])\nprob = discretize(pdesystem, discretization)\nphi = discretization.phi\n\nsym_prob = NeuralPDE.symbolic_discretize(pdesystem, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions\naprox_derivative_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncb_ = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p.u), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p.u), bcs_inner_loss_functions))\n    println(\"additional_loss: \", norm_loss_function(phi, p.u, nothing))\n    return false\nend\n\nres = Optimization.solve(\n    prob, BFGS(linesearch = BackTracking()), callback = cb_, maxiters = 600)","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"And some analysis:","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"using Plots\nC = 142.88418699042 #fitting param\nanalytic_sol_func(x) = C * exp((1 / (2 * _σ^2)) * (2 * α * x^2 - β * x^4))\n\nxs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains][1]\nu_real = [analytic_sol_func(x) for x in xs]\nu_predict = [first(phi(x, res.u)) for x in xs]\n\nplot(xs, u_real, label = \"analytic\")\nplot!(xs, u_predict, label = \"predict\")","category":"page"},{"location":"developer/debugging/#Debugging-PINN-Solutions","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"","category":"section"},{"location":"developer/debugging/#Note-this-is-all-not-current-right-now!","page":"Debugging PINN Solutions","title":"Note this is all not current right now!","text":"","category":"section"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"Let's walk through debugging functions for the physics-informed neural network PDE solvers.","category":"page"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"using NeuralPDE, ModelingToolkit, Flux, Zygote\nimport ModelingToolkit: Interval, infimum, supremum\n# 2d wave equation, neumann boundary condition\n@parameters x, t\n@variables u(..)\nDxx = Differential(x)^2\nDtt = Differential(t)^2\nDt = Differential(t)\n#2D PDE\nC = 1\neq = Dtt(u(x, t)) ~ C^2 * Dxx(u(x, t))\n\n# Initial and boundary conditions\nbcs = [u(0, t) ~ 0.0,\n    u(1, t) ~ 0.0,\n    u(x, 0) ~ x * (1.0 - x),\n    Dt(u(x, 0)) ~ 0.0]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n    t ∈ Interval(0.0, 1.0)]\n\n# Neural network\nchain = FastChain(FastDense(2, 16, Flux.σ), FastDense(16, 16, Flux.σ), FastDense(16, 1))\ninit_params = DiffEqFlux.initial_params(chain)\n\neltypeθ = eltype(init_params)\nphi = NeuralPDE.get_phi(chain)\nderivative = NeuralPDE.get_numeric_derivative()\n\nu_ = (cord, θ, phi) -> sum(phi(cord, θ))\n\nphi([1, 2], init_params)\n\nphi_ = (p) -> phi(p, init_params)[1]\ndphi = Zygote.gradient(phi_, [1.0, 2.0])\n\ndphi1 = derivative(phi, u_, [1.0, 2.0], [[0.0049215667, 0.0]], 1, init_params)\ndphi2 = derivative(phi, u_, [1.0, 2.0], [[0.0, 0.0049215667]], 1, init_params)\nisapprox(dphi[1][1], dphi1, atol = 1e-8)\nisapprox(dphi[1][2], dphi2, atol = 1e-8)\n\nindvars = [x, t]\ndepvars = [u(x, t)]\ndict_depvars_input = Dict(:u => [:x, :t])\ndim = length(domains)\ndx = 0.1\nmultioutput = chain isa AbstractArray\nstrategy = NeuralPDE.GridTraining(dx)\nintegral = NeuralPDE.get_numeric_integral(strategy, indvars, multioutput, chain, derivative)\n\n_pde_loss_function = NeuralPDE.build_loss_function(eq, indvars, depvars, phi, derivative,\n    integral, multioutput, init_params,\n    strategy)","category":"page"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"julia> expr_pde_loss_function = NeuralPDE.build_symbolic_loss_function(eq,indvars,depvars,dict_depvars_input,phi,derivative,integral,multioutput,init_params,strategy)\n\n:((cord, var\"##θ#529\", phi, derivative, integral, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  derivative.(phi, u, cord, Array{Float32,1}[[0.0, 0.0049215667], [0.0, 0.0049215667]], 2, var\"##θ#529\") .- derivative.(phi, u, cord, Array{Float32,1}[[0.0049215667, 0.0], [0.0049215667, 0.0]], 2, var\"##θ#529\")\n              end\n          end\n      end)\n\njulia> bc_indvars = NeuralPDE.get_variables(bcs,indvars,depvars)\n4-element Array{Array{Any,1},1}:\n [:t]\n [:t]\n [:x]\n [:x]","category":"page"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"_bc_loss_functions = [NeuralPDE.build_loss_function(bc, indvars, depvars,\n                          phi, derivative, integral, multioutput,\n                          init_params, strategy,\n                          bc_indvars = bc_indvar)\n                      for (bc, bc_indvar) in zip(bcs, bc_indvars)]","category":"page"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"julia> expr_bc_loss_functions = [NeuralPDE.build_symbolic_loss_function(bc,indvars,depvars,dict_depvars_input,\n                                                                        phi,derivative,integral,multioutput,init_params,strategy,\n                                                                        bc_indvars = bc_indvar) for (bc,bc_indvar) in zip(bcs,bc_indvars)]\n4-element Array{Expr,1}:\n :((cord, var\"##θ#529\", phi, derivative, integral, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  u.(cord, var\"##θ#529\", phi) .- 0.0\n              end\n          end\n      end)\n :((cord, var\"##θ#529\", phi, derivative, integral, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  u.(cord, var\"##θ#529\", phi) .- 0.0\n              end\n          end\n      end)\n :((cord, var\"##θ#529\", phi, derivative, integral, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  u.(cord, var\"##θ#529\", phi) .- (*).(x, (+).(1.0, (*).(-1, x)))\n              end\n          end\n      end)\n :((cord, var\"##θ#529\", phi, derivative, integral, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  derivative.(phi, u, cord, Array{Float32,1}[[0.0, 0.0049215667]], 1, var\"##θ#529\") .- 0.0\n              end\n          end\n      end)","category":"page"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"train_sets = NeuralPDE.generate_training_sets(domains, dx, [eq], bcs, eltypeθ, indvars,\n    depvars)\npde_train_set, bcs_train_set = train_sets","category":"page"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"julia> pde_train_set\n1-element Array{Array{Float32,2},1}:\n [0.1 0.2 … 0.8 0.9; 0.1 0.1 … 1.0 1.0]\n\n\njulia> bcs_train_set\n4-element Array{Array{Float32,2},1}:\n [0.0 0.0 … 0.0 0.0; 0.0 0.1 … 0.9 1.0]\n [1.0 1.0 … 1.0 1.0; 0.0 0.1 … 0.9 1.0]\n [0.0 0.1 … 0.9 1.0; 0.0 0.0 … 0.0 0.0]\n [0.0 0.1 … 0.9 1.0; 0.0 0.0 … 0.0 0.0]","category":"page"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"pde_bounds, bcs_bounds = NeuralPDE.get_bounds(\n    domains, [eq], bcs, eltypeθ, indvars, depvars,\n    NeuralPDE.StochasticTraining(100))","category":"page"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"julia> pde_bounds\n1-element Vector{Vector{Any}}:\n [Float32[0.01, 0.99], Float32[0.01, 0.99]]\n\njulia> bcs_bounds\n4-element Vector{Vector{Any}}:\n [0, Float32[0.0, 1.0]]\n [1, Float32[0.0, 1.0]]\n [Float32[0.0, 1.0], 0]\n [Float32[0.0, 1.0], 0]","category":"page"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"discretization = NeuralPDE.PhysicsInformedNN(chain, strategy)\n\n@named pde_system = PDESystem(eq, bcs, domains, indvars, depvars)\nprob = NeuralPDE.discretize(pde_system, discretization)\n\nexpr_prob = NeuralPDE.symbolic_discretize(pde_system, discretization)\nexpr_pde_loss_function, expr_bc_loss_functions = expr_prob","category":"page"}]
}
